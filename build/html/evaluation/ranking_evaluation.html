

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Ranking Evaluation &mdash; MLAI 0.1 documentation</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=80d5e7a1" />
      <link rel="stylesheet" type="text/css" href="../_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="../_static/custom.css?v=3eba48d4" />

  
    <link rel="canonical" href="https://xinliyu.github.io/ML-AI-From-Theory-To-Industry/evaluation/ranking_evaluation.html" />
      <script src="../_static/jquery.js?v=5d32c60e"></script>
      <script src="../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../_static/documentation_options.js?v=2709fde1"></script>
      <script src="../_static/doctools.js?v=9bcbadda"></script>
      <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
      <script src="../_static/foldable_admonitions.js?v=351fa817"></script>
      <script>window.MathJax = {"tex": {"inlineMath": [["$", "$"]], "displayMath": [["$$", "$$"]]}, "options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
      <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      <script src="../_static/js/mathjax-config.js?v=c54ad740"></script>
      <script src="https://unpkg.com/react@17/umd/react.production.min.js"></script>
      <script src="https://unpkg.com/react-dom@17/umd/react-dom.production.min.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Diversity Evaluation" href="diversity_evaluation.html" />
    <link rel="prev" title="Basic Evaluation" href="basic_evaluation.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../index.html" class="icon icon-home">
            MLAI
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../system_design/index.html">ML/AI Systen Design</a></li>
<li class="toctree-l1"><a class="reference internal" href="../modeling/index.html">Modeling</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="index.html">Evaluation</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="basic_evaluation.html">Basic Evaluation</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Ranking Evaluation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#precision-k-recall-k">Precision&#64;k &amp; Recall&#64;k</a></li>
<li class="toctree-l3"><a class="reference internal" href="#mean-average-precision-map">Mean Average Precision (MAP)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#normalized-discounted-cumulative-gain-ndcg">Normalized Discounted Cumulative Gain (NDCG)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#zipf-s-law">Zipf’s Law</a></li>
<li class="toctree-l4"><a class="reference internal" href="#dcg">DCG</a></li>
<li class="toctree-l4"><a class="reference internal" href="#ndcg">NDCG</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#ranking-metrics-for-early-relevance-low-precision-ranking-scenarios">Ranking Metrics For Early-Relevance &amp; Low-Precision Ranking Scenarios</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#mean-reciprocal-rank-mrr">Mean Reciprocal Rank (MRR)</a></li>
<li class="toctree-l4"><a class="reference internal" href="#mean-expected-reciprocal-rank-merr">Mean Expected Reciprocal Rank (MERR)</a></li>
<li class="toctree-l4"><a class="reference internal" href="#other-metrics">Other Metrics</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#summary">Summary</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#position-based-metrics">Position-Based Metrics</a></li>
<li class="toctree-l4"><a class="reference internal" href="#comprehensive-ranking-metrics">Comprehensive Ranking Metrics</a></li>
<li class="toctree-l4"><a class="reference internal" href="#low-precision-ranking-scenarios">Low-Precision Ranking Scenarios</a></li>
<li class="toctree-l4"><a class="reference internal" href="#best-practices">Best Practices</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="diversity_evaluation.html">Diversity Evaluation</a></li>
<li class="toctree-l2"><a class="reference internal" href="novelty_evaluation.html">Novelty Metrics</a></li>
<li class="toctree-l2"><a class="reference internal" href="novelty_evaluation.html#calibrated-diversity">Calibrated Diversity</a></li>
<li class="toctree-l2"><a class="reference internal" href="novelty_evaluation.html#personalized-diversity">Personalized Diversity</a></li>
<li class="toctree-l2"><a class="reference internal" href="novelty_evaluation.html#trade-offs-between-relevance-and-diversity">Trade-offs Between Relevance and Diversity</a></li>
<li class="toctree-l2"><a class="reference internal" href="novelty_evaluation.html#evaluation-challenges">Evaluation Challenges</a></li>
<li class="toctree-l2"><a class="reference internal" href="novelty_evaluation.html#selection-of-an-appropriate-diversity-metric">Selection of an Appropriate Diversity Metric</a></li>
<li class="toctree-l2"><a class="reference internal" href="novelty_evaluation.html#practical-applications">Practical Applications</a></li>
<li class="toctree-l2"><a class="reference internal" href="novelty_evaluation.html#summary">Summary</a></li>
</ul>
</li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">MLAI</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="index.html">Evaluation</a></li>
      <li class="breadcrumb-item active">Ranking Evaluation</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/evaluation/ranking_evaluation.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="ranking-evaluation">
<h1>Ranking Evaluation<a class="headerlink" href="#ranking-evaluation" title="Link to this heading"></a></h1>
<p>Many ML/AI systems must rank items in order of predicted relevance, such as search engines and recommendation systems. In this section, we assume that given a <strong>query</strong>, the ML/AI system returns an ordered list of items.</p>
<p>Unlike precision and recall which only measure how well the ML/AI system makes binary decisions about returning relevant items, <span class="target" id="newconcept-ranking_evaluation"></span><span class="newconcept">Ranking Evaluation</span> focuses on assessing how well a system orders items. The common approach is to assign higher weights to items appearing higher in the ranked results during metric calculation. Ranking evaluation is particularly valuable in applications where the position of an item in results directly impacts user experience.</p>
<section id="precision-k-recall-k">
<h2>Precision&#64;k &amp; Recall&#64;k<a class="headerlink" href="#precision-k-recall-k" title="Link to this heading"></a></h2>
<p><span class="target" id="newconcept-precision&#64;k"></span><span class="newconcept">Precision&#64;k</span> (P&#64;k) and <span class="target" id="newconcept-recall&#64;k"></span><span class="newconcept">Recall&#64;k</span> (R&#64;k) are basic metrics for ranking evaluation. Given a query, they adapt the traditional <a class="reference external" href="basic_evaluation.html#newconcept-precision"><span class="refconcept">precision</span></a> and <a class="reference external" href="basic_evaluation.html#newconcept-recall"><span class="refconcept">recall</span></a> metrics to evaluate only the top-k ranked items:</p>
<div class="math notranslate nohighlight">
\[\text{Precision&#64;k} = \frac{\text{Number of relevant items in top-k results}}{k}\]</div>
<div class="math notranslate nohighlight">
\[\text{Recall&#64;k} = \frac{\text{Number of relevant items in top-k results}}{\text{Total number of relevant items}}\]</div>
<p>These metrics are useful for evaluating systems where users typically only examine a limited number of top results (e.g., first page of search results). To evaluation the ML/AI system ranking, we can avearge P&#64;k and R&#64;k across all queries in the ground truth.</p>
</section>
<section id="mean-average-precision-map">
<h2>Mean Average Precision (MAP)<a class="headerlink" href="#mean-average-precision-map" title="Link to this heading"></a></h2>
<p>Given a query, <span class="target" id="newconcept-average_precision"></span><span class="newconcept">Average Precision (AP)</span> averages P&#64;k and R&#64;k metrics across a range of $k$ from 1 to a chosen maximum rank number $n$ when the item at position $k$ is relevant.</p>
<div class="math notranslate nohighlight">
\[AP(q) = \frac{\sum_{k=1}^{n} P(k) \cdot \text{rel}(k)}{\sum_{k=1}^{n} \text{rel}(k)}\]</div>
<p>Where:</p>
<ul>
<li><p>$q$ is a query.</p></li>
<li><p>$P(k)$ is P&#64;k for query $q$.</p></li>
<li><p>$\text{rel}(k)$ is the relevance score for the item ranked at $k$, and is typically an indicator function for MAP,</p>
<div class="math notranslate nohighlight">
\[\begin{split}\text{rel}(k) = \mathbf{1}[{\text{item at position $k$ is relevant}}] =
\begin{cases}
1 &amp; \text{if item at position $k$ is relevant} \\
0 &amp; \text{otherwise}
\end{cases}\end{split}\]</div>
</li>
<li><p>Given the formula, item at higher rank has more weight in AP calculation. For example, item at rank $1$ will be considered across $k=1, …, n$, while the item at rank $n$ is only considered once.</p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Why using indicator $\text{rel}(k)$ to skip irrelevant items?</p>
<p>AP’s formula skips items that are not relevant, through applying the indicator $\text{rel}$, for the following reasons.</p>
<ul class="simple">
<li><p><strong>Intuitive interpretation</strong>: The overall AP value represents the average precision a user would experience when finding each relevant document in the ranked list. This is more meaningful than averaging precision at every position regardless of relevance.</p></li>
<li><p><strong>Historical development</strong>: The formula evolved from the <a class="reference external" href="basic_evaluation.html#newconcept-pr_curve"><span class="refconcept">PR Curve</span></a> where precision is plotted against recall. By averaging precision at each recall point (which occurs exactly when a new relevant document is found), AP approximates the <a class="reference external" href="basic_evaluation.html#newconcept-area_under_curve"><span class="refconcept">area under curve</span></a>.</p></li>
</ul>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Can $\text{rel}(k)$ be a non-indicator function?</p>
<p>The relevance metric in Mean Average Precision (MAP) doesn’t necessarily have to be an indicator function, but using a binary indicator function ($\text{rel}(k) \in \{0,1\}$) has become the standard approach. MAP was originally developed for information retrieval scenarios with binary relevance judgments (relevant/not relevant). The indicator function directly captures this binary nature. With binary relevance, AP has a clear interpretation: “the average precision a user experiences when finding each relevant document.”, and has an AUC based interpretation.</p>
<p>There have been extensions of MAP that incorporate non-indicator relevance scores rather than just binary judgments to allow higher weights to more relevant items. The extended version is typically considered a separate metric with its own names (i.e., <span class="target" id="newconcept-graded_average_precision"></span><span class="newconcept">Graded Average Precision (GAP)</span>) rather than standard MAP.</p>
</div>
<div class="example-green admonition">
<p class="admonition-title">AP’s Sensitivity to Ranking Order</p>
<p>Consider two different rankings for the same query, each with 3 relevant items (R) and 2 irrelevant items (N):</p>
<p><strong>Ranking A:</strong> [R, R, R, N, N]</p>
<p>Precision calculations:</p>
<ul class="simple">
<li><p>P&#64;1 = 1/1 = 1.0 (rel=1)</p></li>
<li><p>P&#64;2 = 2/2 = 1.0 (rel=1)</p></li>
<li><p>P&#64;3 = 3/3 = 1.0 (rel=1)</p></li>
<li><p>P&#64;4 = 3/4 = 0.75 (rel=0)</p></li>
<li><p>P&#64;5 = 3/5 = 0.6 (rel=0)</p></li>
</ul>
<p>AP = (1.0×1 + 1.0×1 + 1.0×1 + 0.75×0 + 0.6×0)/3 = 3.0/3 = 1.0</p>
<p><strong>Ranking B:</strong> [R, N, R, N, R]</p>
<p>Precision calculations:</p>
<ul class="simple">
<li><p>P&#64;1 = 1/1 = 1.0 (rel=1)</p></li>
<li><p>P&#64;2 = 1/2 = 0.5 (rel=0)</p></li>
<li><p>P&#64;3 = 2/3 = 0.67 (rel=1)</p></li>
<li><p>P&#64;4 = 2/4 = 0.5 (rel=0)</p></li>
<li><p>P&#64;5 = 3/5 = 0.6 (rel=1)</p></li>
</ul>
<p>AP = (1.0×1 + 0.5×0 + 0.67×1 + 0.5×0 + 0.6×1)/3 = 2.27/3 = 0.76</p>
<p>Despite having the same relevant documents, Ranking B has a lower AP (0.76 vs 1.0) because the relevant documents are ranked lower.</p>
</div>
<p><span class="target" id="newconcept-mean_average_precision"></span><span class="newconcept">Mean Average Precision (MAP)</span> is simply the avearge of AP across all queries.</p>
<div class="math notranslate nohighlight">
\[MAP = \frac{1}{|Q|} \sum_{q \in Q} \text{AP}(q)\]</div>
<p>MAP rewards methods that place relevant documents higher in the ranking and is particularly useful for comparing different ranking algorithms.</p>
<div class="example-green admonition">
<p class="admonition-title">Example: Step-by-Step MAP Calculation</p>
<p>Let’s work through a complete example to illustrate MAP calculation with multiple queries.</p>
<p>Consider a search engine evaluation with three queries: “machine learning frameworks”, “neural networks”, and “data visualization”. For each query, we have an ordered list of search results with relevance judgments (R = relevant, N = not relevant).</p>
<p><strong>Query 1: “machine learning frameworks”</strong></p>
<p>Top 10 ranked results:</p>
<ol class="arabic simple">
<li><p>TensorFlow (R)</p></li>
<li><p>PyTorch (R)</p></li>
<li><p>Weather forecast (N)</p></li>
<li><p>Scikit-learn (R)</p></li>
<li><p>Keras (R)</p></li>
<li><p>Random news article (N)</p></li>
<li><p>Theano (R)</p></li>
<li><p>E-commerce site (N)</p></li>
<li><p>Caffe (R)</p></li>
<li><p>Restaurant review (N)</p></li>
</ol>
<p>Total relevant items for this query: 6
Relevant items are at positions: 1, 2, 4, 5, 7, 9</p>
<p>Let’s calculate precision at each relevant position:</p>
<ul class="simple">
<li><p>P&#64;1 = 1/1 = 1.000 (rel=1)</p></li>
<li><p>P&#64;2 = 2/2 = 1.000 (rel=1)</p></li>
<li><p>P&#64;4 = 3/4 = 0.750 (rel=1)</p></li>
<li><p>P&#64;5 = 4/5 = 0.800 (rel=1)</p></li>
<li><p>P&#64;7 = 5/7 = 0.714 (rel=1)</p></li>
<li><p>P&#64;9 = 6/9 = 0.667 (rel=1)</p></li>
</ul>
<p>AP for Query 1 = (1.000 + 1.000 + 0.750 + 0.800 + 0.714 + 0.667) / 6 = 4.931 / 6 = 0.822</p>
<p><strong>Query 2: “neural networks”</strong></p>
<p>Top 8 ranked results:</p>
<ol class="arabic simple">
<li><p>Deep learning article (R)</p></li>
<li><p>Sports news (N)</p></li>
<li><p>Convolutional networks paper (R)</p></li>
<li><p>Online shop (N)</p></li>
<li><p>Recurrent networks tutorial (R)</p></li>
<li><p>Biography of a celebrity (N)</p></li>
<li><p>Tech blog post (N)</p></li>
<li><p>Backpropagation explanation (R)</p></li>
</ol>
<p>Total relevant items for this query: 4
Relevant items are at positions: 1, 3, 5, 8</p>
<p>Let’s calculate precision at each relevant position:</p>
<ul class="simple">
<li><p>P&#64;1 = 1/1 = 1.000 (rel=1)</p></li>
<li><p>P&#64;3 = 2/3 = 0.667 (rel=1)</p></li>
<li><p>P&#64;5 = 3/5 = 0.600 (rel=1)</p></li>
<li><p>P&#64;8 = 4/8 = 0.500 (rel=1)</p></li>
</ul>
<p>AP for Query 2 = (1.000 + 0.667 + 0.600 + 0.500) / 4 = 2.767 / 4 = 0.692</p>
<p><strong>Query 3: “data visualization”</strong></p>
<p>Top 6 ranked results:</p>
<ol class="arabic simple">
<li><p>Movie review (N)</p></li>
<li><p>Matplotlib tutorial (R)</p></li>
<li><p>D3.js gallery (R)</p></li>
<li><p>Social media post (N)</p></li>
<li><p>Tableau guide (R)</p></li>
<li><p>Visualization best practices (R)</p></li>
</ol>
<p>Total relevant items for this query: 4
Relevant items are at positions: 2, 3, 5, 6</p>
<p>Let’s calculate precision at each relevant position:</p>
<ul class="simple">
<li><p>P&#64;2 = 1/2 = 0.500 (rel=1)</p></li>
<li><p>P&#64;3 = 2/3 = 0.667 (rel=1)</p></li>
<li><p>P&#64;5 = 3/5 = 0.600 (rel=1)</p></li>
<li><p>P&#64;6 = 4/6 = 0.667 (rel=1)</p></li>
</ul>
<p>AP for Query 3 = (0.500 + 0.667 + 0.600 + 0.667) / 4 = 2.434 / 4 = 0.609</p>
<p><strong>MAP Calculation</strong></p>
<p>MAP = (AP_Query1 + AP_Query2 + AP_Query3) / 3
MAP = (0.822 + 0.692 + 0.609) / 3 = 2.123 / 3 = 0.708</p>
<p><strong>Analysis</strong></p>
<p>The MAP score of 0.708 indicates good overall ranking performance across the three queries. Breaking down the results:</p>
<ul class="simple">
<li><p><strong>Query 1 (AP = 0.822)</strong>: Best performance, with relevant items clustered near the top and good precision throughout.</p></li>
<li><p><strong>Query 2 (AP = 0.692)</strong>: Good performance but with some relevant items appearing lower in the ranking.</p></li>
<li><p><strong>Query 3 (AP = 0.609)</strong>: Weakest performance, starting with an irrelevant result and having more inconsistent precision.</p></li>
</ul>
<p>This example illustrates how MAP rewards systems that rank relevant items higher while penalizing those that place irrelevant items at top positions. The use of macro-averaging gives equal weight to each query regardless of how many relevant items it contains, ensuring that performance on all queries contributes equally to the final metric.</p>
</div>
<p>MAP has its counterpart for recall. <span class="target" id="newconcept-mean_average_recall"></span><span class="newconcept">Mean Average Recall (MAR)</span> is the metric for evaluating ranking performance but focusing on recall. While MAP emphasizes precision, MAR measures how well the system retrieves all relevant items across different ranks. MAR also shares a similar interpretation to MAP - related to the Recall-Precision curve and the AUC-RP metric.</p>
<div class="math notranslate nohighlight">
\[AR(q) = \frac{\sum_{k=1}^{n} R(k) \cdot \text{rel}(k)}{\sum_{k=1}^{n} \text{rel}(k)}\]</div>
<p>Where:</p>
<ul class="simple">
<li><p>$q$ is a query.</p></li>
<li><p>$R(k)$ is R&#64;k for query $q$.</p></li>
<li><p>$\text{rel}(k)$ is the relevance score for the item ranked at $k$, typically an indicator function as in MAP.</p></li>
</ul>
<p><span class="target" id="newconcept-mean_average_recall"></span><span class="newconcept">Mean Average Recall (MAR)</span> is then calculated as the average of AR across all queries:</p>
<div class="math notranslate nohighlight">
\[MAR = \frac{1}{|Q|} \sum_{q \in Q} \text{AR}(q)\]</div>
</section>
<section id="normalized-discounted-cumulative-gain-ndcg">
<h2>Normalized Discounted Cumulative Gain (NDCG)<a class="headerlink" href="#normalized-discounted-cumulative-gain-ndcg" title="Link to this heading"></a></h2>
<section id="zipf-s-law">
<h3>Zipf’s Law<a class="headerlink" href="#zipf-s-law" title="Link to this heading"></a></h3>
<p><span class="target" id="newconcept-zipf's_law"></span><span class="newconcept">Zipf’s Law</span> (<a class="reference external" href="https://en.wikipedia.org/wiki/Zipf%27s_law">Wiki</a>) is an empirical law that states that when a list of items are sorted in decreasing order by certain value (e.g., words sorted by frequency in descending order), the value of the $i$-th ranked item from the ordered list is often approximately inversely proportional to $i$:</p>
<div class="math notranslate nohighlight">
\[f(i) \propto \frac{1}{(i+p)^s}\]</div>
<p>where $p$ and $s$ are two parameters.</p>
<p>Zipf’s Law is highly relevant to information retrieval, recommendation systems, and ranking evaluation. Research has shown that user attention to search results follows a similar distribution, with dramatically less attention given to items as their rank increases. While Zipf’s Law suggests a power-law decline ($\frac{1}{(i+p)^s}$), many ranking metrics use logarithmic discounting as a more balanced approximation of this attention drop-off. The <span class="target" id="newconcept-discount_factor"></span><span class="newconcept">discount factor</span> used in DCG and NDCG draws inspiration from this relationship between item position and user attention.</p>
</section>
<section id="dcg">
<h3>DCG<a class="headerlink" href="#dcg" title="Link to this heading"></a></h3>
<p><span class="target" id="newconcept-normalized_discounted_cumulative_gain"></span><span class="newconcept">Normalized Discounted Cumulative Gain (NDCG)</span> is a widely used ranking metric that considers the position of relevant documents. It is based on <span class="target" id="newconcept-discounted_cumulative_gain"></span><span class="newconcept">Discounted Cumulative Gain (DCG)</span>, which assigns higher importance to relevant documents appearing earlier in the ranked list. NDCG and DCG are typically calculated for the top-k items, and are noted as <strong>NDCG&#64;k</strong> and <strong>DCG&#64;k</strong>.</p>
<p>DCG is calculated for the top-k of the ranked list as:</p>
<div class="math notranslate nohighlight">
\[\text{DCG}&#64;k = \sum_{i=1}^{k} \frac{g(i)}{d(i)}\]</div>
<p>Where:</p>
<ul class="simple">
<li><p>$g(i)$ is the <span class="target" id="newconcept-gain_function"></span><span class="newconcept">gain function</span> of the item at position $i$ in the ranked list</p></li>
<li><p>$d(i)$ is the <span class="target" id="newconcept-discount_factor"></span><span class="newconcept">discount factor</span> that reduces the contribution of items at lower positions</p></li>
</ul>
<p>In the canonical formulation, the gain function is simply the relevance score of the item $g(i) = \text{rel}(i)$, and the discount factor is logarithmic $d(i) = \log_2(i+1)$, giving us:</p>
<div class="math notranslate nohighlight">
\[\text{DCG}&#64;k = \sum_{i=1}^{k} \frac{\text{rel}(i)}{\log_2(i+1)}\]</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>About Discount Factor $d(i)$</p>
<p>The <strong>discount factor</strong> in Discounted Cumulative Gain (DCG) can use logarithms with bases other than 2. The choice of denominator affects how quickly relevance is discounted as position increases.
* Higher base (e.g., the natural logarithm with base $e=2.71828$) create a more gradual discount, placing more importance on items deeper in the results.
* Smaller base (e.g., 1.5) creates a steeper discount, severely penalizing lower positions.</p>
<p>The discount factor need not necessarily be a logarithm. It can be <strong>linear discount</strong>, <strong>exponential discount</strong>, or <strong>power discount</strong> (as in the original Zipf’s Law), etc.</p>
</div>
</section>
<section id="ndcg">
<h3>NDCG<a class="headerlink" href="#ndcg" title="Link to this heading"></a></h3>
<p>DCG scores have interpretability issues that make it challenging to use on its own.</p>
<ul class="simple">
<li><p><strong>Query-specific interpretation</strong>: A DCG of 10 might be excellent for a query with few relevant documents but poor for a query with many highly relevant documents.</p></li>
<li><p><strong>Scale &amp; upperbound issue</strong>: Unlike metrics that are normalized between 0 and 1, DCG can grow unbounded depending on the number of relevant documents.</p></li>
</ul>
<p>To address these limitations, NDCG normalizes DCG by the maximum possible DCG for that query:</p>
<p>The <span class="target" id="newconcept-ideal_dcg"></span><span class="newconcept">Ideal DCG (IDCG)</span> is calculated by sorting all relevant items by their relevance scores (highest to lowest) and computing the DCG of this ideal ordering:</p>
<div class="math notranslate nohighlight">
\[\text{IDCG}&#64;k = \sum_{i=1}^{k} \frac{\text{rel}^*(i)}{\log_2(i+1)}\]</div>
<p>Where $\text{rel}^*(i)$ is the relevance score at position $i$ in the ideally ranked list.</p>
<p>NDCG is then defined as:</p>
<div class="math notranslate nohighlight">
\[\text{NDCG}&#64;k = \frac{\text{DCG}&#64;k}{\text{IDCG}&#64;k}\]</div>
<div class="example-green admonition">
<p class="admonition-title">Example: NDCG Calculation</p>
<p>Consider a movie recommendation system that ranks films on a relevance scale:</p>
<ul class="simple">
<li><p>3: Highly relevant</p></li>
<li><p>2: Relevant</p></li>
<li><p>1: Somewhat relevant</p></li>
<li><p>0: Irrelevant</p></li>
</ul>
<p><strong>User Query: “Sci-fi action movies”</strong></p>
<p>System ranking with relevance scores:</p>
<ol class="arabic simple">
<li><p>Star Wars: Episode V (rel=3)</p></li>
<li><p>Blade Runner (rel=3)</p></li>
<li><p>Romantic comedy (rel=0)</p></li>
<li><p>The Matrix (rel=3)</p></li>
<li><p>Avatar (rel=2)</p></li>
</ol>
<p>DCG&#64;5 calculation:</p>
<ul class="simple">
<li><p>DCG&#64;5 = 3/log₂(1+1) + 3/log₂(2+1) + 0/log₂(3+1) + 3/log₂(4+1) + 2/log₂(5+1)</p></li>
<li><p>DCG&#64;5 = 3/1 + 3/1.585 + 0/2 + 3/2.322 + 2/2.585</p></li>
<li><p>DCG&#64;5 = 3 + 1.892 + 0 + 1.292 + 0.774 = 6.958</p></li>
</ul>
<p>The ideal ranking would place all highly relevant (3) items first, followed by relevant (2) items:</p>
<ol class="arabic simple">
<li><p>Star Wars: Episode V (rel=3)</p></li>
<li><p>Blade Runner (rel=3)</p></li>
<li><p>The Matrix (rel=3)</p></li>
<li><p>Avatar (rel=2)</p></li>
<li><p>Romantic comedy (rel=0)</p></li>
</ol>
<p>IDCG&#64;5 calculation:</p>
<ul class="simple">
<li><p>IDCG&#64;5 = 3/log₂(1+1) + 3/log₂(2+1) + 3/log₂(3+1) + 2/log₂(4+1) + 0/log₂(5+1)</p></li>
<li><p>IDCG&#64;5 = 3/1 + 3/1.585 + 3/2 + 2/2.322 + 0/2.585</p></li>
<li><p>IDCG&#64;5 = 3 + 1.892 + 1.5 + 0.861 + 0 = 7.253</p></li>
</ul>
<p>NDCG&#64;5 = DCG&#64;5/IDCG&#64;5 = 6.958/7.253 = 0.959</p>
<p>This high NDCG score of 0.959 indicates that despite ranking one irrelevant item at position 3, the system still performs very well overall, capturing most of the ideal ordering.</p>
</div>
</section>
</section>
<section id="ranking-metrics-for-early-relevance-low-precision-ranking-scenarios">
<h2>Ranking Metrics For Early-Relevance &amp; Low-Precision Ranking Scenarios<a class="headerlink" href="#ranking-metrics-for-early-relevance-low-precision-ranking-scenarios" title="Link to this heading"></a></h2>
<p>Some ranking applications inherently involve a low-precision scenario, which demands metrics that specifically evaluate a system’s ability to present relevant content early in the ranking order.</p>
<ol class="arabic simple">
<li><p>Only one or very few items in a large candidate pool are relevant</p></li>
<li><p>The primary goal is to surface at least one relevant item within top positions</p></li>
</ol>
<p>The following are some concrete scenarios.</p>
<blockquote>
<div><ul class="simple">
<li><p><strong>Voice assistants</strong>: For smart devices like Alexa or Google Home, the screen may be small and low resolution, showing only three items at a time. On headless devices, voice recommendations can typically only present one item before user patience is exhausted.</p></li>
<li><p><strong>Sponsored Ads</strong>: On search engines or social media platforms, there are typically only 2-3 available spots for sponsored results. Advertisers pay premiums for these positions, making the correct ranking critical for monetization.</p></li>
<li><p><strong>Mobile search</strong>: On mobile screens, often only the top a few results appear without scrolling. Users frequently select from just these top results, rarely scrolling past the first few items.</p></li>
<li><p><strong>Fact-based queries</strong>: For questions with a single correct answer (like “Who is the president of France?”), users primarily care about finding that one correct answer quickly rather than seeing multiple relevant documents.</p></li>
<li><p><strong>Featured snippets</strong>: Search engines often display a single “featured snippet” at the top of results for certain queries. The system must correctly identify the most relevant result to feature in this high-visibility position.</p></li>
<li><p><strong>Autocomplete suggestions</strong>: Search bars typically show 4-5 autocomplete suggestions, with users commonly selecting from only the top 1-2 options.</p></li>
</ul>
</div></blockquote>
<section id="mean-reciprocal-rank-mrr">
<h3>Mean Reciprocal Rank (MRR)<a class="headerlink" href="#mean-reciprocal-rank-mrr" title="Link to this heading"></a></h3>
<p><span class="target" id="newconcept-mean_reciprocal_rank"></span><span class="newconcept">Mean Reciprocal Rank (MRR)</span> evaluates ranking quality by measuring how soon the first relevant document appears in the ranked list. It is calculated as the average reciprocal rank of the first relevant result across multiple queries.</p>
<div class="math notranslate nohighlight">
\[MRR = \frac{1}{|Q|} \sum_{q \in Q} \frac{1}{\text{rank}(q)}\]</div>
<p>Where $\text{rank}(q)$ is the position of the first relevant item for query $q$.</p>
<p>In comparison to the other metric <a class="reference internal" href="#newconcept-first_relevant_position"><span class="refconcept">First Relevant Position (FRP)</span></a> introduced below, MRR is less affected by extreme outliers since the reciprocal transformation compresses very large rank values.</p>
</section>
<section id="mean-expected-reciprocal-rank-merr">
<h3>Mean Expected Reciprocal Rank (MERR)<a class="headerlink" href="#mean-expected-reciprocal-rank-merr" title="Link to this heading"></a></h3>
<p><span class="target" id="newconcept-mean_expected_reciprocal_rank"></span><span class="newconcept">Mean Expected Reciprocal Rank (MERR)</span> extends the concept of Mean Reciprocal Rank (MRR) by incorporating relevance score and modeling user behavior (stopping at an ealier item) more realistically.</p>
<p>For a single query, <span class="target" id="newconcept-expected_reciprocal_rank"></span><span class="newconcept">Expected Reciprocal Rank (ERR)</span> is calculated as:</p>
<div class="math notranslate nohighlight">
\[\text{ERR}(q) = \sum_{i=1}^{n} \frac{1}{i} \times P(\text{stop at i})\]</div>
<p>where $P(\text{stop at i})$ is the probability that the user stops at position $i$, which is a function of the document’s relevance and the probability that the user didn’t stop at earlier positions:</p>
<div class="math notranslate nohighlight">
\[P(\text{stop at i}) = P(i) \times \prod_{j=1}^{i-1} (1 - P(j))\]</div>
<p>Here $P(i) \in [0, 1]$ represents the probability that the user examines item at position $i$. It usually simply uses the relevacne scores, i.e., $P(i) = \text{rel}(i)$. For relevance scores not in the range $[0, 1]$, normalization is needed. For example,</p>
<ul>
<li><p><span class="target" id="newconcept-exponential_utility_transformation"></span><span class="newconcept">Exponential Utility Transformation</span></p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\text{rel}^{\text{exponential-utility-normalized}}(i) = \frac{2^{\text{rel}(i)} - 1}{2^R}\]</div>
<p>where $\text{rel}(i)$ is the assigned relevance grade for the document at position $i$, and $R$ is the maximum possible relevance score.</p>
</div></blockquote>
</li>
<li><p><span class="target" id="newconcept-sigmoid_transformation"></span><span class="newconcept">Sigmoid Transformation</span></p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\text{rel}^{\text{sigmoid-normalized}}(i) = \frac{1}{1 + \text{e}^{-\alpha(\text{rel}(i) - \beta)}}\]</div>
<p>where $\alpha$ controls the steepness of the curve and $\beta$ shifts the midpoint. This transformation smoothly maps any range to $(0,1)$ and can better model gradual differences in relevance levels.</p>
</div></blockquote>
</li>
</ul>
<div class="example-green admonition">
<p class="admonition-title">Example: ERR Calculation</p>
<p>Consider a search for “smartphone reviews” with the following ranked results and relevance grades (on a scale of 0-3):</p>
<ol class="arabic simple">
<li><p>Comprehensive smartphone comparison guide (rel_score=3)</p></li>
<li><p>Latest iPhone review (rel_score=2)</p></li>
<li><p>Budget smartphones of 2024 (rel_score=3)</p></li>
<li><p>Smartphone accessories (rel_score=1)</p></li>
<li><p>Laptop reviews (rel_score=0)</p></li>
</ol>
<p>First, convert relevance scores to probabilities (using <a class="reference internal" href="#newconcept-exponential_utility_transformation"><span class="refconcept">Exponential Utility Transformation</span></a>):</p>
<p>rel(1) = (2^3 - 1)/2^3 = 7/8 = 0.875
rel(2) = (2^2 - 1)/2^3 = 3/8 = 0.375
rel(3) = (2^3 - 1)/2^3 = 7/8 = 0.875
rel(4) = (2^1 - 1)/2^3 = 1/8 = 0.125
rel(5) = (2^0 - 1)/2^3 = 0</p>
<p>ERR calculation:</p>
<p>P(stop at 1) = rel(1) = 0.875
P(stop at 2) = rel(2) × (1-rel(1)) = 0.375 × 0.125 = 0.047
P(stop at 3) = rel(3) × (1-rel(1)) × (1-rel(2)) = 0.875 × 0.125 × 0.953 = 0.104
P(stop at 4) = rel(4) × (1-rel(1)) × (1-rel(2)) × (1-rel(3)) = 0.125 × 0.125 × 0.953 × 0.896 = 0.013
P(stop at 5) = rel(5) × … = 0 (since rel(5)=0)</p>
<p>ERR = (1/1 × 0.875) + (1/2 × 0.047) + (1/3 × 0.104) + (1/4 × 0.013) + (1/5 × 0)
ERR = 0.875 + 0.023 + 0.035 + 0.003 = 0.936</p>
<p>This high ERR score (0.936) indicates that users are likely to find a highly relevant result very early in the list, with most users stopping at the first position.</p>
</div>
<p>Mean ERR (MERR) is calculated by averaging ERR across all queries:</p>
<div class="math notranslate nohighlight">
\[\text{MERR} = \frac{1}{|Q|} \sum_{q \in Q} \text{ERR}(q)\]</div>
<p>If we cut off a ranked list to position $k$, we have the <strong>ERR&#64;k</strong> metric, and $MERR&#64;k$ can be calculated accordingly.</p>
</section>
<section id="other-metrics">
<h3>Other Metrics<a class="headerlink" href="#other-metrics" title="Link to this heading"></a></h3>
<p>The following additional metrics are also suited for evaluating ranking performance in these contexts.</p>
<p><span class="target" id="newconcept-hit&#64;k"></span><span class="newconcept">Hit&#64;k</span> is a binary metric that evaluates whether at least one relevant item appears in the top-k ranked results:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\text{Hit&#64;k} =
\begin{cases}
1 &amp; \text{if any relevant item appears in top-k results} \\
0 &amp; \text{otherwise}
\end{cases}\end{split}\]</div>
<p><span class="target" id="newconcept-mean_rank"></span><span class="newconcept">Mean Rank (MR)</span> measures the average position of relevant items in the ranked results.</p>
<div class="math notranslate nohighlight">
\[\text{MR} = \frac{1}{|Q|} \sum_{q \in Q} \frac{1}{|R_q|} \sum_{r \in R_q} \text{rank}(r)\]</div>
<p>Where:
- $R_q$ is the set of relevant items for query $q$
- $\text{rank}(r)$ is the position of relevant item $r$ in the ranked list</p>
<p><span class="target" id="newconcept-first_relevant_position"></span><span class="newconcept">First Relevant Position (FRP)</span> focuses solely on the rank of the first relevant item (similar to MRR):</p>
<div class="math notranslate nohighlight">
\[\text{FRP} = \frac{1}{|Q|} \sum_{q \in Q} \min_{r \in R_q} \text{rank}(r)\]</div>
<p>Compared to <a class="reference internal" href="#newconcept-mean_reciprocal_rank"><span class="refconcept">Mean Reciprocal Rank (MRR)</span></a> and <cite>Mean Expected Reciprocal Rank (MERR)</cite>, both <span class="underline">MR and FRP offer the benefit of being straightforward and immediately interpretable</span>, making them easier to understand for stakeholders without technical backgrounds. However, MR and FRP can be heavily skewed by a few queries where relevant items appear very late in the ranking.
A simple technique to address this limitation is to calculate <strong>MR&#64;k</strong> and <strong>FRP&#64;k</strong>, which only consider items within the top-k positions. For FRP&#64;k specifically, a fixed constant “rank” value (e.g., k+1) can be assigned if no relevant item appears in the top-k results, creating a bounded metric while maintaining interpretability.</p>
</section>
</section>
<section id="summary">
<h2>Summary<a class="headerlink" href="#summary" title="Link to this heading"></a></h2>
<p>This chapter examined ranking evaluation metrics for ML/AI systems that return ordered lists of items.</p>
<section id="position-based-metrics">
<h3>Position-Based Metrics<a class="headerlink" href="#position-based-metrics" title="Link to this heading"></a></h3>
<blockquote>
<div><ul class="simple">
<li><p><strong>Precision&#64;k</strong>: Proportion of relevant items in the top-k results</p></li>
<li><p><strong>Recall&#64;k</strong>: Proportion of all relevant items found in the top-k results</p></li>
<li><p><strong>Mean Reciprocal Rank (MRR)</strong> and <strong>First Relevant Position (FRP)</strong>: Focuses on the position of the first relevant item</p></li>
<li><p><strong>Mean Expected Reciprocal Rank (MERR)</strong>: Focuses on early-item relevance, and incorporates user behavior modeling</p></li>
<li><p><strong>Hit&#64;k</strong>: Binary metric indicating if at least one relevant item appears in top-k</p></li>
</ul>
</div></blockquote>
</section>
<section id="comprehensive-ranking-metrics">
<h3>Comprehensive Ranking Metrics<a class="headerlink" href="#comprehensive-ranking-metrics" title="Link to this heading"></a></h3>
<blockquote>
<div><ul class="simple">
<li><p><strong>Mean Average Precision (MAP)</strong> and <strong>Mean Average Recall (MAR)</strong>: Averages precision/recall at each position where a relevant item appears</p></li>
<li><p><strong>Normalized Discounted Cumulative Gain (NDCG)</strong>: Evaluates both relevance and position with discount factors</p></li>
<li><p><strong>Mean Rank</strong>: Average position of relevant items in the results</p></li>
</ul>
</div></blockquote>
</section>
<section id="low-precision-ranking-scenarios">
<h3>Low-Precision Ranking Scenarios<a class="headerlink" href="#low-precision-ranking-scenarios" title="Link to this heading"></a></h3>
<blockquote>
<div><ul class="simple">
<li><p>Few relevant items in a large candidate pool</p></li>
<li><p>Primary goal is surfacing at least one relevant item in top positions</p></li>
<li><p>MRR, Hit&#64;k, and First Relevant Position are particularly well-suited</p></li>
</ul>
</div></blockquote>
</section>
<section id="best-practices">
<h3>Best Practices<a class="headerlink" href="#best-practices" title="Link to this heading"></a></h3>
<blockquote>
<div><ul class="simple">
<li><p>Choose metrics aligned with how users interact with results</p></li>
<li><p>Consider the constraints of the interface (e.g., visibility constraints on mobile/smarthome devices)</p></li>
</ul>
</div></blockquote>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="basic_evaluation.html" class="btn btn-neutral float-left" title="Basic Evaluation" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="diversity_evaluation.html" class="btn btn-neutral float-right" title="Diversity Evaluation" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2025, Tony.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>