

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Reinforcement Learning &mdash; MLAI 0.1 documentation</title>
      <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=80d5e7a1" />
      <link rel="stylesheet" type="text/css" href="../../_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="../../_static/custom.css?v=e159ab47" />

  
    <link rel="canonical" href="https://xinliyu.github.io/ML-AI-From-Theory-To-Industry/modeling/classic_modeling/03_reinforcement_learning.html" />
      <script src="../../_static/jquery.js?v=5d32c60e"></script>
      <script src="../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../../_static/documentation_options.js?v=2709fde1"></script>
      <script src="../../_static/doctools.js?v=9bcbadda"></script>
      <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
      <script src="../../_static/foldable_admonitions.js?v=351fa817"></script>
      <script>window.MathJax = {"tex": {"inlineMath": [["$", "$"]], "displayMath": [["$$", "$$"]]}, "options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
      <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      <script src="../../_static/js/mathjax-config.js?v=c54ad740"></script>
    <script src="../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Evaluation" href="../../evaluation/index.html" />
    <link rel="prev" title="Transformer Models" href="02_transformer_models.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../index.html" class="icon icon-home">
            MLAI
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../system_design/index.html">ML/AI Systen Design</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../index.html">Modeling</a><ul class="current">
<li class="toctree-l2 current"><a class="reference internal" href="index.html">Classic Modeling</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="01_data_preparation.html">Data Preparation</a></li>
<li class="toctree-l3"><a class="reference internal" href="02_transformer_models.html">Transformer Models</a></li>
<li class="toctree-l3 current"><a class="current reference internal" href="#">Reinforcement Learning</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#policy-optimization">Policy Optimization</a><ul>
<li class="toctree-l5"><a class="reference internal" href="#reward-value-functions">Reward &amp; Value Functions</a></li>
<li class="toctree-l5"><a class="reference internal" href="#deep-policy-networks">Deep Policy Networks</a><ul>
<li class="toctree-l6"><a class="reference internal" href="#policy-networks">Policy Networks</a></li>
<li class="toctree-l6"><a class="reference internal" href="#multi-armed-bandit">Multi-Armed Bandit</a></li>
<li class="toctree-l6"><a class="reference internal" href="#deep-q-networks">Deep Q-Networks</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../evaluation/index.html">Evaluation</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">MLAI</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../index.html">Modeling</a></li>
          <li class="breadcrumb-item"><a href="index.html">Classic Modeling</a></li>
      <li class="breadcrumb-item active">Reinforcement Learning</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../_sources/modeling/classic_modeling/03_reinforcement_learning.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="reinforcement-learning">
<h1>Reinforcement Learning<a class="headerlink" href="#reinforcement-learning" title="Link to this heading"></a></h1>
<p><span class="target" id="newconcept-reinforcement_learning"></span><span class="newconcept">Reinforcement Learning (RL)</span> is a ML approach that builds a model (known as <span class="target" id="newconcept-policy"></span><span class="newconcept">policy</span> in RL context, denoted by $\pi$) to decide the ML/AI system’s next action. We use $x_t in X$ to denote the system’s current <span class="target" id="newconcept-state"></span><span class="newconcept">state</span> at time $t$, where $x_t$ consists of all the context the system can observe at this time $t$ (before any further action), and $X$ is the <span class="target" id="newconcept-state_space"></span><span class="newconcept">state space</span>. We let $a_t=\pi(x_t), a_t \in A$ be the <span class="target" id="newconcept-action"></span><span class="newconcept">action</span> suggested by the policy $\pi$, where $A$ is the <span class="target" id="newconcept-action_space"></span><span class="newconcept">action space</span> (the set of all possible actions under policy $\pi$). For example,</p>
<ul class="simple">
<li><p>In the context of language models, the current state $x_t$ can be the input tokens, and the next action $a_t$ can be the next token, and the action space $A$ is the vocabulary.</p></li>
<li><p>In the context of search/recommendation/Ads system, the current state $x_t$ is all the historical and runtime context the system can observe (user profile/history, runtiem session signals, etc., see also <a class="reference external" href="../../system_design/recommendation_and_ads_system_design/01_recommendation_system_design.html">Recommendation ML/AI System Design</a>), and the next action $a_t$ represents the results presented to the user, and the action space is the infinitely many result combinations.</p></li>
</ul>
<p>Mathematically, a policy is a probability distribution $p$ over the action space $A$ given a known state $x_t$ where, i.e.,</p>
<div class="math notranslate nohighlight">
\[\pi(x_t) \sim p(a_t|x_t), a_t \in A, x_t \in X\]</div>
<section id="policy-optimization">
<h2>Policy Optimization<a class="headerlink" href="#policy-optimization" title="Link to this heading"></a></h2>
<section id="reward-value-functions">
<h3>Reward &amp; Value Functions<a class="headerlink" href="#reward-value-functions" title="Link to this heading"></a></h3>
<p><span class="target" id="newconcept-reward_function"></span><span class="newconcept">Reward Function</span>, <span class="target" id="newconcept-value_function"></span><span class="newconcept">Value Function</span> (also called <span class="target" id="newconcept-state_value_function"></span><span class="newconcept">State Value Function</span>) and <span class="target" id="newconcept-q-function"></span><span class="newconcept">Q-Function</span> (also called <span class="target" id="newconcept-action-value_function"></span><span class="newconcept">Action-Value Function</span>) are three fundamental and related concepts.</p>
<ul class="simple">
<li><p><strong>Reward Function</strong> $R(x_t, a_t)$: Provides immediate reward about a single action $a_t$ in a given state $x$.</p></li>
<li><p><strong>Value Function</strong> $V(x_t)$: Estimates the total expected future rewards from being in state $x_t$. More formally, the value function is defined as the expected sum of discounted future rewards:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[V(x_t) = \mathbb{E}[\sum_{k=0}^{\infty} \gamma^k R(x_{t+k}, a_{t+k}) | x_t]\]</div>
<p>where $\gamma$ is the discount factor (typically $&lt;1$). The expectation $\mathbb{E}$ is taken over all possible future trajectories.</p>
<ul class="simple">
<li><p><strong>Q-Function</strong> $Q(x_t, a_t)$: Estimates the total expected future rewards from being in state $x_t$ and taken a specific action $a_t$, defined as:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[Q(x_t, a_t) = \mathbb{E}[\sum_{k=0}^{\infty} \gamma^k R(x_{t+k}, a_{t+k}) | x_t, a_t]\]</div>
<p>The relationship between Q-values and V-values is:</p>
<div class="math notranslate nohighlight">
\[V(x_t) = \mathbb{E}_{a_t \sim \pi}[Q(x_t,a_t)] = \sum_{a \in A} \pi(a|x_t)Q(x_t,a)\]</div>
<p>In other words:</p>
<ul class="simple">
<li><p>$Q(x_t,a_t)$ tells us the value of taking a specific action $a_t$ at state $x_t$.</p></li>
<li><p>$V(x_t)$ is the weighted average of Q-values over all possible future actions according to the policy.</p></li>
</ul>
<p>The value function and Q-function help the model consider long-term consequences rather than just immediate rewards. See the following Example: Chatbot Debugging Assistant.</p>
<div class="example-green admonition">
<p class="admonition-title">Chatbot Debugging Assistant</p>
<p>Consider a chatbot trained to help users debug code. The immediate reward function evaluates each response on:</p>
<div class="math notranslate nohighlight">
\[R(x_t, a_t) = \text{politeness}(a_t) + \text{relevance}(a_t)\]</div>
<p>where scores range from 0 to 1 for each term.</p>
<p>Consider two possible responses to “My code is giving an error”:</p>
<p><strong>Response A:</strong> “Thank you for reaching out about your code error. How can I assist you today?”</p>
<blockquote>
<div><ul class="simple">
<li><p>Immediate reward calculation:</p>
<ul>
<li><p>$\text{politeness} = 1.0$ (very polite)</p></li>
<li><p>$\text{relevance} = 0.3$ (generic, no debugging progress)</p></li>
<li><p>$R(x_t, a_t) = 1.0 + 0.3 = 1.3$</p></li>
</ul>
</li>
<li><p>Expected trajectory (with $\gamma = 0.9$):</p>
<ul>
<li><p>$t+0$: $R = 1.3$ (initial response, polite but generic)</p></li>
<li><p>$t+1$: $R = 1.2$ (user explains error)</p></li>
<li><p>$t+2$: $R = 1.7$ (bot asks for stack trace: politeness=0.7, relevance=1.0)</p></li>
<li><p>$t+3$: $R = 1.5$ (user provides stack trace)</p></li>
<li><p>$t+4$: $R = 1.8$ (bot begins actual debugging)</p></li>
</ul>
</li>
</ul>
</div></blockquote>
<ul>
<li><p>Value calculation from initial state:</p>
<div class="math notranslate nohighlight">
\[\begin{split}V(x_t) &amp;= \mathbb{E}[\sum_{k=0}^{\infty} \gamma^k R(x_{t+k}, a_{t+k}) | x_t] \\
&amp;= 1.3 + 0.9(1.2) + 0.9^2(1.7) + 0.9^3(1.5) + 0.9^4(1.8) \\
&amp;= 1.3 + 1.08 + 1.377 + 1.097 + 1.190 \\
&amp;= 6.044\end{split}\]</div>
</li>
<li><p>Q-function calculation for this state-action pair:</p>
<div class="math notranslate nohighlight">
\[\begin{split}Q(x_t, a_t) &amp;= \mathbb{E}[\sum_{k=0}^{\infty} \gamma^k R(x_{t+k}, a_{t+k}) | x_t, a_t] \\
&amp;= 1.3 + 0.9(1.2) + 0.9^2(1.7) + 0.9^3(1.5) + 0.9^4(1.8) \\
&amp;= 1.3 + 1.08 + 1.377 + 1.097 + 1.190 \\
&amp;= 6.044\end{split}\]</div>
</li>
</ul>
<p><strong>Response B:</strong> “Could you share the error message and stack trace you’re seeing?”</p>
<ul>
<li><p>Immediate reward calculation:</p>
<blockquote>
<div><ul class="simple">
<li><p>$\text{politeness} = 0.7$ (direct but still professional)</p></li>
<li><p>$\text{relevance} = 1.0$ (immediately useful for debugging)</p></li>
<li><p>$R(x_t, a_t) = 0.7 + 1.0 = 1.7$</p></li>
</ul>
</div></blockquote>
</li>
<li><p>Expected trajectory (with $\gamma = 0.9$):</p>
<blockquote>
<div><ul class="simple">
<li><p>$t+0$: $R = 1.7$ (direct request for stack trace)</p></li>
<li><p>$t+1$: $R = 1.5$ (user provides stack trace)</p></li>
<li><p>$t+2$: $R = 1.8$ (bot begins debugging with complete info)</p></li>
<li><p>$t+3$: $R = 1.7$ (debugging progress)</p></li>
<li><p>$t+4$: $R = 1.6$ (resolution)</p></li>
</ul>
</div></blockquote>
</li>
<li><p>Value calculation from initial state:</p>
<div class="math notranslate nohighlight">
\[\begin{split}V(x_t) &amp;= \mathbb{E}[\sum_{k=0}^{\infty} \gamma^k R(x_{t+k}, a_{t+k}) | x_t] \\
&amp;= 1.7 + 0.9(1.5) + 0.9^2(1.8) + 0.9^3(1.7) + 0.9^4(1.6) \\
&amp;= 1.7 + 1.35 + 1.458 + 1.241 + 1.058 \\
&amp;= 6.807\end{split}\]</div>
</li>
<li><p>Q-function calculation for this state-action pair:</p>
<div class="math notranslate nohighlight">
\[\begin{split}Q(x_t, a_t) &amp;= \mathbb{E}[\sum_{k=0}^{\infty} \gamma^k R(x_{t+k}, a_{t+k}) | x_t, a_t] \\
&amp;= 1.7 + 0.9(1.5) + 0.9^2(1.8) + 0.9^3(1.7) + 0.9^4(1.6) \\
&amp;= 1.7 + 1.35 + 1.458 + 1.241 + 1.058 \\
&amp;= 6.807\end{split}\]</div>
</li>
</ul>
<p>Note that in this simplified example with deterministic transitions and a single trajectory per action, the Q-value equals the Value since there’s no uncertainty in the trajectories. In a real system with stochastic transitions, Q-function would evaluate the expected rewards conditioned on taking action a_t in state x_t, while Value function would evaluate expected rewards under the policy’s action choices.</p>
<p>Response B achieves a higher cumulative value (6.807 &gt; 6.044) because it leads to a trajectory with higher future rewards through more efficient problem-solving. While both responses eventually get to asking for the stack trace, Response B does so immediately, leading to faster problem resolution. This demonstrates how considering long-term value can help select actions that might not maximize immediate politeness but lead to more efficient problem-solving.</p>
</div>
<p>Another related concept is <span class="target" id="newconcept-advantage_estimation"></span><span class="newconcept">Advantage Estimation</span>, denoted by $\hat{A}_t$, and estimates how much better or worse a particular action brings in comparison to the current state. It is computed as:</p>
<div class="math notranslate nohighlight">
\[\hat{A}_t(x_t, a_t) = Q(x_t, a_t) - V(x_t)\]</div>
<p>In reality, if it is not convenient to estimate $V(x)$ (for example there isn’t another model for $V$), then the advantage can be instead calculated as:</p>
<div class="math notranslate nohighlight">
\[\hat{A}_t = Q(x_t, a_t) - Q(x_{t-1}, a_{t-1})\]</div>
<p>where $a_{t-1}$ is the previous action that already happened, and $Q(x_{t-1}, a_{t-1})$ is the previous value.</p>
</section>
<section id="deep-policy-networks">
<h3>Deep Policy Networks<a class="headerlink" href="#deep-policy-networks" title="Link to this heading"></a></h3>
<p><span class="target" id="newconcept-deep_reinforcement_learning"></span><span class="newconcept">Deep Reinforcement Learning (Deep RL)</span> leverages deep neural netowrks for RL. There are two major types, <span class="target" id="newconcept-deep_policy_networks"></span><span class="newconcept">Deep Policy Networks</span> to model both policy $\pi(a_t|x_t)$, and <span class="target" id="newconcept-deep_q-networks"></span><span class="newconcept">Deep Q-Networks (DQN)</span> to model long-term value. Although both are <span class="target" id="newconcept-action-centric_modeling"></span><span class="newconcept">action-centric modeling</span>, they have key difference in their training objectives in theory.</p>
<ul class="simple">
<li><p>The training targets/labels for a deep policy network is the best next action(s).</p></li>
<li><p>The training targets for a deep-Q network is the long-term value.</p></li>
</ul>
<p>However, in modern deep RL, <strong class="underline-bold">the boundary between a policy network and a Q-network have been significantly blurred</strong>. Many “policy networks” in science publications and materials that target long-term values are actually Q-networks.</p>
<ul class="simple">
<li><p>If we already know which next action has best long-term value, we can use these long-term optimal actions as labels to train a policy network directly.</p></li>
<li><p>Conversely, if our interest primarily lies in identifying optimal actions rather than explicitly computing their long-term values, normalizing the toal Q-values across all actions as $1$ can turn a Q-network into a probability distribution over actions $p(x_t, a_t) = p(a_t|x_t)$, given that current state $x_t$ is usually known and thus $p(x_t) = 1$. This effectively makes the Q-network behave like a policy - a probability distribution $p(a_t|x_t)$.</p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Value function is not often leveraged for deep RL for two main reasons:</p>
<ul class="simple">
<li><p>It is not action-centric, not suitable for most modern ML/AI applications.</p></li>
<li><p>Real-world ML/AI systems typically have vast or infinite state spaces (e.g., extensive user profiles, session contexts, etc.), making direct evaluation of every possible state infeasible.</p></li>
</ul>
</div>
<section id="policy-networks">
<h4>Policy Networks<a class="headerlink" href="#policy-networks" title="Link to this heading"></a></h4>
</section>
<section id="multi-armed-bandit">
<h4>Multi-Armed Bandit<a class="headerlink" href="#multi-armed-bandit" title="Link to this heading"></a></h4>
</section>
<section id="deep-q-networks">
<h4>Deep Q-Networks<a class="headerlink" href="#deep-q-networks" title="Link to this heading"></a></h4>
</section>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="02_transformer_models.html" class="btn btn-neutral float-left" title="Transformer Models" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="../../evaluation/index.html" class="btn btn-neutral float-right" title="Evaluation" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2025, Tony.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>