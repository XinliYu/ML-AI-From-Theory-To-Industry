

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Reinforcement Learning &mdash; MLAI 0.1 documentation</title>
      <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=80d5e7a1" />
      <link rel="stylesheet" type="text/css" href="../../_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="../../_static/custom.css?v=3eba48d4" />

  
    <link rel="canonical" href="https://xinliyu.github.io/ML-AI-From-Theory-To-Industry/modeling/classic_modeling/04_reinforcement_learning.html" />
      <script src="../../_static/jquery.js?v=5d32c60e"></script>
      <script src="../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../../_static/documentation_options.js?v=2709fde1"></script>
      <script src="../../_static/doctools.js?v=9bcbadda"></script>
      <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
      <script src="../../_static/foldable_admonitions.js?v=351fa817"></script>
      <script>window.MathJax = {"tex": {"inlineMath": [["$", "$"]], "displayMath": [["$$", "$$"]]}, "options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
      <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      <script src="../../_static/js/mathjax-config.js?v=c54ad740"></script>
      <script src="https://unpkg.com/react@17/umd/react.production.min.js"></script>
      <script src="https://unpkg.com/react-dom@17/umd/react-dom.production.min.js"></script>
    <script src="../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Evaluation" href="../../evaluation/index.html" />
    <link rel="prev" title="Supervised Learning" href="03_supervised_learning.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../index.html" class="icon icon-home">
            MLAI
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../system_design/index.html">ML/AI Systen Design</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../index.html">Modeling</a><ul class="current">
<li class="toctree-l2 current"><a class="reference internal" href="index.html">Classic Modeling</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="01_data_preparation.html">Data Preparation</a></li>
<li class="toctree-l3"><a class="reference internal" href="02_transformer_models.html">Transformer Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="03_supervised_learning.html">Supervised Learning</a></li>
<li class="toctree-l3 current"><a class="current reference internal" href="#">Reinforcement Learning</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#reward-value-functions">Reward &amp; Value Functions</a></li>
<li class="toctree-l4"><a class="reference internal" href="#deep-policy-networks">Deep Policy Networks</a><ul>
<li class="toctree-l5"><a class="reference internal" href="#multi-armed-bandits-mabs">Multi-Armed Bandits (MABs)</a><ul>
<li class="toctree-l6"><a class="reference internal" href="#modeling-without-explicit-uncertainty">Modeling Without Explicit Uncertainty</a></li>
<li class="toctree-l6"><a class="reference internal" href="#mbas-exploration-strategies">MBAs Exploration Strategies</a></li>
<li class="toctree-l6"><a class="reference internal" href="#exploration-with-explicit-uncertainty-modeling">Exploration With Explicit Uncertainty Modeling</a></li>
<li class="toctree-l6"><a class="reference internal" href="#upper-confidence-bound-ucb">Upper Confidence Bound (UCB)</a></li>
<li class="toctree-l6"><a class="reference internal" href="#thompson-sampling">Thompson Sampling</a></li>
<li class="toctree-l6"><a class="reference internal" href="#contextual-bandits">4. Contextual Bandits</a></li>
<li class="toctree-l6"><a class="reference internal" href="#advanced-mab-techniques">Advanced MAB Techniques</a></li>
</ul>
</li>
<li class="toctree-l5"><a class="reference internal" href="#training-objectives">Training Objectives</a></li>
<li class="toctree-l5"><a class="reference internal" href="#implementation-with-neural-thompson-sampling">Implementation with Neural Thompson Sampling</a></li>
<li class="toctree-l5"><a class="reference internal" href="#handling-combinatorial-actions-and-exploration">Handling Combinatorial Actions and Exploration</a></li>
<li class="toctree-l5"><a class="reference internal" href="#continuous-learning-and-adaptation">Continuous Learning and Adaptation</a><ul>
<li class="toctree-l6"><a class="reference internal" href="#policy-networks">Policy Networks</a></li>
<li class="toctree-l6"><a class="reference internal" href="#deep-q-networks">Deep Q-Networks</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../evaluation/index.html">Evaluation</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">MLAI</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../index.html">Modeling</a></li>
          <li class="breadcrumb-item"><a href="index.html">Classic Modeling</a></li>
      <li class="breadcrumb-item active">Reinforcement Learning</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../_sources/modeling/classic_modeling/04_reinforcement_learning.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="reinforcement-learning">
<h1>Reinforcement Learning<a class="headerlink" href="#reinforcement-learning" title="Link to this heading"></a></h1>
<p><span class="target" id="newconcept-reinforcement_learning"></span><span class="newconcept">Reinforcement Learning (RL)</span> is a ML approach that builds a model (known as <span class="target" id="newconcept-policy"></span><span class="newconcept">policy</span> in RL context, denoted by $\pi$) to decide the ML/AI system’s next action (in RL context, a ML/AI system is also called an <span class="target" id="newconcept-agent"></span><span class="newconcept">agent</span>). We use $x_t in X$ to denote the system’s current <span class="target" id="newconcept-state"></span><span class="newconcept">state</span> at time $t$, where $x_t$ consists of all the context the system can observe at this time $t$ (before any further action), and $X$ is the <span class="target" id="newconcept-state_space"></span><span class="newconcept">state space</span>. We let $a_t=\pi(x_t), a_t \in A$ be the <span class="target" id="newconcept-action"></span><span class="newconcept">action</span> suggested by the policy $\pi$, where $A$ is the <span class="target" id="newconcept-action_space"></span><span class="newconcept">action space</span> (the set of all possible actions under policy $\pi$). For example,</p>
<ul class="simple">
<li><p>In the context of language models, the current state $x_t$ can be the input tokens, and the next action $a_t$ can be the next token, and the action space $A$ is the vocabulary.</p></li>
<li><p>In the context of search/recommendation/Ads system, the current state $x_t$ is all the historical and runtime context the system can observe (user profile/history, runtiem session signals, etc., see also <a class="reference external" href="../../system_design/recommendation_and_ads_system_design/01_recommendation_system_design.html">Recommendation ML/AI System Design</a>), and the next action $a_t$ represents the results presented to the user, and the action space is the infinitely many result combinations.</p></li>
</ul>
<p>Mathematically, a policy is a probability distribution $p$ over the action space $A$ given a known state $x_t$ where, i.e.,</p>
<div class="math notranslate nohighlight">
\[\pi(x_t) \sim p(a_t|x_t), a_t \in A, x_t \in X\]</div>
<section id="reward-value-functions">
<h2>Reward &amp; Value Functions<a class="headerlink" href="#reward-value-functions" title="Link to this heading"></a></h2>
<p><span class="target" id="newconcept-reward_function"></span><span class="newconcept">Reward Function</span>, <span class="target" id="newconcept-value_function"></span><span class="newconcept">Value Function</span> (also called <span class="target" id="newconcept-state_value_function"></span><span class="newconcept">State Value Function</span>) and <span class="target" id="newconcept-q-function"></span><span class="newconcept">Q-Function</span> (also called <span class="target" id="newconcept-action-value_function"></span><span class="newconcept">Action-Value Function</span>) are three fundamental and related concepts.</p>
<ul class="simple">
<li><p><strong>Reward Function</strong> $R(x_t, a_t)$: Provides immediate reward after a single action $a_t$ is taken at a given state $x_t$.</p></li>
<li><p><strong>Value Function</strong> $V(x_t)$: Estimates the total expected future rewards from being in state $x_t$. More formally, the value function is defined as the expected sum of discounted future rewards:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[V(x_t) = \mathbb{E}[\sum_{k=0}^{\infty} \gamma^k R(x_{t+k}, a_{t+k}) | x_t]\]</div>
<p>where $\gamma$ is the discount factor (typically $&lt;1$). The expectation $\mathbb{E}$ is taken over all possible future trajectories.</p>
<ul class="simple">
<li><p><strong>Q-Function</strong> $Q(x_t, a_t)$: Estimates the total expected future rewards from being in state $x_t$ and taken a specific action $a_t$, defined as:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[Q(x_t, a_t) = \mathbb{E}[\sum_{k=0}^{\infty} \gamma^k R(x_{t+k}, a_{t+k}) | x_t, a_t]\]</div>
<p>The relationship between Q-values and V-values is:</p>
<div class="math notranslate nohighlight">
\[V(x_t) = \mathbb{E}_{a_t \sim \pi}[Q(x_t,a_t)] = \sum_{a \in A} \pi(a|x_t)Q(x_t,a)\]</div>
<p>In other words:</p>
<ul class="simple">
<li><p>$Q(x_t,a_t)$ tells us the value of taking a specific action $a_t$ at state $x_t$.</p></li>
<li><p>$V(x_t)$ is the weighted average of Q-values over all possible future actions according to the policy.</p></li>
</ul>
<p>The value function and Q-function enable RL models to consider long-term consequences rather than just immediate rewards, supporting a crucial feature for RL - <span class="target" id="newconcept-exploration-exploitation_tradeoff"></span><span class="newconcept">Exploration-Exploitation Tradeoff</span>:</p>
<blockquote>
<div><ul class="simple">
<li><p><strong>Exploration</strong>: Trying new actions to discover potentially better strategies</p></li>
<li><p><strong>Exploitation</strong>: Using known good strategies to maximize rewards</p></li>
</ul>
</div></blockquote>
<p>This trade-off is crucial for real-world business applications. The following example of a “Chatbot Debugging Assistant” demonstrates how an immediately higher reward does not necessarily lead to the best long-term outcome.</p>
<ul class="simple">
<li><p>While exploitation functions like a classic supervised learning approach that leverages existing knowledge to obtain immediate rewards, exploration enables the RL framework to <strong class="underline-bold">discover valuable actions that may not be immediately apparent or sufficiently represented in the training data</strong>.</p></li>
<li><p>These new observations can <strong class="underline-bold">enhance the model’s breadth and diversity of knowledge</strong>, leading to improved future rewards compared to solely exploiting current knowledge without exploring underrepresented actions.</p></li>
<li><p>Therefore, the RL approach is typically maximizing the <span class="target" id="newconcept-long-term_cumulative_reward"></span><span class="newconcept">Long-Term Cumulative Reward</span> over time rather than focusing solely on the immediate reward, through the exploration-exploitation trade-off.</p></li>
<li><p>In practice, this trade-off is <strong class="underline-bold">typically controlled by RL framework’s hyper-parameters</strong>.</p></li>
</ul>
<div class="example-green admonition">
<p class="admonition-title">Example: Chatbot Debugging Assistant</p>
<p>Consider a chatbot trained to help users debug code. The immediate reward function evaluates each response on:</p>
<div class="math notranslate nohighlight">
\[R(x_t, a_t) = \text{politeness}(a_t) + \text{relevance}(a_t)\]</div>
<p>where scores range from 0 to 1 for each term.</p>
<p>Consider two possible responses to “My code is giving an error”:</p>
<p><strong>Response A:</strong> “Thank you for reaching out about your code error. How can I assist you today?”</p>
<blockquote>
<div><ul class="simple">
<li><p>Immediate reward calculation:</p>
<ul>
<li><p>$\text{politeness} = 1.0$ (very polite)</p></li>
<li><p>$\text{relevance} = 0.3$ (generic, no debugging progress)</p></li>
<li><p>$R(x_t, a_t) = 1.0 + 0.3 = 1.3$</p></li>
</ul>
</li>
<li><p>Expected trajectory (with $\gamma = 0.9$):</p>
<ul>
<li><p>$t+0$: $R = 1.3$ (initial response, polite but generic)</p></li>
<li><p>$t+1$: $R = 1.2$ (user explains error)</p></li>
<li><p>$t+2$: $R = 1.7$ (bot asks for stack trace: politeness=0.7, relevance=1.0)</p></li>
<li><p>$t+3$: $R = 1.5$ (user provides stack trace)</p></li>
<li><p>$t+4$: $R = 1.8$ (bot begins actual debugging)</p></li>
</ul>
</li>
</ul>
</div></blockquote>
<ul>
<li><p>Value calculation from initial state:</p>
<div class="math notranslate nohighlight">
\[\begin{split}V(x_t) &amp;= \mathbb{E}[\sum_{k=0}^{\infty} \gamma^k R(x_{t+k}, a_{t+k}) | x_t] \\
&amp;= 1.3 + 0.9(1.2) + 0.9^2(1.7) + 0.9^3(1.5) + 0.9^4(1.8) \\
&amp;= 1.3 + 1.08 + 1.377 + 1.097 + 1.190 \\
&amp;= 6.044\end{split}\]</div>
</li>
<li><p>Q-function calculation for this state-action pair:</p>
<div class="math notranslate nohighlight">
\[\begin{split}Q(x_t, a_t) &amp;= \mathbb{E}[\sum_{k=0}^{\infty} \gamma^k R(x_{t+k}, a_{t+k}) | x_t, a_t] \\
&amp;= 1.3 + 0.9(1.2) + 0.9^2(1.7) + 0.9^3(1.5) + 0.9^4(1.8) \\
&amp;= 1.3 + 1.08 + 1.377 + 1.097 + 1.190 \\
&amp;= 6.044\end{split}\]</div>
</li>
</ul>
<p><strong>Response B:</strong> “Could you share the error message and stack trace you’re seeing?”</p>
<ul>
<li><p>Immediate reward calculation:</p>
<blockquote>
<div><ul class="simple">
<li><p>$\text{politeness} = 0.7$ (direct but still professional)</p></li>
<li><p>$\text{relevance} = 1.0$ (immediately useful for debugging)</p></li>
<li><p>$R(x_t, a_t) = 0.7 + 1.0 = 1.7$</p></li>
</ul>
</div></blockquote>
</li>
<li><p>Expected trajectory (with $\gamma = 0.9$):</p>
<blockquote>
<div><ul class="simple">
<li><p>$t+0$: $R = 1.7$ (direct request for stack trace)</p></li>
<li><p>$t+1$: $R = 1.5$ (user provides stack trace)</p></li>
<li><p>$t+2$: $R = 1.8$ (bot begins debugging with complete info)</p></li>
<li><p>$t+3$: $R = 1.7$ (debugging progress)</p></li>
<li><p>$t+4$: $R = 1.6$ (resolution)</p></li>
</ul>
</div></blockquote>
</li>
<li><p>Value calculation from initial state:</p>
<div class="math notranslate nohighlight">
\[\begin{split}V(x_t) &amp;= \mathbb{E}[\sum_{k=0}^{\infty} \gamma^k R(x_{t+k}, a_{t+k}) | x_t] \\
&amp;= 1.7 + 0.9(1.5) + 0.9^2(1.8) + 0.9^3(1.7) + 0.9^4(1.6) \\
&amp;= 1.7 + 1.35 + 1.458 + 1.241 + 1.058 \\
&amp;= 6.807\end{split}\]</div>
</li>
<li><p>Q-function calculation for this state-action pair:</p>
<div class="math notranslate nohighlight">
\[\begin{split}Q(x_t, a_t) &amp;= \mathbb{E}[\sum_{k=0}^{\infty} \gamma^k R(x_{t+k}, a_{t+k}) | x_t, a_t] \\
&amp;= 1.7 + 0.9(1.5) + 0.9^2(1.8) + 0.9^3(1.7) + 0.9^4(1.6) \\
&amp;= 1.7 + 1.35 + 1.458 + 1.241 + 1.058 \\
&amp;= 6.807\end{split}\]</div>
</li>
</ul>
<p>Note that in this simplified example with deterministic transitions and a single trajectory per action, the Q-value equals the Value since there’s no uncertainty in the trajectories. In a real system with stochastic transitions, Q-function would evaluate the expected rewards conditioned on taking action a_t in state x_t, while Value function would evaluate expected rewards under the policy’s action choices.</p>
<p>Response B achieves a higher cumulative value (6.807 &gt; 6.044) because it leads to a trajectory with higher future rewards through more efficient problem-solving. While both responses eventually get to asking for the stack trace, Response B does so immediately, leading to faster problem resolution. This demonstrates how considering long-term value can help select actions that might not maximize immediate politeness but lead to more efficient problem-solving.</p>
</div>
<p>Another related concept is <span class="target" id="newconcept-advantage_estimation"></span><span class="newconcept">Advantage Estimation</span>, denoted by $\hat{A}_t$, and estimates how much better or worse a particular action brings in comparison to the current state. It is computed as:</p>
<div class="math notranslate nohighlight">
\[\hat{A}_t(x_t, a_t) = Q(x_t, a_t) - V(x_t)\]</div>
<p>In reality, if it is not convenient to estimate $V(x)$ (for example there isn’t another model for $V$), then the advantage can be instead calculated as:</p>
<div class="math notranslate nohighlight">
\[\hat{A}_t = Q(x_t, a_t) - Q(x_{t-1}, a_{t-1})\]</div>
<p>where $a_{t-1}$ is the previous action that already happened, and $Q(x_{t-1}, a_{t-1})$ is the previous value.</p>
</section>
<section id="deep-policy-networks">
<h2>Deep Policy Networks<a class="headerlink" href="#deep-policy-networks" title="Link to this heading"></a></h2>
<p><span class="target" id="newconcept-deep_reinforcement_learning"></span><span class="newconcept">Deep Reinforcement Learning (Deep RL)</span> leverages deep neural networks within the reinforcement learning framework. There are two major types: <span class="target" id="newconcept-deep_policy_networks"></span><span class="newconcept">Deep Policy Networks</span>, which directly model policies (pi(a_t|x_t)), and <span class="target" id="newconcept-deep_q-networks"></span><span class="newconcept">Deep Q-Networks (DQN)</span>, which directly estimate the long-term cumulative value of actions.</p>
<ul class="simple">
<li><p>Both methods are <span class="target" id="newconcept-action-centric_modeling"></span><span class="newconcept">action-centric modeling</span>. The common goal of both policy networks and DQNs is to determine system-side actions that maximize long-term business rewards (e.g., revenue, user engagement, retention).</p>
<ul>
<li><p>In practical search, recommendation, or advertising systems, both methods typically operate at the session, sub-session, or engagement level rather than at the individual interaction level (e.g., clicks). For instance:</p>
<ul>
<li><p><strong>Search/Ads</strong>: One user query combined with interactions on the query results or ads.</p></li>
<li><p><strong>Recommendations</strong>: One browsing session on a content feed along with related user interactions (e.g., likes, replies).</p></li>
<li><p><strong>Ads</strong>: An email campaign coupled with subsequent user interactions with the email content.</p></li>
</ul>
</li>
</ul>
</li>
<li><p>Their major difference is their modeling mechanisms and optimization objectives.</p>
<ul>
<li><p><strong>Deep Policy Networks</strong> explicitly model the policy by predicting a probability distribution over action space $A$ given the current state $x_t$. Their training objective typically involves <strong class="underline-bold">maximizing the expected cumulative future rewards (returns)</strong>. Optionally, policy networks can be trained together with supervised signals alongside RL signals.</p></li>
<li><p><strong>Deep Q-Networks (DQN)</strong> directly model the action-value function $Q(x_t, a_t)$, representing the expected cumulative future reward for taking an action $a_t$ at state $x_t$. DQNs are trained by <strong class="underline-bold">minimizing the temporal difference (TD) error</strong>, where the network’s predicted values are iteratively aligned with target estimates based on observed rewards and future Q-values. Once trained, optimal actions are chosen by selecting the highest estimated Q-values.</p></li>
</ul>
</li>
</ul>
<p>The <span class="target" id="newconcept-multi-armed_bandits"></span><span class="newconcept">Multi-Armed Bandits (MABs)</span> is <strong class="underline-bold">a special RL approach popular in its application to search/recommendation/ads systems</strong>.</p>
<ul class="simple">
<li><p>It can be viewed as simplified value learning that assumes the system state will not be impacted by actions (kind of single-state Q-learning but not exactly). It still helps balance the exploration-exploitation trade-off by dynamically selecting items to present to users (through sampling), while aiming to maximize system’s future cumulative performance (in comparison to always choosing the optimal actions).</p></li>
<li><p>MAB assumes the system state is not significantly impacted by actions, effectively considering each action’s outcome as independent of previous actions. This assumption aligns with scenarios where the system or user state remains relatively stable over short periods. Even when sessions exist, session history can be incorporated as context signals (known as <span class="target" id="newconcept-contextual_bandit"></span><span class="newconcept">Contextual Bandit</span>). For engagements that are far apart, any state shifts can be treated as independent interactions, with recent user interactions input as context.</p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Value function is rarely involved for deep RL for search/recommendation/ads, because it is not action-centric (the value function itself is not sufficient for action selection).</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The “long-term value” concept in Multi-Armed Bandits (MABs) differs from traditional RL. In MABs, “long-term” typically refers to cumulative performance improvement across the entire future decision horizon in a non-sequential setting. This is <strong class="underline-bold">conceptually closer to continuously improving a supervised learning model through additional data collection</strong>. In contrast, for traditional RL approaches, “long-term value” often specifically refers to maximizing cumulative rewards within episodes or sessions where current decisions affect future states, although this process ultimately aims to improve model performance across the entire future decision horizon.</p>
</div>
<section id="multi-armed-bandits-mabs">
<h3>Multi-Armed Bandits (MABs)<a class="headerlink" href="#multi-armed-bandits-mabs" title="Link to this heading"></a></h3>
<p>The Multi-Armed Bandits (MABs) framework provides a principled approach to balancing exploration and exploitation in decision-making under uncertainty. It derives its name from the “one-armed bandit” casino slot machines, conceptualizing a scenario where an agent must decide which of several slot machines (arms) to play to maximize cumulative rewards.</p>
<p>In the standard MAB setting:</p>
<ul class="simple">
<li><p>The agent has $K$ arms (actions) to choose from: $mathcal{A} = {a_1, a_2, …, a_K}$</p></li>
<li><p>Each arm $a$ has an unknown reward distribution with mean $mu_a$</p></li>
<li><p>In each round $t$, the agent selects an arm $a_t$, takes action, and observes a reward $r_t$ (provided by a reward function)</p>
<ul>
<li><p>In search/recommendation/ads context, this behavior is modified to select a slate of $k$ arms (i.e., top-$k$ selection from the item candidates) to propose to the users, and observes a total reward $r_t$ from the outcome of user engagement with the arms (items).</p></li>
</ul>
</li>
<li><p>The objective is to maximize the cumulative reward $sum_{t=1}^T r_t$ over $T$ rounds</p></li>
</ul>
<p>The performance of a MAB algorithm is typically measured by <span class="target" id="newconcept-regret"></span><span class="newconcept">regret</span>, which quantifies the difference between the rewards obtained by always choosing the optimal arm and the rewards obtained by the algorithm:</p>
<div class="math notranslate nohighlight">
\[R(T) = T \cdot \max_a \mu_a - \mathbb{E}\left[\sum_{t=1}^T r_t\right]\]</div>
<section id="modeling-without-explicit-uncertainty">
<h4>Modeling Without Explicit Uncertainty<a class="headerlink" href="#modeling-without-explicit-uncertainty" title="Link to this heading"></a></h4>
<p>In MABs context, <strong>modeling without explicit uncertainty refers</strong> to direct regression toward target rewards. Assume the MABs framework employs a model (e.g. <a class="reference external" href="02_transformer_models.html#newconcept-transformer_architecture"><span class="refconcept">Transformer Architecture</span></a>) to capture sequential dependencies in user behavior:</p>
<div class="math notranslate nohighlight">
\[r_{\mathbf{u},\mathbf{I}} = f_\mathbf{\theta}(\mathbf{u}, \mathbf{I}, \mathbf{H})\]</div>
<p>where:</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(\mathbf{u}\)</span> represents encoded user features</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbf{I}\)</span> represents encoded item features (multiple but limited number of items); we target to rank them and take the top-$k$</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbf{H}\)</span> represents encoded runtime contextual features</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(\mathbf{H}\)</span> is a sequential encodings of past interactions in the current runtime session (e.g., by a <a class="reference external" href="02_transformer_models.html#code-transformer-encoder">TransformerEncoder</a>)..</p>
<div class="math notranslate nohighlight">
\[\mathbf{H} = \text{SequentialEncoder}([\mathbf{h_1}, \mathbf{h_2}, ..., \mathbf{h_n}])\]</div>
<p>where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\mathbf{h}_j\)</span> is the encoding of the j-th interaction (including its post-action results).</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbf{H}\)</span> is the contextualized representation of the interaction history.</p></li>
<li><p>When using neural models, a masking mechanism similar to <a class="reference external" href="02_transformer_models.html#code-transformer-masking">transformer decoder masking</a> need to be applied.</p></li>
</ul>
</li>
</ul>
</li>
<li><p><span class="math notranslate nohighlight">\(f_\mathbf{\theta}\)</span> is a model with parameters <span class="math notranslate nohighlight">\(\mathbf{\theta}\)</span></p></li>
</ul>
<p>The following is an example design of $f$, consisting of two modules.</p>
<ol class="arabic">
<li><p><strong class="underline-bold">User-Item Interaction Module</strong>: Captures the interactions between user, current item, and historical context, and mix the features. For example, using a <a class="reference external" href="02_transformer_models.html#code-transformer-encoder">TransformerEncoder</a>.</p>
<div class="math notranslate nohighlight">
\[\mathbf{u}, \mathbf{I}, \mathbf{H} = \text{UserItemInteraction}([\mathbf{u}, \mathbf{I}, \mathbf{H}])\]</div>
<p>For simplicity we still use the same letters to denote the post-mixing features.</p>
</li>
<li><p><strong class="underline-bold">Reward Head</strong>: Estimates the expected reward $\hat{r}_{\mathbf{u},\mathbf{I}}$ for each user-item pair</p>
<div class="math notranslate nohighlight">
\[\hat{r}_{\mathbf{u},\mathbf{I}} = \text{RewardHead}(\mathbf{u},\mathbf{I}, \mathbf{H})\]</div>
<p>The reward head can be a simple MLP between $\mathbf{u}$ and $\mathbf{I}$ (ignoring $\mathbf{H}$), or before that we can further pre-process by pooling across $[\mathbf{u}, \mathbf{H}]$, depending on the requirements and experiments.</p>
</li>
</ol>
<p>In practice, different types of losses can be used together with multiple head,</p>
<section id="regression-loss">
<h5>Regression Loss<a class="headerlink" href="#regression-loss" title="Link to this heading"></a></h5>
<p>The training objective is straightforward regression to predict reward values. For example, using <span class="target" id="newconcept-mean_absolute_error"></span><span class="newconcept">Mean Absolute Error (MAE)</span> or <span class="target" id="newconcept-mean_square_error"></span><span class="newconcept">Mean Square Error (MSE)</span> loss. Additional regularization terms (e.g., <span class="target" id="newconcept-l2_regularization"></span><span class="newconcept">L2 regularization</span>) may be added to prevent overfitting.</p>
<ul class="simple">
<li><p>MAE can be <strong class="underline-bold">straightforwardly interpreted</strong> as how far off predictions are on average. MAE <strong class="underline-bold">gradient is not continuous</strong> but it is <strong class="underline-bold">not so sensitive to outliers</strong>.</p></li>
<li><p>MSE has <strong class="underline-bold">smooth gradient</strong> but <strong class="underline-bold">sensitive to outliers</strong>.</p></li>
</ul>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}\mathcal{L}_{\text{MAE}}(\mathbf{\theta}) = \text{mean}(|(r - \hat{r})|) + \text{regularization}\\\mathcal{L}_{\text{MSE}}(\mathbf{\theta}) = \text{mean}((r - \hat{r})^2) + \text{regularization}\end{aligned}\end{align} \]</div>
<p>Other regression losses might also be suitable,</p>
<ul>
<li><p>The <span class="target" id="newconcept-huber_loss"></span><span class="newconcept">Huber Loss</span> combines the best properties of <strong>MSE Loss</strong> and <strong>MAE Loss</strong> by being quadratic for small errors and linear for large errors, making it less sensitive to outliers.</p>
<div class="math notranslate nohighlight">
\[\mathcal{L}_{\text{Huber}}(\mathbf{\theta}) = \text{mean}\left(\sum_{i=1}^{N} L_\delta(r_i - \hat{r}_i)\right) + \text{regularization}\]</div>
<p>where:</p>
<div class="math notranslate nohighlight">
\[\begin{split}L_\delta(a) =
\begin{cases}
\frac{1}{2}a^2 &amp; \text{for } |a| \leq \delta \\
\delta(|a| - \frac{1}{2}\delta) &amp; \text{otherwise}
\end{cases}\end{split}\]</div>
<p>The parameter <span class="math notranslate nohighlight">\(\delta\)</span> controls the transition point between quadratic and linear behavior. Smaller values of <span class="math notranslate nohighlight">\(\delta\)</span> make the loss more robust to outliers but may slow down learning for small errors. In the MAB context, Huber loss is particularly useful when:</p>
<ul class="simple">
<li><p>Reward distributions have heavy tails or occasional extreme values</p></li>
<li><p>You want stability in training without completely discarding large errors</p></li>
</ul>
<p><span class="target" id="newconcept-log-cosh_loss"></span><span class="newconcept">Log-Cosh Loss</span> is a smooth approximation of the Huber Loss that is twice differentiable everywhere, making the gradient more smooth and suitable for optimization algorithms that use second derivatives.</p>
<div class="math notranslate nohighlight">
\[\mathcal{L}_{\text{LogCosh}}(\mathbf{\theta}) = \text{mean}(\sum \log(\cosh(r_i - \hat{r}_i))) + \text{regularization}\]</div>
<p>where <span class="math notranslate nohighlight">\(\cosh(x) = \frac{e^x + e^{-x}}{2}\)</span> is the <span class="target" id="newconcept-hyperbolic_cosine_function"></span><span class="newconcept">hyperbolic cosine function</span>. The gradient of this loss is sigmoid-like - the derivative of <span class="math notranslate nohighlight">\(\log(\cosh(a))\)</span> is <span class="math notranslate nohighlight">\(\tanh(a) = 2\sigma(2a) - 1\)</span> where $\sigma$ is the standard logistic function.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The somewhat strange formulation of Huber loss (like the coefficient $\frac{1}{2}$) is to ensure a continuous derivative.</p>
<ul class="simple">
<li><p>For $|a| ≤ δ$: the derivative is $a$.</p></li>
<li><p>For $|a| &gt; δ$: the derivative is $δ \cdot \text{sign}(a)$.</p></li>
<li><p>At $|a| = δ$: both sides have the same derivative ($±δ$).</p></li>
</ul>
<p>This is effectively capping the loss gradient, as showing in the following visualization. We also compare with Log-Cosh loss in the visualization, whose gradient is sigmoid-like.</p>

        <div id="wrapper-react-HuberLossVisualizer-7857" class="react-component-wrapper " style="display: flex; justify-content: center; align-items: center; width: 100%;">
            <div id="react-HuberLossVisualizer-7857" class="react-component-container " style="width: auto; height: auto; max-width: 1000px; min-width: auto;"></div>
            <div id="error-react-HuberLossVisualizer-7857" style="display: none; color: red; padding: 10px; border: 1px solid #ffcccc; margin-top: 10px; background-color: #fff8f8;"></div>
        </div>
        <script type="text/javascript">
            (function() {
                // Self-contained function to avoid global scope pollution
                function showError(message) {
                    // Display error in contained error div instead of modifying component container
                    var errorDiv = document.getElementById('error-react-HuberLossVisualizer-7857');
                    if (errorDiv) {
                        errorDiv.innerHTML = '<strong>Error:</strong> ' + message;
                        errorDiv.style.display = 'block';
                        console.error(message);
                    }
                }

                function loadComponentScript() {
                    try {
                        // Only proceed if React is available
                        if (typeof React === 'undefined' || typeof ReactDOM === 'undefined') {
                            showError('React or ReactDOM is not available');
                            return;
                        }

                        // Load component script - using the exact filename and path
                        var script = document.createElement('script');
                        script.src = '../../_static/js/modeling/classic_modeling/reinforcement_learning/HuberLossVisualizer.js';
                        script.onerror = function(e) {
                            showError('Failed to load component script: ../../_static/js/modeling/classic_modeling/reinforcement_learning/HuberLossVisualizer.js');
                        };
                        script.onload = function() {
                            // Mount component with error handling - use the exact component name
                            try {
                                if (window.HuberLossVisualizer) {
                                    ReactDOM.render(
                                        React.createElement(window.HuberLossVisualizer, {}, null),
                                        document.getElementById('react-HuberLossVisualizer-7857')
                                    );
                                } else {
                                    showError('Component HuberLossVisualizer not found in global scope');
                                }
                            } catch (error) {
                                showError(error.message);
                            }
                        };
                        document.head.appendChild(script);
                    } catch (error) {
                        showError('Unexpected error: ' + error.message);
                    }
                }

                // Initialize component loading based on KaTeX option
                function initComponent() {
                    
            // Only load KaTeX if it's not already loaded
            if (typeof window.katex === 'undefined') {
                // Load KaTeX CSS
                var katexCss = document.createElement('link');
                katexCss.rel = 'stylesheet';
                katexCss.href = 'https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css';
                katexCss.integrity = 'sha384-GvrOXuhMATgEsSwCs4smul74iXGOixntxDrHanlDqC0IIziTXcrXPnpVcVB8n2eHZ';
                katexCss.crossOrigin = 'anonymous';
                document.head.appendChild(katexCss);

                // Load KaTeX JS
                var katexScript = document.createElement('script');
                katexScript.src = 'https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js';
                katexScript.integrity = 'sha384-cpW21h6RZv/phavutF+AuVYrr+dA8xD9zs6FwLpaCct6O9ctzYFfFr4dgmgccOTx';
                katexScript.crossOrigin = 'anonymous';
                katexScript.onload = function() {
                    // Once KaTeX is loaded, load the component
                    loadComponentScript();
                };
                document.head.appendChild(katexScript);
            } else {
                // KaTeX already loaded, proceed to load component
                loadComponentScript();
            }
            
                }

                // Initialize when DOM is ready
                if (document.readyState === 'loading') {
                    document.addEventListener('DOMContentLoaded', initComponent);
                } else {
                    initComponent();
                }
            })();
        </script>
        </div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The true gradient of a neural network <span class="math notranslate nohighlight">\(f_{\mathbf{\theta}}\)</span> is computed with respect to all its parameters <span class="math notranslate nohighlight">\(\mathbf{\theta}\)</span>. However, this complete gradient is complex to analyze directly, so we typically focus on gradients at the output/loss layer with respect to intermediate variables like <span class="math notranslate nohighlight">\(\mathbf{z}_{\mathbf{\theta}}\)</span> (e.g., predicted reward scores, logits, etc., which are parameterized by <span class="math notranslate nohighlight">\(\mathbf{\theta}\)</span>). By the chain rule:</p>
<div class="math notranslate nohighlight">
\[\frac{\partial \mathcal{L}}{\partial \mathbf{\theta}} = \frac{\partial \mathcal{L}}{\partial \mathbf{z}_{\mathbf{\theta}}} \cdot \frac{\partial \mathbf{z}_{\mathbf{\theta}}}{\partial \mathbf{\theta}}\]</div>
<p>The gradient term <span class="math notranslate nohighlight">\(\frac{\partial \mathcal{L}}{\partial \mathbf{z}_{\mathbf{\theta}}}\)</span> acts as a scaling factor that can effectively amplify or zero out downstream gradients, making it critical for model behavior. This is why our discussions of gradients typically focus on variables near the output/loss layer rather than tracking through the entire network.</p>
</div>
</li>
<li><p><span class="target" id="newconcept-quantile_regression"></span><span class="newconcept">Quantile Regression</span> provides a more complete view of the relationship between prediction and outcome by estimating conditional quantiles rather than just the conditional mean.</p>
<p>For a specific quantile <span class="math notranslate nohighlight">\(\tau \in (0,1)\)</span>, the loss is:</p>
<div class="math notranslate nohighlight">
\[\mathcal{L}_{\text{Quantile}}(\mathbf{\theta}, \tau) = \text{mean}(\rho_\tau(r_i - \hat{r}_i)） + \text{regularization}\]</div>
<p>where:</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\begin{split}\rho_\tau(a) =
\begin{cases}
\tau \cdot a &amp; \text{if } a \geq 0 \\
(1-\tau) \cdot (-a) &amp; \text{if } a &lt; 0
\end{cases}\end{split}\]</div>
</div></blockquote>
<p>when <span class="math notranslate nohighlight">\(\tau = 0.5\)</span>, this corresponds to <span class="target" id="newconcept-median_regression"></span><span class="newconcept">Median Regression</span>, which is $0.5 \times (\text{MAE loss})$. Values of <span class="math notranslate nohighlight">\(\tau\)</span> closer to 0 or 1 focus on lower or upper quantiles of the distribution, respectively.</p>
<p>The parameter <span class="math notranslate nohighlight">\(\tau\)</span> controls which quantile is estimated through asymmetric weighting of errors:</p>
<ul class="simple">
<li><p>When the model predicts too high (<span class="math notranslate nohighlight">\(\hat{r}_i &gt; r_i\)</span>, so <span class="math notranslate nohighlight">\(a &lt; 0\)</span>), the error is weighted by <span class="math notranslate nohighlight">\((1-\tau)\)</span></p></li>
<li><p>When the model predicts too low (<span class="math notranslate nohighlight">\(\hat{r}_i &lt; r_i\)</span>, so <span class="math notranslate nohighlight">\(a &gt; 0\)</span>), the error is weighted by <span class="math notranslate nohighlight">\(\tau\)</span></p></li>
</ul>
<p>This setup will optimize toward $P(r_i &gt; \hat{r}_i) = tau$, meaning only <span class="math notranslate nohighlight">\(\tau\)</span>-quantile is underestimation. This makes <strong class="underline-bold">quantile regression particularly suitable for a risk-averse model</strong> - we set a large $\tau$ (e.g., 0.9) to ensure only a small fraction is overestimation.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>During optimization, the model is incentivized to position its predictions at the <span class="math notranslate nohighlight">\(\tau\)</span>-quantile due to the mathematics of the loss function. Let’s examine the derivative of the loss function with respect to the prediction <span class="math notranslate nohighlight">\(\hat{r}_i\)</span>:</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\begin{split}\frac{\partial \rho_\tau(r_i - \hat{r}_i)}{\partial \hat{r}_i} =
\begin{cases}
-\tau &amp; \text{if } r_i &gt; \hat{r}_i \text{ (i.e., } a &gt; 0 \text{)} \\
(1-\tau) &amp; \text{if } r_i &lt; \hat{r}_i \text{ (i.e., } a &lt; 0 \text{)}
\end{cases}\end{split}\]</div>
</div></blockquote>
<p>At the optimum, the expected gradient should equal zero:</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\mathbb{E}\left[\frac{\partial \rho_\tau(r_i - \hat{r}_i)}{\partial \hat{r}_i}\right] = 0\]</div>
</div></blockquote>
<p>This expectation can be written as:</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[-\tau \cdot P(r_i &gt; \hat{r}_i) + (1-\tau) \cdot P(r_i &lt; \hat{r}_i) = 0\]</div>
</div></blockquote>
<p>Rearranging, we get:</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\tau \cdot P(r_i &gt; \hat{r}_i) = (1-\tau) \cdot P(r_i &lt; \hat{r}_i)\]</div>
</div></blockquote>
<p>This simplifies to:</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[P(r_i &gt; \hat{r}_i) = \tau\]</div>
</div></blockquote>
<p>Meaning that the probability of the true value exceeding the prediction is exactly <span class="math notranslate nohighlight">\(\tau\)</span>, which is precisely the definition of the <span class="math notranslate nohighlight">\(\tau\)</span>-quantile.</p>

        <div id="wrapper-react-QuantileLossVisualizer-9922" class="react-component-wrapper " style="display: flex; justify-content: center; align-items: center; width: 100%;">
            <div id="react-QuantileLossVisualizer-9922" class="react-component-container " style="width: auto; height: auto; max-width: 1000px; min-width: auto;"></div>
            <div id="error-react-QuantileLossVisualizer-9922" style="display: none; color: red; padding: 10px; border: 1px solid #ffcccc; margin-top: 10px; background-color: #fff8f8;"></div>
        </div>
        <script type="text/javascript">
            (function() {
                // Self-contained function to avoid global scope pollution
                function showError(message) {
                    // Display error in contained error div instead of modifying component container
                    var errorDiv = document.getElementById('error-react-QuantileLossVisualizer-9922');
                    if (errorDiv) {
                        errorDiv.innerHTML = '<strong>Error:</strong> ' + message;
                        errorDiv.style.display = 'block';
                        console.error(message);
                    }
                }

                function loadComponentScript() {
                    try {
                        // Only proceed if React is available
                        if (typeof React === 'undefined' || typeof ReactDOM === 'undefined') {
                            showError('React or ReactDOM is not available');
                            return;
                        }

                        // Load component script - using the exact filename and path
                        var script = document.createElement('script');
                        script.src = '../../_static/js/modeling/classic_modeling/reinforcement_learning/QuantileLossVisualizer.js';
                        script.onerror = function(e) {
                            showError('Failed to load component script: ../../_static/js/modeling/classic_modeling/reinforcement_learning/QuantileLossVisualizer.js');
                        };
                        script.onload = function() {
                            // Mount component with error handling - use the exact component name
                            try {
                                if (window.QuantileLossVisualizer) {
                                    ReactDOM.render(
                                        React.createElement(window.QuantileLossVisualizer, {}, null),
                                        document.getElementById('react-QuantileLossVisualizer-9922')
                                    );
                                } else {
                                    showError('Component QuantileLossVisualizer not found in global scope');
                                }
                            } catch (error) {
                                showError(error.message);
                            }
                        };
                        document.head.appendChild(script);
                    } catch (error) {
                        showError('Unexpected error: ' + error.message);
                    }
                }

                // Initialize component loading based on KaTeX option
                function initComponent() {
                    
            // Only load KaTeX if it's not already loaded
            if (typeof window.katex === 'undefined') {
                // Load KaTeX CSS
                var katexCss = document.createElement('link');
                katexCss.rel = 'stylesheet';
                katexCss.href = 'https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css';
                katexCss.integrity = 'sha384-GvrOXuhMATgEsSwCs4smul74iXGOixntxDrHanlDqC0IIziTXcrXPnpVcVB8n2eHZ';
                katexCss.crossOrigin = 'anonymous';
                document.head.appendChild(katexCss);

                // Load KaTeX JS
                var katexScript = document.createElement('script');
                katexScript.src = 'https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js';
                katexScript.integrity = 'sha384-cpW21h6RZv/phavutF+AuVYrr+dA8xD9zs6FwLpaCct6O9ctzYFfFr4dgmgccOTx';
                katexScript.crossOrigin = 'anonymous';
                katexScript.onload = function() {
                    // Once KaTeX is loaded, load the component
                    loadComponentScript();
                };
                document.head.appendChild(katexScript);
            } else {
                // KaTeX already loaded, proceed to load component
                loadComponentScript();
            }
            
                }

                // Initialize when DOM is ready
                if (document.readyState === 'loading') {
                    document.addEventListener('DOMContentLoaded', initComponent);
                } else {
                    initComponent();
                }
            })();
        </script>
        </div>
</li>
</ul>
</section>
<section id="classification-ordinal-loss">
<h5>Classification &amp; Ordinal Loss<a class="headerlink" href="#classification-ordinal-loss" title="Link to this heading"></a></h5>
<p>As mentioned earlier, MABs can be viewed as a generalization of supervised learning where the training targets can be any numerical numbers in general. As a special case, if the reward is binary $0$ or $1$ (or $-1$, $+1$, as long as it distinguishes the two classes), then the above model effectively becomes supervised learning that can work with <strong>binary classification loss</strong>. In this case, we follow supervised-learning conversion to denote the label (reward) as $y$ and the estimation as $\hat{y}$.</p>
<p>Reward functions in practice are often synthetic and discontinuous in nature (e.g., <a class="reference internal" href="01_data_preparation.html#code-example-ecommerce-reward-function"><span class="std std-ref">ecommerce reward function</span></a>), even if it appears to be numeric. Therefore a common strategy to simplify the reward is using ordinal categories $C$. The categorization can be done by</p>
<ul class="simple">
<li><p>Simply rounding the numerical rewards.</p></li>
<li><p>Using milestone events, such as <code class="docutils literal notranslate"><span class="pre">{no-action:0,</span> <span class="pre">click:1,</span> <span class="pre">dwell-60sec-plus:2,</span> <span class="pre">add-to-cart:3,</span> <span class="pre">purchase:4}</span></code>.</p></li>
</ul>
<p>The the reward head will typically predict a distribution over the categories $\hat{y}_i sim \hat{p}_{i, c}, c \in C$, where $\hat{p}_{i, c}$ is the probability selecting item $i$ will result in a reward in category $c$. This loss design is suitable when there are clear milestone events in the application, and the reward is itself synthetic and based on the milestone events. Then <strong>multi-class classification loss</strong> or <strong>ordinal classification loss</strong> can then be applied. During inference, a reward can still be estimated and apply the <a class="reference internal" href="#newconcept-mbas_exploration_strategies"><span class="refconcept">MBAs Exploration Strategies</span></a>.</p>
<div class="math notranslate nohighlight">
\[\hat{r}_i = \sum_{c \in C} c \cdot \hat{p}_{i,c}\]</div>
<p>We start with <span class="target" id="newconcept-classification_loss"></span><span class="newconcept">Classification Loss</span>.</p>
<ul>
<li><p>The most common binary classification loss is <span class="target" id="newconcept-cross-entropy_loss"></span><span class="newconcept">Cross-Entropy Loss</span>. The following is the binary case and the multi-class case.</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}\mathcal{L}_{\text{CE}} = -\text{mean}(y_i \log(\hat{y}_i) + (1 - y_i) \log(1 - \hat{y}_i)\\\mathcal{L}_{\text{multiclass-CE}} = -\text{mean}(\sum_{c \in C} \mathbb{I}(y_i = c) \log(\hat{p}_{i,c})\end{aligned}\end{align} \]</div>
<p>where $\mathbb{I}$ is the identity function.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The gradient of cross-entropy loss with respect to the logit is the logistic function.</p>

        <div id="wrapper-react-CrossEntropyLossVisualizer-1537" class="react-component-wrapper " style="display: flex; justify-content: center; align-items: center; width: 100%;">
            <div id="react-CrossEntropyLossVisualizer-1537" class="react-component-container " style="width: auto; height: auto; max-width: 1000px; min-width: auto;"></div>
            <div id="error-react-CrossEntropyLossVisualizer-1537" style="display: none; color: red; padding: 10px; border: 1px solid #ffcccc; margin-top: 10px; background-color: #fff8f8;"></div>
        </div>
        <script type="text/javascript">
            (function() {
                // Self-contained function to avoid global scope pollution
                function showError(message) {
                    // Display error in contained error div instead of modifying component container
                    var errorDiv = document.getElementById('error-react-CrossEntropyLossVisualizer-1537');
                    if (errorDiv) {
                        errorDiv.innerHTML = '<strong>Error:</strong> ' + message;
                        errorDiv.style.display = 'block';
                        console.error(message);
                    }
                }

                function loadComponentScript() {
                    try {
                        // Only proceed if React is available
                        if (typeof React === 'undefined' || typeof ReactDOM === 'undefined') {
                            showError('React or ReactDOM is not available');
                            return;
                        }

                        // Load component script - using the exact filename and path
                        var script = document.createElement('script');
                        script.src = '../../_static/js/modeling/classic_modeling/reinforcement_learning/CrossEntropyLossVisualizer.js';
                        script.onerror = function(e) {
                            showError('Failed to load component script: ../../_static/js/modeling/classic_modeling/reinforcement_learning/CrossEntropyLossVisualizer.js');
                        };
                        script.onload = function() {
                            // Mount component with error handling - use the exact component name
                            try {
                                if (window.CrossEntropyLossVisualizer) {
                                    ReactDOM.render(
                                        React.createElement(window.CrossEntropyLossVisualizer, {}, null),
                                        document.getElementById('react-CrossEntropyLossVisualizer-1537')
                                    );
                                } else {
                                    showError('Component CrossEntropyLossVisualizer not found in global scope');
                                }
                            } catch (error) {
                                showError(error.message);
                            }
                        };
                        document.head.appendChild(script);
                    } catch (error) {
                        showError('Unexpected error: ' + error.message);
                    }
                }

                // Initialize component loading based on KaTeX option
                function initComponent() {
                    
            // Only load KaTeX if it's not already loaded
            if (typeof window.katex === 'undefined') {
                // Load KaTeX CSS
                var katexCss = document.createElement('link');
                katexCss.rel = 'stylesheet';
                katexCss.href = 'https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css';
                katexCss.integrity = 'sha384-GvrOXuhMATgEsSwCs4smul74iXGOixntxDrHanlDqC0IIziTXcrXPnpVcVB8n2eHZ';
                katexCss.crossOrigin = 'anonymous';
                document.head.appendChild(katexCss);

                // Load KaTeX JS
                var katexScript = document.createElement('script');
                katexScript.src = 'https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js';
                katexScript.integrity = 'sha384-cpW21h6RZv/phavutF+AuVYrr+dA8xD9zs6FwLpaCct6O9ctzYFfFr4dgmgccOTx';
                katexScript.crossOrigin = 'anonymous';
                katexScript.onload = function() {
                    // Once KaTeX is loaded, load the component
                    loadComponentScript();
                };
                document.head.appendChild(katexScript);
            } else {
                // KaTeX already loaded, proceed to load component
                loadComponentScript();
            }
            
                }

                // Initialize when DOM is ready
                if (document.readyState === 'loading') {
                    document.addEventListener('DOMContentLoaded', initComponent);
                } else {
                    initComponent();
                }
            })();
        </script>
        </div>
<p>To handle class imbalance issue, one may introduce weights <span class="math notranslate nohighlight">\(w^+\)</span> and <span class="math notranslate nohighlight">\(w^-\)</span> to reflect the inverse frequency of positive vs. negative classes. The <span class="target" id="newconcept-weighted_cross-entropy_loss"></span><span class="newconcept">Weighted Cross-Entropy Loss</span> is fomulated as:</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\mathcal{L}_{\text{weighted-CE}}
    = -\text{mean}
    (
    w^+ \,y_i \,\log(\hat{y}_i)
    + w^- \,(1 - y_i)\,\log\bigl(1 - \hat{y}_i\bigr)
    ).\]</div>
<div class="math notranslate nohighlight">
\[\mathcal{L}_{\text{weighted-multiclass-CE}}(\mathbf{\theta}) = -\text{mean}\left(\sum_{c \in C} w_c \cdot \mathbb{I}(y_i = c) \log(\hat{p}_{i, c})\right)\]</div>
</div></blockquote>
<p>where <span class="math notranslate nohighlight">\(w^+\)</span> is larger if positives are rarer; <span class="math notranslate nohighlight">\(w^-\)</span> is smaller if negatives are more frequent, and similarly for <span class="math notranslate nohighlight">\(w_k\)</span>.</p>
</li>
<li><p><span class="target" id="newconcept-focal_loss"></span><span class="newconcept">Focal Loss</span> is another variant of cross-entropy, especially helpful for <strong class="underline-bold">extremely imbalanced binary or multi-class classification</strong> tasks and <strong class="underline-bold">focusing on hard cases</strong>.</p>
<div class="math notranslate nohighlight">
\[\mathcal{L}_{\text{focal-CE}}
  = -\,\alpha \,(1 - p)^\gamma \,y \,\log(p)
    \;-\;\bigl(1 - \alpha\bigr)\,p^\gamma \,(1 - y)\,\log(1 - p),\]</div>
<p>where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\alpha\)</span> is a weighting factor (e.g., balancing positives vs. negatives, similar to the weighted CE).</p></li>
<li><p><span class="math notranslate nohighlight">\(\gamma\)</span> (the <span class="target" id="newconcept-focusing_parameter"></span><span class="newconcept">focusing parameter</span>) <span class="math notranslate nohighlight">\(\ge 0\)</span> controls how strongly to reduce the loss for well-classified samples, effectively <strong class="underline-bold">down-weighting easy examples</strong> and focus on misclassified or hard examples:</p>
<ul>
<li><p>If the example is <strong>easy</strong> (e.g., <span class="math notranslate nohighlight">\(y = 1\)</span> and <span class="math notranslate nohighlight">\(p \approx 1\)</span>), then <span class="math notranslate nohighlight">\((1 - p)^\gamma\)</span> further reduces its loss, freeing capacity to learn from harder examples.</p></li>
<li><p><span class="math notranslate nohighlight">\(\gamma = 0\)</span> recovers standard cross-entropy (i.e., no down-weighting).</p></li>
<li><p>Larger <span class="math notranslate nohighlight">\(\gamma\)</span> places more emphasis on hard or misclassified samples, diminishing the gradient for trivially correct ones.</p></li>
<li><p>Commonly used with <span class="math notranslate nohighlight">\(\gamma = 2\)</span> or <span class="math notranslate nohighlight">\(\gamma = 4\)</span> and <span class="math notranslate nohighlight">\(\alpha \in [0.25, 0.75]\)</span>. Tuning is often required to match the dataset.</p>
<ul>
<li><p>By emphasizing hard examples, focal loss may cause the model to <strong class="underline-bold">output less confident probabilities</strong>, leading to under-confident predictions.</p></li>
</ul>
</li>
</ul>
</li>
</ul>
<p>In the multi-class case, the model output a softmax distribution <span class="math notranslate nohighlight">\(\mathbf{p} = (p_1, p_2, \dots, p_k)\)</span>, with a true label <span class="math notranslate nohighlight">\(y \in \{1, 2, \dots, k\}\)</span>. A multi-class focal loss is similarly formulated as</p>
<div class="math notranslate nohighlight">
\[\mathcal{L}_{\text{focal-multiclass-CE}} = -\text{mean}\left(\sum_{c \in C}
\mathbb{I}(y_i = c)
\,\alpha_c
\,\bigl(1 - \hat{p}_{i,c}\bigr)^\gamma
\,\log\bigl(\hat{p}_{i,c}\bigr)\right)\]</div>
<p>where <span class="math notranslate nohighlight">\(\alpha_k\)</span> is the optional weight for class <span class="math notranslate nohighlight">\(k\)</span>. If <span class="math notranslate nohighlight">\(p_y\)</span> is large (easy sample), <span class="math notranslate nohighlight">\((1 - p_y)^\gamma\)</span> suppresses its loss contribution.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><strong>Origins</strong>
Focal loss was originally proposed for <strong>object detection</strong> (Lin et al., ICCV 2017), where background examples vastly outnumber foreground objects. It has since been adopted in other highly imbalanced settings.</p>
<p><strong>How it is used in recommendation, search, and ads?</strong></p>
<ul>
<li><p>While focal loss is viable in these domains (due to frequent class imbalance, e.g., very low click or conversion rates), however, <strong class="underline-bold">data sampling, cross-entropy with negative sampling or weighted cross-entropy is the dominant approach</strong>. Even for recommendation and ads systems, they typically still find <strong class="underline-bold">cross-entropy adequate once negative sampling or weighting is well-tuned</strong>.</p></li>
<li><p>The major reason is the focal loss penalizing easy cases, potentially making the model under-confident, and complicating the Probability Calibration that is very important for business interpretation.</p></li>
<li><p>When to Consider Focal Loss:</p>
<ol class="arabic simple">
<li><p><strong>Extreme Rare Positives</strong>: If the dataset has an extremely low positive rate (e.g., far below 1%) and standard negative sampling isn’t enough, focal loss can help highlight rare but important positives.</p></li>
<li><p><strong>Flood of Trivial Negatives</strong>: If there are a large number of obviously irrelevant impressions, focal loss can reduce their overshadowing effect and shift focus to borderline (hard) examples.</p></li>
<li><p><strong>Experimental Tuning</strong>: If cross-entropy + negative sampling is under-performing, trying focal loss with different <span class="math notranslate nohighlight">\(\gamma\)</span> and <span class="math notranslate nohighlight">\(\alpha\)</span> may improve recall for the minority.</p></li>
</ol>
<p><strong class="underline-bold">A typical use case for above is Anomaly Detection</strong>, such as network intrusion detection, where the positive cases are extremely rase.</p>
<p>In practice, although cross-entropy is typically found adequate, focal loss can still be an auxiliary loss function in Multi-Loss Learning so that its “focus” can softly feedback and impact the main loss (typically Cross-Entropy).</p>
</li>
</ul>
</div>
</li>
<li><p><span class="target" id="newconcept-hinge_loss"></span><span class="newconcept">Hinge Loss</span> is another option for binary classification. Unlike cross-entropy which focuses on probabilities, hinge loss enforces a margin between classes.</p>
<ul class="simple">
<li><p>Hinge loss considers <strong class="underline-bold">negative class label as -1 rather than 0</strong>.</p></li>
<li><p>Hinge loss is <strong class="underline-bold">directly applied on the logits</strong>, not on after softmax, because its formula and mechanism requires non-probabilistic scores.</p></li>
</ul>
<p>For binary classification with labels <span class="math notranslate nohighlight">\(y \in \{-1, +1\}\)</span> and model output <span class="math notranslate nohighlight">\(\hat{y}\)</span> (a continuous numeric score), the hinge loss is defined as:</p>
<div class="math notranslate nohighlight">
\[\mathcal{L}_{\text{Hinge}} = \text{mean}(\max(0, \text{margin} - y \cdot \hat{y}))\]</div>
<p>Key properties of hinge loss:</p>
<ul class="simple">
<li><p>$\text{margin}$ is a positive number and is typically set to 1. Higher margin in theory only scales model parameter.</p></li>
<li><p>When <span class="math notranslate nohighlight">\(r \cdot \hat{y} \geq \text{margin}\)</span>, the loss is zero - the example is correctly classified and outside the margin</p></li>
<li><p>When <span class="math notranslate nohighlight">\(r \cdot \hat{y} &lt; \text{margin}\)</span>, a penalty is applied - either the example is misclassified (<span class="math notranslate nohighlight">\(r \cdot \hat{y} &lt; 0\)</span>) or falls within the margin (<span class="math notranslate nohighlight">\(0 \leq y \cdot \hat{y} &lt; \text{margin}\)</span>)</p></li>
<li><p>The <strong class="underline-bold">zero-gradient region</strong> (when <span class="math notranslate nohighlight">\(y \cdot \hat{y} &gt; \text{margin}\)</span>) helps prevent overfitting by not pushing already well-classified points further</p></li>
</ul>
<p>Comparison with cross-entropy loss:</p>
<ul class="simple">
<li><p>[Pro] Hinge loss enforces the margin, offers especially robustness for hard marginal cases.</p></li>
<li><p>[Pro &amp; Con] Hinge loss has zero gradient for well-classified examples but tackling the hard marginal examples.</p>
<ul>
<li><p>However, this has similar adverse effect as <a class="reference internal" href="#newconcept-focal_loss"><span class="refconcept">Focal Loss</span></a> that will make model less confident in its predictions.</p></li>
</ul>
</li>
<li><p>[Con] Hinge loss <strong class="underline-bold">does not produce probability scores</strong>, which can hurt interpretability if a probabilistic output is needed.</p></li>
<li><p>[Con] Hinge loss is less common nowadays, but still <strong class="underline-bold">specifically applies to pairwise preference learning</strong> due to its capability to separate two preferences with a margin. Cross-entropy is the de facto standard for classification tasks in modern frameworks, with extensive tooling for performance support.</p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Hinge loss was traditionally popular in margin-based classifiers like Support Vector Machines (SVMs).</p>
<p>Hinge loss can be extended to multi-class settings (e.g., Crammer-Singer Formulation) or ordinal classification where constraints enforce ordering among classes, each with its own hinge penalty if that order is violated. However, preference learning is where today hinge loss is most commonly employed today. Pairwise preference learning is also dominant today in preference learning, therefore binary hinge loss usually suffices.</p>

        <div id="wrapper-react-HingeLossVisualizer-5172" class="react-component-wrapper " style="display: flex; justify-content: center; align-items: center; width: 100%;">
            <div id="react-HingeLossVisualizer-5172" class="react-component-container " style="width: auto; height: auto; max-width: 1000px; min-width: auto;"></div>
            <div id="error-react-HingeLossVisualizer-5172" style="display: none; color: red; padding: 10px; border: 1px solid #ffcccc; margin-top: 10px; background-color: #fff8f8;"></div>
        </div>
        <script type="text/javascript">
            (function() {
                // Self-contained function to avoid global scope pollution
                function showError(message) {
                    // Display error in contained error div instead of modifying component container
                    var errorDiv = document.getElementById('error-react-HingeLossVisualizer-5172');
                    if (errorDiv) {
                        errorDiv.innerHTML = '<strong>Error:</strong> ' + message;
                        errorDiv.style.display = 'block';
                        console.error(message);
                    }
                }

                function loadComponentScript() {
                    try {
                        // Only proceed if React is available
                        if (typeof React === 'undefined' || typeof ReactDOM === 'undefined') {
                            showError('React or ReactDOM is not available');
                            return;
                        }

                        // Load component script - using the exact filename and path
                        var script = document.createElement('script');
                        script.src = '../../_static/js/modeling/classic_modeling/reinforcement_learning/HingeLossVisualizer.js';
                        script.onerror = function(e) {
                            showError('Failed to load component script: ../../_static/js/modeling/classic_modeling/reinforcement_learning/HingeLossVisualizer.js');
                        };
                        script.onload = function() {
                            // Mount component with error handling - use the exact component name
                            try {
                                if (window.HingeLossVisualizer) {
                                    ReactDOM.render(
                                        React.createElement(window.HingeLossVisualizer, {}, null),
                                        document.getElementById('react-HingeLossVisualizer-5172')
                                    );
                                } else {
                                    showError('Component HingeLossVisualizer not found in global scope');
                                }
                            } catch (error) {
                                showError(error.message);
                            }
                        };
                        document.head.appendChild(script);
                    } catch (error) {
                        showError('Unexpected error: ' + error.message);
                    }
                }

                // Initialize component loading based on KaTeX option
                function initComponent() {
                    
            // Only load KaTeX if it's not already loaded
            if (typeof window.katex === 'undefined') {
                // Load KaTeX CSS
                var katexCss = document.createElement('link');
                katexCss.rel = 'stylesheet';
                katexCss.href = 'https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css';
                katexCss.integrity = 'sha384-GvrOXuhMATgEsSwCs4smul74iXGOixntxDrHanlDqC0IIziTXcrXPnpVcVB8n2eHZ';
                katexCss.crossOrigin = 'anonymous';
                document.head.appendChild(katexCss);

                // Load KaTeX JS
                var katexScript = document.createElement('script');
                katexScript.src = 'https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js';
                katexScript.integrity = 'sha384-cpW21h6RZv/phavutF+AuVYrr+dA8xD9zs6FwLpaCct6O9ctzYFfFr4dgmgccOTx';
                katexScript.crossOrigin = 'anonymous';
                katexScript.onload = function() {
                    // Once KaTeX is loaded, load the component
                    loadComponentScript();
                };
                document.head.appendChild(katexScript);
            } else {
                // KaTeX already loaded, proceed to load component
                loadComponentScript();
            }
            
                }

                // Initialize when DOM is ready
                if (document.readyState === 'loading') {
                    document.addEventListener('DOMContentLoaded', initComponent);
                } else {
                    initComponent();
                }
            })();
        </script>
        </div>
</li>
</ul>
<p>Another category of loss functions is the <span class="target" id="newconcept-ordinal_classification_loss"></span><span class="newconcept">Ordinal Classification Loss</span>. This is less common, but still see their often applications where milestone events are especially strong (e.g., in e-commerce, or ads). The most popular losses in this category are <strong class="underline-bold">adapted</strong> from classification CE losses.</p>
<ul>
<li><p>The <span class="target" id="newconcept-all-threshold_loss"></span><span class="newconcept">All-Threshold Loss</span> (a.k.a. <span class="target" id="newconcept-cumulative_probability_loss"></span><span class="newconcept">Cumulative Probability Loss</span>) is a common approach for ordinal regression that models $|C|-1$ binary thresholds for <a href="#id2"><span class="problematic" id="id3">|C|</span></a> ordinal categories:</p>
<div class="math notranslate nohighlight">
\[\mathcal{L}_{\text{all-threshold}} = -\text{mean}\left(\sum_{j=1}^{|C|-1} \left[
\mathbb{I}(y_i &gt; j) \log(\hat{p}_{y_i&gt;j}) +
\mathbb{I}(y_i \leq j) \log(1-\hat{p}_{y_i&gt;j})
\right]\right)\]</div>
<p>where <span class="math notranslate nohighlight">\(\hat{p}_{y_i&gt;j}\)</span> represents the probability that the predicted outcome for item i exceeds threshold j (<strong class="underline-bold">cumulative probabilities</strong> above thresholds).</p>
<div class="example-green admonition">
<p class="admonition-title">Example: Cumulative Probabilities Above Thresholds</p>
<p>Consider a sequence of $|C|=5$ ordinal categories <code class="docutils literal notranslate"><span class="pre">{no-action:0,</span> <span class="pre">click:1,</span> <span class="pre">dwell-60sec-plus:2,</span> <span class="pre">add-to-cart:3,</span> <span class="pre">purchase:4}</span></code>. The All-Threshold model predicts $|C|-1=4$ thresholds:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\hat{p}_{y_i&gt;0}\)</span>: Probability of at least clicking (categories 1-4)</p></li>
<li><p><span class="math notranslate nohighlight">\(\hat{p}_{y_i&gt;1}\)</span>: Probability of at least dwelling 60+ seconds (categories 2-4)</p></li>
<li><p><span class="math notranslate nohighlight">\(\hat{p}_{y_i&gt;2}\)</span>: Probability of at least adding to cart (categories 3-4)</p></li>
<li><p><span class="math notranslate nohighlight">\(\hat{p}_{y_i&gt;3}\)</span>: Probability of purchasing (category 4)</p></li>
</ul>
<p>For a user who adds to cart but doesn’t purchase (true label 3):</p>
<ul class="simple">
<li><p>The model should predict high probabilities for <span class="math notranslate nohighlight">\(\hat{p}_{y_i&gt;0}\)</span>, <span class="math notranslate nohighlight">\(\hat{p}_{y_i&gt;1}\)</span>, and <span class="math notranslate nohighlight">\(\hat{p}_{y_i&gt;2}\)</span></p></li>
<li><p>The model should predict a low probability for <span class="math notranslate nohighlight">\(\hat{p}_{y_i&gt;3}\)</span></p></li>
</ul>
</div>
<p>This approach preserves ordinality because:</p>
<ul class="simple">
<li><p>The thresholds have an inherent ordering (<span class="math notranslate nohighlight">\(\hat{p}_{y_i&gt;0} \geq \hat{p}_{y_i&gt;1} \geq \hat{p}_{y_i&gt;2} \geq \hat{p}_{y_i&gt;3}\)</span>)</p></li>
<li><p>It mathematically enforces that higher categories cannot be more likely than lower ones, preserving the ordinal relationship between categories in C.</p></li>
<li><p>A mistake predicting category 2 when the true category is 3 incurs less penalty than predicting category 0</p></li>
</ul>
<p>Despite the mathematical difference, all-threshold loss is essentially a Multi-Label Binary Cross-Entropy Loss. The <strong class="underline-bold">manipulation is on the label side</strong>, so that if the highest category one training example belongs to is $j$, then it also belongs to higher categories like $j+1$. During inference, the individual probabilities for each category can be derived from these cumulative thresholds:</p>
<div class="math notranslate nohighlight">
\[P(\hat{y}_i = c) = P(\hat{y}_i &gt; c-1) - P(\hat{y}_i &gt; c)\]</div>
<p>This convenience makes “all-threshold loss” a most popular ordinal classification loss.</p>
</li>
<li><p>The <span class="target" id="newconcept-ordinal_cross-entropy_loss"></span><span class="newconcept">Ordinal Cross-Entropy Loss</span> directly extends standard cross-entropy by incorporating a distance penalty that increases with the ordinal distance between predicted and true classes:</p>
<div class="math notranslate nohighlight">
\[\mathcal{L}_{\text{ordinal-CE}}(\mathbf{\theta}) = -\text{mean}\left(\sum_{c \in C} w_{|y_i - c|} \cdot \mathbb{I}(y_i = c) \log(\hat{p}_{i, c})\right)\]</div>
<p>where this is essentially a Weight Cross-Entropy Loss, and <span class="math notranslate nohighlight">\(w_{|y_i - c|}\)</span> is a weight that increases with the distance between the true class <span class="math notranslate nohighlight">\(y_i\)</span> and class c. Common weight formulations include squared distance (<span class="math notranslate nohighlight">\(w_d = d^2\)</span>) or exponential distance (<span class="math notranslate nohighlight">\(w_d = e^d - 1\)</span>).</p>
</li>
</ul>
</section>
<section id="preference-rank-loss">
<h5>Preference &amp; Rank Loss<a class="headerlink" href="#preference-rank-loss" title="Link to this heading"></a></h5>
<p>There exists a common scenario where the reward itself is implicit or hard to label directly. For instance, in search, recommendation, and advertising systems, user preferences are often only revealed through relative choices rather than through absolute ratings (e.g., thumb up one post instead of another, click one search result instead of the other). In these contexts, <span class="target" id="newconcept-preference_learning"></span><span class="newconcept">Preference Learning</span> and <span class="target" id="newconcept-rank_learning"></span><span class="newconcept">Rank Learning</span> approaches become especially valuable.</p>
<p><span class="target" id="newconcept-pairwise_preference_loss"></span><span class="newconcept">Pairwise Preference Loss</span> functions model the relative preference between pairs of items. They are particularly useful when direct reward values are unavailable but relative preferences can be annotated and inferred from user behavior. Nowadays pairwise preference learning is popular because it requires simpler labels, and is driven by its application in LLM development (a.k.a. <span class="target" id="newconcept-preference_alignment"></span><span class="newconcept">Preference Alignment</span>, focusing on aligning LLM with human preferences, enhancing their utility in terms of helpfulness, truthfulness, safety, and harmlessness).</p>
<ul class="simple">
<li><p>In search/recommendation/ads systems, pairwise prefernece is mostly applied to offline learning of user prefernece. However, in some scenarios where there are only two candidates (e.g., amazon places one preferred ad right below a product intro section, and the other less preferred ad on the side), then pairwise preference learning can be applied to runtime system.</p></li>
<li><p>In applications where final candidates can be often limited to two (e.g., chatbot through beam search), then pairwise preference learning can be applied to runtime system.</p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In realistic annotation practice, <strong class="underline-bold">asking annotators to rank preference, especially pairwise preference, is much easier than other annotations</strong>, less noisy and suitable for large-scale annotations. Instead of annotating A is relevant, B is not relevant (it becomes hard when relevance is not appearant), it is <strong class="underline-bold">easier to answer which one of A and B is more relevant</strong>.</p>
</div>
<p>In pairwise preference, a pair of items $i$ and $j$ are assumed, and we use $i \succ j$ to denote that item $i$ is preferred over item $j$ by the labels. We also denote $s_i$ and $s_j$ as the predicted scores for items $i$ and $j$ respectively. Popular pairwise losses are usually adaptation from classification losses.</p>
<ul class="simple">
<li><p>We view $i \succ j$ as the “positive label”, and denoting $y_{ij} = 1$ if item $i$ is preferred over item $j$, and $y_{ij} = 0$ (or $y_{ij} = -1$ for hinge loss) otherwise.</p></li>
<li><p>We view $\Delta s_{_ij} = s_i - s_j$ as “reward”, and $\sigma(\Delta s_{ij}) = \frac{1}{1 + e^{-(s_i - s_j)}}$ as the “positive probability” ($\sigma$ is the standard logistic function).</p></li>
<li><p>Due to its pairwise nature, vague and marginal examples are more frequent in preference-labeled data.</p>
<ul>
<li><p>Many comparisons might be between almost equally relevant examples from annotated data.</p></li>
<li><p>Human feedback for recommendation/ads are vague, i.e., user clicking on item A does not necessarily mean item B is irrelevant.</p></li>
</ul>
</li>
</ul>
<p>Then the adaptation naturally follows.</p>
<ul>
<li><p>The <span class="target" id="newconcept-pairwise_cross-entropy_loss"></span><span class="newconcept">Pairwise Cross-Entropy Loss</span> (also known as the <span class="target" id="newconcept-pairwise_logistic_loss"></span><span class="newconcept">Pairwise Logistic Loss</span>), and the <span class="target" id="newconcept-pairwise_hinge_loss"></span><span class="newconcept">Pairwise Hinge Loss</span> (also called <span class="target" id="newconcept-margin_ranking_loss"></span><span class="newconcept">Margin Ranking Loss</span>).</p>
<div class="math notranslate nohighlight">
\[\mathcal{L}_{\text{pairwise-CE}} = -\text{mean}\left( y_{ij} \log(\sigma(\Delta s_{ij})) + (1 - y_{ij}) \log(1 - \sigma(\Delta s_{ij})) \right)\]</div>
<div class="math notranslate nohighlight">
\[\mathcal{L}_{\text{pairwise-hinge}} = \text{mean}\left( \max(0, \text{margin} - \text{sign}(y_{ij}) \cdot (\Delta s_{ij})) \right)\]</div>
<p>The pairwise hinge loss is more common in preference learning than in classification, because</p>
<ul class="simple">
<li><p>Hinge loss enforces a margin (see <a class="reference internal" href="#newconcept-hinge_loss"><span class="refconcept">Hinge Loss</span></a>), and therefore it offers robustness against marginal cases (marginal cases are more often in preference learning). Such robustness is especially valued in ads.</p></li>
<li><p>Probabilistic interpretation is less important in pairwise preference learning. We can always convert it to probability by applying the logistic function.</p></li>
<li><p>Still the most common practice is used as jointly with cross-entropy loss.</p></li>
</ul>
</li>
<li><p><span class="target" id="newconcept-ranknet_loss"></span><span class="newconcept">RankNet Loss</span> has exactly the same formula as the <a class="reference internal" href="#newconcept-pairwise_cross-entropy_loss"><span class="refconcept">Pairwise Cross-Entropy Loss</span></a>, but</p>
<div class="math notranslate nohighlight">
\[\mathcal{L}_{\text{RankNet}} = -\text{mean}\left( y_{ij} \log(\sigma(\Delta s_{ij})) + (1 - y_{ij}) \log(1 - \sigma(\Delta s_{ij})) \right)\]</div>
<p>but simply allow three level of labels</p>
<ul class="simple">
<li><p>( y_{ij} = 1 ) if item ( i ) is preferred over item ( j ).</p></li>
<li><p>( y_{ij} = 0 ) if item ( j ) is preferred over item ( i ).</p></li>
<li><p>( y_{ij} = 0.5 ) if items ( i ) and ( j ) are equally preferred. In this case the minimum loss is achieved when $s_i = s_j$, aligned with the semantic meaning of $y_{ij} = 0.5$.</p></li>
</ul>
<p>This is effectively handling the scenario that many comparisons between equally relevant examples.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>** Exact Loss Behavior When $y_{ij} = 0.5$</p>
<p>When ( y_{ij} = 0.5 ), it signifies that items ( i ) and ( j ) are equally preferred. In this case, the RankNet loss function becomes:</p>
<div class="math notranslate nohighlight">
\[\mathcal{L}_{\text{RankNet}} = -\left( 0.5 \log(\sigma(\Delta s_{ij})) + 0.5 \log(1 - \sigma(\Delta s_{ij})) \right)\]</div>
<p>This simplifies to:</p>
<div class="math notranslate nohighlight">
\[\mathcal{L}_{\text{RankNet}} = -0.5 \left( \log(\sigma(\Delta s_{ij})) + \log(1 - \sigma(\Delta s_{ij})) \right)\]</div>
<p>Given that ( sigma(x) + sigma(-x) = 1 ), the loss further simplifies to:</p>
<div class="math notranslate nohighlight">
\[\mathcal{L}_{\text{RankNet}} = -0.5 \left( \log(\sigma(\Delta s_{ij})) + \log(\sigma(s_j - s_i)) \right)\]</div>
<p>Simplifying the logarithmic terms:</p>
<div class="math notranslate nohighlight">
\[\mathcal{L}_{\text{RankNet}} = -0.5 \left( -\log(1 + e^{-(\Delta s_{ij})}) + \log(e^{-(\Delta s_{ij})}) - \log(1 + e^{-(\Delta s_{ij})}) \right)\]</div>
<div class="math notranslate nohighlight">
\[\mathcal{L}_{\text{RankNet}} = -0.5 \left( -\log(1 + e^{-(\Delta s_{ij})}) - (\Delta s_{ij}) - \log(1 + e^{-(\Delta s_{ij})}) \right)\]</div>
<p>Combining terms:</p>
<div class="math notranslate nohighlight">
\[\mathcal{L}_{\text{RankNet}} = -0.5 \left( -2\log(1 + e^{-(\Delta s_{ij})}) - (\Delta s_{ij}) \right)\]</div>
<div class="math notranslate nohighlight">
\[\mathcal{L}_{\text{RankNet}} = \log(1 + e^{-(\Delta s_{ij})}) + 0.5(\Delta s_{ij})\]</div>
<ul class="simple">
<li><p>When $\Delta s_{ij} &gt;&gt; 0$, the logarithm term approaches $0$, and the loss becomes $0.5(\Delta s_{ij})$. When $\Delta s_{ij} &lt;&lt; 0$, the logarithm term approaches $-\Delta s_{ij}$, and the loss becomes $-0.5(\Delta s_{ij})$. Therefore $\mathcal{L}_{text{RankNet}} \approx 0.5|\Delta s_{ij}|$ when $|\Delta s_{ij}|$ is large.</p></li>
<li><p>Obviously the loss achieves minimum $0$ when $|\Delta s_{ij}| = 0$</p></li>
</ul>
</div>
</li>
</ul>
<p><span class="target" id="newconcept-listwise_ranking_loss"></span><span class="newconcept">Listwise Ranking Loss</span> functions consider entire ranked lists, rather than just pairs of items. They aim to optimize the overall quality of a ranking.</p>
<p><span class="target" id="newconcept-listnet_loss"></span><span class="newconcept">ListNet Loss</span> converts ranking scores into probability distributions using the softmax function and then compares these distributions using cross-entropy:</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}P(i|\mathbf{s}) = \frac{e^{s_i}}{\sum_{j} e^{s_j}}\\P(i|\mathbf{y}) = \frac{e^{y_i}}{\sum_{j} e^{y_j}}\\\mathcal{L}_{\text{ListNet}} = -\text{mean}\left( \sum_{i} P(i|\mathbf{y}) \log(P(i|\mathbf{s})) \right)\end{aligned}\end{align} \]</div>
<p>where:</p>
<ul class="simple">
<li><p>$mathbf{s}$ is the vector of predicted scores</p></li>
<li><p>$mathbf{y}$ is the vector of true relevance scores (which may be binary or graded)</p></li>
<li><p>$P(i|mathbf{s})$ and $P(i|mathbf{y})$ are the probabilities assigned to item $i$ in the predicted and true distributions, respectively</p></li>
</ul>
<p>ListNet has these key properties:</p>
<ul class="simple">
<li><p>It naturally handles multiple levels of relevance</p></li>
<li><p>It optimizes the entire ranking as a unit</p></li>
<li><p>It’s <strong class="underline-bold">permutation invariant within items of the same relevance</strong></p></li>
</ul>
<p>This loss is particularly effective for web search ranking where different documents can have varying degrees of relevance to a query.</p>
<p><span class="target" id="newconcept-lambdarank_loss"></span><span class="newconcept">LambdaRank Loss</span> doesn’t have a closed-form expression but is derived from the gradient of RankNet loss, scaled by the change in a non-differentiable evaluation metric (like NDCG or MAP):</p>
<div class="math notranslate nohighlight">
\[\lambda_{ij} = \frac{\partial \mathcal{L}_{\text{RankNet}}}{\partial s_i} \cdot |\Delta \text{NDCG}_{ij}|\]</div>
<p>where:</p>
<ul class="simple">
<li><p>$lambda_{ij}$ is the lambda gradient for a pair of items $(i, j)$</p></li>
<li><p>$Delta text{NDCG}_{ij}$ is the change in the NDCG metric if items $i$ and $j$ were swapped in the ranking</p></li>
</ul>
<p>The key insight of LambdaRank is that:</p>
<ul class="simple">
<li><p>It focuses model updates on item pairs where <strong class="underline-bold">improving the order would significantly impact the target metric</strong></p></li>
<li><p>Higher up in the ranking gets more weight (errors in top positions are penalized more)</p></li>
<li><p>It allows direct optimization of non-differentiable ranking metrics</p></li>
</ul>
<p>This loss is extensively used in commercial search engines like Bing and is effective for directly optimizing ranking quality metrics that users care about.</p>
<p><span class="target" id="newconcept-lambdamart_loss"></span><span class="newconcept">LambdaMART Loss</span> extends LambdaRank by incorporating it into a gradient boosting framework using regression trees. While not strictly a loss function, it uses the lambda gradients to guide tree construction:</p>
<div class="math notranslate nohighlight">
\[\mathcal{L}_{\text{LambdaMART}} \approx \sum_{i,j: y_i &gt; y_j} \lambda_{ij} \log(1 + e^{-(s_i - s_j)})\]</div>
<p>LambdaMART:</p>
<ul class="simple">
<li><p>Combines the strengths of boosted trees with lambda gradients</p></li>
<li><p>Shows excellent empirical performance on benchmark datasets</p></li>
<li><p>Handles <strong class="underline-bold">non-linear relationships effectively without requiring feature engineering</strong></p></li>
<li><p>Is the basis for many production ranking systems in search and recommendation</p></li>
</ul>
<p>Attention-based Listwise Context Loss</p>
<p>Recent approaches incorporate attention mechanisms to model contextual effects in rankings:</p>
<div class="math notranslate nohighlight">
\[\mathcal{L}_{\text{AttListRank}} = -\text{mean}\left( \sum_{i} P(i|\mathbf{y}) \log(P(i|\text{Attn}(\mathbf{s}, \mathbf{C}))) \right)\]</div>
<p>where:</p>
<ul class="simple">
<li><p>$text{Attn}(mathbf{s}, mathbf{C})$ applies self-attention over the scores considering the context $mathbf{C}$</p></li>
<li><p>The context $mathbf{C}$ may include information about item positions, query intent, user history, etc.</p></li>
</ul>
<p>This approach:</p>
<ul class="simple">
<li><p>Captures position bias and item interactions within a list</p></li>
<li><p>Models how items might <strong class="underline-bold">complement or substitute for each other</strong></p></li>
<li><p>Accounts for diversity in the ranking</p></li>
</ul>
<p>This type of loss is especially valuable in recommendation systems where item diversity and complementarity are important, beyond just relevance.</p>
<p>Pointwise Rank Loss with Position Bias</p>
<p>Some approaches incorporate position bias into pointwise formulations to achieve listwise effects:</p>
<p>Position-aware Logistic Loss</p>
<div class="math notranslate nohighlight">
\[\mathcal{L}_{\text{pos-logistic}} = -\text{mean}\left( \sum_{i} w(pos_i) \cdot (y_i \log(\sigma(s_i)) + (1 - y_i) \log(1 - \sigma(s_i))) \right)\]</div>
<p>where:</p>
<ul class="simple">
<li><p>$w(pos_i)$ is a position-dependent weight function, often decreasing with position</p></li>
<li><p>Common weight functions include $w(pos) = frac{1}{log_2(pos + 1)}$ (DCG-like) or $w(pos) = e^{-alpha cdot pos}$</p></li>
</ul>
<p>This approach:</p>
<ul class="simple">
<li><p>Approximates listwise behavior while keeping the simplicity of pointwise methods</p></li>
<li><p><strong class="underline-bold">Emphasizes correct predictions for top positions</strong></p></li>
<li><p>Is computationally efficient compared to true listwise methods</p></li>
</ul>
<p>Expected Reciprocal Rank (ERR) Loss</p>
<p><span class="target" id="newconcept-err_loss"></span><span class="newconcept">ERR Loss</span> models user behavior as a cascade process where users scan results from top to bottom and may stop at any position if satisfied:</p>
<div class="math notranslate nohighlight">
\[\mathcal{L}_{\text{ERR}} = -\text{mean}\left( \sum_{i} \frac{R(y_i)}{pos_i} \prod_{j &lt; i} (1 - R(y_j)) \right)\]</div>
<p>where:</p>
<ul class="simple">
<li><p>$R(y)$ is a function mapping relevance to satisfaction probability, e.g., $R(y) = frac{2^y - 1}{2^{y_{max}}}$</p></li>
<li><p>$j &lt; i$ refers to items ranked above item $i$</p></li>
</ul>
<p>ERR loss:</p>
<ul class="simple">
<li><p>Models user satisfaction and the probability of stopping at each position</p></li>
<li><p><strong class="underline-bold">Highly penalizes irrelevant items in top positions</strong></p></li>
<li><p>Has been shown to correlate well with user engagement metrics</p></li>
</ul>
<p>Direct Metric Optimization</p>
<p>Modern approaches often attempt to directly optimize ranking metrics through differentiable approximations.</p>
<p>ApproxNDCG Loss</p>
<p><span class="target" id="newconcept-approxndcg_loss"></span><span class="newconcept">ApproxNDCG Loss</span> creates a differentiable approximation of the Normalized Discounted Cumulative Gain (NDCG) metric:</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}\text{rank}_i \approx 1 + \sum_{j \neq i} \sigma(\alpha(s_j - s_i))\\\mathcal{L}_{\text{ApproxNDCG}} = 1 - \frac{\sum_{i} \frac{2^{y_i} - 1}{\log_2(1 + \text{rank}_i)}}{\text{IDCG}}\end{aligned}\end{align} \]</div>
<p>where:</p>
<ul class="simple">
<li><p>$alpha$ controls the sharpness of the sigmoid approximation</p></li>
<li><p>$text{IDCG}$ is the ideal DCG (when items are perfectly ranked by relevance)</p></li>
</ul>
<p>This loss:</p>
<ul class="simple">
<li><p>Directly optimizes a differentiable proxy for NDCG</p></li>
<li><p>Avoids the need for pair sampling strategies</p></li>
<li><p>Provides a <strong class="underline-bold">more direct path to optimizing the actual evaluation metric</strong></p></li>
</ul>
<p>NeuralNDCG Loss</p>
<p><span class="target" id="newconcept-neuralndcg_loss"></span><span class="newconcept">NeuralNDCG Loss</span> uses neural networks to model permutations and directly optimize NDCG:</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}P_{\theta}(\pi|\mathbf{s}) = \text{SoftSortNet}_{\theta}(\mathbf{s})\\\mathcal{L}_{\text{\end{aligned}\end{align} \]</div>
<p>The connection between reinforcement learning and ranking is particularly strong, as ranking can be viewed as a sequential decision process.</p>
<p>### Policy Gradient for Ranking</p>
<p><span class="target" id="newconcept-policy_gradient_for_ranking"></span><span class="newconcept">Policy Gradient for Ranking</span> treats item selection as a policy and uses reinforcement learning gradients:</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}\mathcal{L}_{\text{PGRank}} = -\mathbb{E}_{\pi \sim P_{\theta}(\pi|\mathbf{s})}[R(\pi, \mathbf{y})]\\\nabla_{\theta} \mathcal{L}_{\text{PGRank}} \approx -\mathbb{E}_{\pi \sim P_{\theta}(\pi|\mathbf{s})}[R(\pi, \mathbf{y}) \cdot \nabla_{\theta} \log P_{\theta}(\pi|\mathbf{s})]\end{aligned}\end{align} \]</div>
<p>where:
* $R(pi, mathbf{y})$ is the reward function (often an IR metric like NDCG or MAP)
* $P_{theta}(pi|mathbf{s})$ is the probability of permutation $pi$ under the current model</p>
<p>This approach:
* Naturally handles the exploration-exploitation tradeoff
* Can optimize non-differentiable metrics directly
* Is well-suited for online learning scenarios with user feedback</p>
<p>### Actor-Critic for Ranking</p>
<p><span class="target" id="newconcept-actor-critic_ranking"></span><span class="newconcept">Actor-Critic Ranking</span> models combine policy gradient with value function approximation:</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}\mathcal{L}_{\text{Actor}} = -\mathbb{E}_{\pi \sim P_{\theta}(\pi|\mathbf{s})}[(R(\pi, \mathbf{y}) - V_{\phi}(\mathbf{s})) \cdot \nabla_{\theta} \log P_{\theta}(\pi|\mathbf{s})]\\\mathcal{L}_{\text{Critic}} = \mathbb{E}_{\pi \sim P_{\theta}(\pi|\mathbf{s})}[(R(\pi, \mathbf{y}) - V_{\phi}(\mathbf{s}))^2]\end{aligned}\end{align} \]</div>
<p>where:
* $V_{phi}(mathbf{s})$ is a learned value function approximating expected reward</p>
<p>This approach:
* Reduces variance in gradient estimates
* Handles the credit assignment problem for sequence-level rewards
* Is effective for dynamic ranking problems where relevance depends on previously ranked items</p>
<p>In advertising systems, this can model how earlier ad impressions might influence the effectiveness of later ones within a user’s session.</p>
<p>## Practical Considerations</p>
<p>When implementing preference and ranking losses in real systems, several practical factors must be considered:</p>
<p>### Training Data Generation</p>
<ul>
<li><p><strong>Implicit vs. Explicit Feedback</strong>: Most search/ad systems rely on implicit feedback (clicks, dwell time) which requires careful interpretation due to position bias and presentation effects</p></li>
<li><p><strong>Negative Sampling Strategies</strong>: How non-clicked or non-interacted items are sampled significantly impacts model quality
* <strong>Uniform Sampling</strong>: Simple but often ineffective
* <strong>Hard Negative Mining</strong>: Focusing on challenging negatives that are close to the decision boundary
* <strong>In-batch Negatives</strong>: Using other positives in the batch as negatives for efficiency</p></li>
<li><p><strong>Debiasing Techniques</strong>: Methods like Inverse Propensity Scoring (IPS) to account for position bias in click data:</p>
<div class="math notranslate nohighlight">
\[\hat{r}_{\text{debiased}} = \frac{r_{\text{observed}}}{p_{\text{observation}}}\]</div>
</li>
</ul>
<p>### Computational Efficiency</p>
<ul class="simple">
<li><p><strong>Pair Generation</strong>: Pure pairwise approaches generate $O(n^2)$ pairs for a list of $n$ items, which can be prohibitive</p></li>
<li><p><strong>Sampling Strategies</strong>: Practical implementations sample pairs or use acceleration techniques:
* Focusing on top-k items
* Leveraging approximate nearest neighbor search for hard negatives
* Lambda-based approaches that focus on impactful pairs</p></li>
</ul>
<p>### Multi-Objective Optimization</p>
<p>Real-world ranking systems often optimize multiple objectives simultaneously:</p>
<ul class="simple">
<li><p><strong>Trade-offs</strong>: Balancing relevance against diversity, revenue, freshness, etc.</p></li>
<li><p><strong>Constrained Optimization</strong>: Ensuring minimum performance on secondary metrics while optimizing the primary objective</p></li>
<li><p><strong>Pareto Optimization</strong>: Finding solutions that cannot be improved on one metric without harming another</p></li>
</ul>
<p>### Warm-Starting Ranking Models</p>
<ul class="simple">
<li><p><strong>Transfer Learning</strong>: Using pretrained general models before fine-tuning with ranking losses</p></li>
<li><p><strong>Two-Stage Approaches</strong>: Combining pointwise losses for warm-starting with pairwise/listwise losses for refinement</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\mathcal{L}_{\text{combined}} = \alpha \cdot \mathcal{L}_{\text{pointwise}} + (1 - \alpha) \cdot \mathcal{L}_{\text{pairwise/listwise}}\]</div>
<p>where $alpha$ typically decreases during training to gradually shift from pointwise to ranking optimization.</p>
</section>
</section>
<section id="mbas-exploration-strategies">
<h4>MBAs Exploration Strategies<a class="headerlink" href="#mbas-exploration-strategies" title="Link to this heading"></a></h4>
<p>The following are common <span class="target" id="newconcept-mbas_exploration_strategies"></span><span class="newconcept">MBAs Exploration Strategies</span>.</p>
<ol class="arabic">
<li><p><strong class="underline-bold">ε-greedy for slate selection</strong>: The next action $a_t$ (consisting of result items) has probability $\epsilon$ to mix random items with top items by $\hat{r}$.</p>
<div class="math notranslate nohighlight">
\[\begin{split}a_t =
\begin{cases}
\text{Top-}k\text{ items by } \hat{r} &amp; \text{with probability } 1-\epsilon \\
\text{Hybrid Slate} &amp; \text{with probability } \epsilon
\end{cases}\end{split}\]</div>
<p>where “Hybrid Slate” combines some top-ranked items with some random items to ensure partial exploration.</p>
<p>Implementation details:</p>
<div class="folding highlight-python notranslate" id="epsilon-greedy-slate"><div class="highlight"><pre><span></span>  <span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
  <span class="kn">import</span> <span class="nn">random</span>

  <span class="k">def</span> <span class="nf">epsilon_greedy_slate</span><span class="p">(</span><span class="n">predictions</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">exploration_ratio</span><span class="o">=</span><span class="mf">0.4</span><span class="p">):</span>
      <span class="n">predictions</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">predictions</span><span class="p">)</span>
      <span class="n">num_items</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">predictions</span><span class="p">)</span>

      <span class="k">if</span> <span class="n">k</span> <span class="o">&gt;=</span> <span class="n">num_items</span><span class="p">:</span>
          <span class="c1"># If k is greater than or equal to the number of items, return all items sorted by predictions</span>
          <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">predictions</span><span class="p">)[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

      <span class="k">if</span> <span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">()</span> <span class="o">&lt;</span> <span class="n">epsilon</span><span class="p">:</span>
          <span class="c1"># Hybrid exploration: some top items, some random items</span>
          <span class="n">num_explore</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="n">k</span> <span class="o">*</span> <span class="n">exploration_ratio</span><span class="p">))</span>

          <span class="k">if</span> <span class="n">num_explore</span> <span class="o">==</span> <span class="n">k</span><span class="p">:</span>
              <span class="c1"># If all items are for exploration, return all items sorted (pure exploitation)</span>
              <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">predictions</span><span class="p">)[::</span><span class="o">-</span><span class="mi">1</span><span class="p">][:</span><span class="n">k</span><span class="p">]</span>

          <span class="n">num_exploit</span> <span class="o">=</span> <span class="n">k</span> <span class="o">-</span> <span class="n">num_explore</span>

          <span class="c1"># Get indices of all items sorted in descending order of predictions</span>
          <span class="n">sorted_indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">predictions</span><span class="p">)[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

          <span class="c1"># Select top items for exploitation</span>
          <span class="n">top_indices</span> <span class="o">=</span> <span class="n">sorted_indices</span><span class="p">[:</span><span class="n">num_exploit</span><span class="p">]</span>

          <span class="c1"># Select remaining items for exploration</span>
          <span class="n">remaining_indices</span> <span class="o">=</span> <span class="n">sorted_indices</span><span class="p">[</span><span class="n">num_exploit</span><span class="p">:]</span>
          <span class="n">random_indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">remaining_indices</span><span class="p">,</span> <span class="n">num_explore</span><span class="p">,</span> <span class="n">replace</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

          <span class="c1"># Combine exploitation and exploration indices</span>
          <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">((</span><span class="n">top_indices</span><span class="p">,</span> <span class="n">random_indices</span><span class="p">))</span>
      <span class="k">else</span><span class="p">:</span>
          <span class="c1"># Pure exploitation: select top-k items</span>
          <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">predictions</span><span class="p">)[::</span><span class="o">-</span><span class="mi">1</span><span class="p">][:</span><span class="n">k</span><span class="p">]</span>
</pre></div>
</div>
</li>
<li><p><strong class="underline-bold">Sampling from Softmax</strong>: The next action $a_t$ is a sampled according to the softmax-normalized distribution based on the estimated reward values. The sampling is exactly the same as when we perform <a class="reference external" href="02_transformer_models.html#newconcept-beam_search"><span class="refconcept">beam search</span></a> for a generative model.</p>
<div class="math notranslate nohighlight">
\[P(a_t^{(i)} = j) = \frac{\exp(\hat{r}^{(i)}/\tau)}{\sum \exp(\hat{r}/\tau)}\]</div>
<p>where $a_t^{(i)} = j$ denotes the $i$th item in $a_t$ and $\hat{r}^{(i)}$ is the estimated reward for user interacting with the $i$th item, and $\tau$ is the <strong>temperature</strong>. You can also apply <strong>top_k</strong> and/or <strong>top_p</strong> normalization on the softmax before the sampling. This sampling can be achieved by <code class="docutils literal notranslate"><span class="pre">np.random.choice</span></code> with <code class="docutils literal notranslate"><span class="pre">replace=False</span></code>, or <code class="docutils literal notranslate"><span class="pre">torch.multinomial</span></code> with <code class="docutils literal notranslate"><span class="pre">replacement=False</span></code>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The original “sampling from softmax” for MABs is performed in a sequential way - sampling first result, then normalize the distribution again, and sampling the next, until all top-$k$ items are selected. The distribution is different in this way, more favoring items of lower probabilities. However, this is less often used now: it does not have build-in support from popular packages, and the behavior like whether to favor low-probability ones can be controlled by parameters (e.g., temperature) as mentioned above.</p>
</div>
</li>
<li><p><strong class="underline-bold">Upper Confidence Bound (UCB) Selection</strong>: This technique adds an additional term to the estimated reward $\hat{r}$ by penalizing items that have been frequently presented to users (e.g., popular items that appear in many results).</p></li>
</ol>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\text{UCB}(i) = \hat{r}^{(i)} + \alpha \sqrt{\frac{\log(t)}{N_i + 1}}\]</div>
<p>where</p>
<ul class="simple">
<li><p>$\hat{r}^{(i)}$ is the estimated reward for user interacting with the $i$th item.</p></li>
<li><p>$N_i$ is the number of times item $i$ has been selected.</p>
<ul>
<li><p>The typical practice is consider a pre-defined time window (e.g., last week, last month); but of course you may count through entire history.</p></li>
<li><p>You may apply decay factor to gradually phase out old counts (e.g., exponential scale $x^{(\text{current_time}-\text{history_time})}, x \in (0, 1)$).</p></li>
<li><p>“+1” to prevent zero count.</p></li>
</ul>
</li>
<li><p>$t$ is the total number of interactions during the same time window, and $\alpha$ is a parameter controlling the exploration-exploitation trade-off (higher value for more exploration). This formula balances exploitation (first term) with exploration (second term), giving higher scores to items with promising rewards or those less frequently shown.</p></li>
</ul>
<p>UCB can be further extended with a diversity term similar to the <a class="reference external" href="../../evaluation/diversity_evaluation.html#newconcept-intra-list_diversity"><span class="refconcept">Intra-List Diversity</span></a>, called <span class="target" id="newconcept-diversity-aware_ucb"></span><span class="newconcept">Diversity-Aware UCB</span>:</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}\text{UCB}_\text{diverse_select_once}(i) = \text{UCB}(i) - \beta \sum_{j \neq i} \text{sim}(\mathbf{I}_i, \mathbf{I}_{a_t^{(j)}})\\\text{UCB}_\text{diverse_sequential_select}(i) = \text{UCB}(i) - \beta \sum_{j=1}^{i-1} \text{sim}(\mathbf{I}_i, \mathbf{I}_{a_t^{(j)}})\end{aligned}\end{align} \]</div>
<p>where:</p>
<ul>
<li><p>The first term is the standard UCB score.</p></li>
<li><p>The second term penalizes similarity to already selected items. $\beta$ is another parameter controlling the weight of diversity penalty (higher $\beta$ giving more weight to diversity). The is the more popular approach with simplified computation.</p></li>
<li><p>You may direct rank all items by $\text{UCB}_\text{diverse_select_once}$ and select top $k$; or you may selected sequentially using $\text{UCB}_\text{diverse_sequential_select}$, where each selection affecting subsequent choices, and at $i$th selection step, the diversity penalty is calculated for all remaining items against selected items.</p>
<ul>
<li><p>Either way, a similarity matrix is pre-computed between every pair of items. For sequential selection, use mask to select similarities during calculation. A torch implementation of both diversity-aware UCB formulas is as follows,</p>
<div class="folding highlight-python notranslate" id="diversity-aware-ucb-torch"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>

<span class="k">def</span> <span class="nf">diversity_aware_ucb_select_once</span><span class="p">(</span><span class="n">predictions</span><span class="p">,</span> <span class="n">counts</span><span class="p">,</span> <span class="n">total_interactions</span><span class="p">,</span> <span class="n">similarity_matrix</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mf">0.5</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Select a diverse slate of items using UCB with diversity penalty (single selection step) in PyTorch.</span>

<span class="sd">    Args:</span>
<span class="sd">        predictions: Tensor of predicted rewards [n_items]</span>
<span class="sd">        counts: Tensor of item selection counts [n_items]</span>
<span class="sd">        total_interactions: Total number of interactions (scalar)</span>
<span class="sd">        similarity_matrix: Precomputed similarity matrix [n_items, n_items]</span>
<span class="sd">        k: Number of items to select</span>
<span class="sd">        alpha: UCB exploration parameter</span>
<span class="sd">        beta: Diversity penalty weight</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor of selected item indices</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">device</span> <span class="o">=</span> <span class="n">predictions</span><span class="o">.</span><span class="n">device</span>
    <span class="n">n_items</span> <span class="o">=</span> <span class="n">predictions</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

    <span class="c1"># Compute UCB scores</span>
    <span class="n">ucb_scores</span> <span class="o">=</span> <span class="n">predictions</span> <span class="o">+</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">total_interactions</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">))</span> <span class="o">/</span> <span class="p">(</span><span class="n">counts</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span>

    <span class="c1"># Compute diversity penalty</span>
    <span class="n">diversity_penalty</span> <span class="o">=</span> <span class="n">beta</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">similarity_matrix</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="c1"># Compute final scores with diversity adjustment</span>
    <span class="n">diverse_scores</span> <span class="o">=</span> <span class="n">ucb_scores</span> <span class="o">-</span> <span class="n">diversity_penalty</span>

    <span class="c1"># Select top-k items based on diverse scores</span>
    <span class="n">selected_indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">diverse_scores</span><span class="p">,</span> <span class="n">descending</span><span class="o">=</span><span class="kc">True</span><span class="p">)[:</span><span class="n">k</span><span class="p">]</span>

    <span class="k">return</span> <span class="n">selected_indices</span>

<span class="k">def</span> <span class="nf">diversity_aware_ucb_sequential_select</span><span class="p">(</span><span class="n">predictions</span><span class="p">,</span> <span class="n">counts</span><span class="p">,</span> <span class="n">total_interactions</span><span class="p">,</span> <span class="n">similarity_matrix</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mf">0.5</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Select a diverse slate of items using UCB with a diversity penalty (PyTorch version).</span>

<span class="sd">    Args:</span>
<span class="sd">        predictions: Tensor of predicted rewards [n_items]</span>
<span class="sd">        counts: Tensor of item selection counts [n_items]</span>
<span class="sd">        total_interactions: Total number of interactions</span>
<span class="sd">        similarity_matrix: Pre-computed item similarity matrix [n_items, n_items]</span>
<span class="sd">        k: Number of items to select</span>
<span class="sd">        alpha: UCB exploration parameter</span>
<span class="sd">        beta: Diversity penalty weight</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor of selected item indices</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">n_items</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">predictions</span><span class="p">)</span>

    <span class="c1"># Calculate initial UCB scores for all items</span>
    <span class="n">ucb_scores</span> <span class="o">=</span> <span class="n">predictions</span> <span class="o">+</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">total_interactions</span><span class="p">))</span> <span class="o">/</span> <span class="p">(</span><span class="n">counts</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span>

    <span class="c1"># Create a mask to track available items (True=available, False=selected or unavailable)</span>
    <span class="n">available_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">n_items</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bool</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">predictions</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

    <span class="c1"># Tensor to store selected indices</span>
    <span class="n">selected_indices</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="c1"># Select items sequentially</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">min</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">n_items</span><span class="p">)):</span>
        <span class="k">if</span> <span class="n">i</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="c1"># Get selected indices as tensor</span>
            <span class="n">selected_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">selected_indices</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">predictions</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

            <span class="c1"># Calculate diversity penalty using matrix operations</span>
            <span class="n">diversity_penalty</span> <span class="o">=</span> <span class="n">beta</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span>
                <span class="n">similarity_matrix</span><span class="p">[</span><span class="n">available_mask</span><span class="p">][:,</span> <span class="n">selected_tensor</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span>
            <span class="p">)</span>

            <span class="c1"># Apply diversity penalty to available items</span>
            <span class="n">current_scores</span> <span class="o">=</span> <span class="n">ucb_scores</span><span class="p">[</span><span class="n">available_mask</span><span class="p">]</span> <span class="o">-</span> <span class="n">diversity_penalty</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># For the first item, no diversity penalty</span>
            <span class="n">current_scores</span> <span class="o">=</span> <span class="n">ucb_scores</span><span class="p">[</span><span class="n">available_mask</span><span class="p">]</span>

        <span class="c1"># Find the best item</span>
        <span class="n">best_item_idx</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">current_scores</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

        <span class="c1"># Convert to original index</span>
        <span class="n">original_idx</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n_items</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">predictions</span><span class="o">.</span><span class="n">device</span><span class="p">)[</span><span class="n">available_mask</span><span class="p">][</span><span class="n">best_item_idx</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

        <span class="c1"># Add to selected items</span>
        <span class="n">selected_indices</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">original_idx</span><span class="p">)</span>

        <span class="c1"># Mark as unavailable</span>
        <span class="n">available_mask</span><span class="p">[</span><span class="n">original_idx</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span>

        <span class="c1"># Break if we&#39;ve selected all available items</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">available_mask</span><span class="p">):</span>
            <span class="k">break</span>

    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">selected_indices</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">predictions</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
</pre></div>
</div>
</li>
</ul>
</li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The term “Upper Confidence Bound” comes from the statistical concept of confidence intervals. Here’s why it has this name:</p>
<ul class="simple">
<li><p>In statistics, when we estimate a parameter (like the mean reward of an item), we typically create a confidence interval around our estimate. This interval has a lower bound and an upper bound, and we can be confident (to some statistical level) that the true value lies within this range.</p></li>
<li><p>This second term is derived from <a class="reference external" href="https://en.wikipedia.org/wiki/Hoeffding%27s_inequality">Hoeffding’s inequality</a>, which bounds the probability that an empirical mean deviates from the true mean by more than a certain amount.</p></li>
<li><p>The term $\sqrt{\frac{1}{N_i + 1}}$ is related to <a class="reference external" href="https://en.wikipedia.org/wiki/Analysis_of_variance">Variance Scaling</a>. If an observation in the real world is assumed to sampled from the underlying normal distribution, then <strong class="underline-bold">the variance of the normal distribution is inversely proportional to the number of observations</strong>.</p></li>
</ul>
</div>
</div></blockquote>
<ol class="arabic simple" start="4">
<li><p><strong>Thompson Sampling</strong>: A technique to sample results from a pre-defined distribution (e.g., normal distribution, beta distribution). Without uncertainty modeling, the model directly regress against the reward targets and does not have build-in support for distribution parameter estimation. However, we can still build a distribution in a rule-based way slate selection. The following code demonstrates a random noise based on past item frequency (<code class="docutils literal notranslate"><span class="pre">counts[i]</span></code>, the same the $N_i$ in above UCB). The variance formula has a statistical foundation in variance scaling.</p></li>
</ol>
<blockquote>
<div><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">approximate_thompson_sampling_slate</span><span class="p">(</span><span class="n">predictions</span><span class="p">,</span> <span class="n">counts</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">noise_scale</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
    <span class="c1"># Add noise proportional to uncertainty (approximated by inverse count)</span>

    <span class="n">noisy_predictions</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">predictions</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">predictions</span><span class="p">)):</span>
        <span class="c1"># More noise for less frequently selected items</span>
        <span class="c1"># From variance scaling: the variance of the latent normal distribution is inversely proportional to the number of observations</span>
        <span class="c1"># +1 to prevent zero count</span>
        <span class="n">uncertainty</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">counts</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">noisy_predictions</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">noise_scale</span> <span class="o">*</span> <span class="n">uncertainty</span><span class="p">)</span>

    <span class="c1"># Select top-k items based on noisy predictions</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">noisy_predictions</span><span class="p">)[::</span><span class="o">-</span><span class="mi">1</span><span class="p">][:</span><span class="n">k</span><span class="p">]</span>
</pre></div>
</div>
</div></blockquote>
<ol class="arabic" start="5">
<li><p><strong>Monte Carlo Dropout</strong>: During inference,</p>
<ol class="arabic simple">
<li><p>Turn on dropout and run inference $m$ times to obtain $m$ slates of reward estimations.</p></li>
<li><p>Estimate mean and variance for each item.</p></li>
<li><p>Apply Thompson Sampling to resample a reward for each item.</p></li>
<li><p>Rerank the sampled rewards and take top-$k$ items.</p></li>
</ol>
<p>The following code specifically enable dropout during inference for <code class="docutils literal notranslate"><span class="pre">Dropout</span></code> modules.</p>
<div class="folding highlight-python notranslate" id="enable-dropout-inference"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>

<span class="k">def</span> <span class="nf">enable_dropout_inference</span><span class="p">(</span><span class="n">model</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Recursively enable dropout layers during inference.</span>

<span class="sd">    Args:</span>
<span class="sd">        model (nn.Module): The PyTorch model to modify</span>

<span class="sd">    Returns:</span>
<span class="sd">        None (modifies the model in-place)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">for</span> <span class="n">module</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">modules</span><span class="p">():</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">):</span>
            <span class="c1"># Enable dropout during inference</span>
            <span class="n">module</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
</pre></div>
</div>
</li>
</ol>
<p>In practice, it is usually a mix of strategies. Strategy simplicity and computational efficiency is also a major consideration. <strong class="underline-bold">Combining context-aware UCB (select once) softmax sampling enables a flexible strategy</strong> integrating reward score, past occurrences, diversity and controllable randomness. Plus, the whole process can be efficiently implemented with a package like Pytorch.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Example data</span>
<span class="n">predictions</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">])</span>
<span class="n">counts</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">7</span><span class="p">])</span>
<span class="n">total_interactions</span> <span class="o">=</span> <span class="n">counts</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
<span class="n">similarity_matrix</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span>
    <span class="p">[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">],</span>
    <span class="p">[</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">],</span>
    <span class="p">[</span><span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">],</span>
    <span class="p">[</span><span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">],</span>
    <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">]</span>
<span class="p">])</span>

<span class="c1"># `selection_size` is how many items we want to select from the total 5 items</span>
<span class="c1"># This is the `top-k` selection that has been discussed above.</span>
<span class="c1"># However, `top_k` also refers to a parameter in softmax sampling (normalizing only across the k highest scores);</span>
<span class="c1">#  thus we rename this parameter as `selection_size` to avoid confusion.</span>
<span class="n">selection_size</span> <span class="o">=</span> <span class="mi">3</span>

<span class="c1"># UCB parameters</span>
<span class="n">alpha</span> <span class="o">=</span> <span class="mf">1.0</span>
<span class="n">beta</span> <span class="o">=</span> <span class="mf">0.5</span>

<span class="c1"># Softmax parameters</span>
<span class="n">temperature</span> <span class="o">=</span> <span class="mf">1.0</span>
<span class="n">top_p</span> <span class="o">=</span> <span class="mf">0.95</span>  <span class="c1"># top_p removes outlier item in the bottom quantile in softmax sampling</span>
<span class="n">top_k</span> <span class="o">=</span> <span class="mi">5</span> <span class="c1"># top_k indicates only to normalize across k highest probable items in softmax sampling</span>

<span class="c1"># Compute UCB scores with diversity penalty</span>
<span class="n">ucb_scores</span> <span class="o">=</span> <span class="n">compute_ucb_scores</span><span class="p">(</span><span class="n">predictions</span><span class="p">,</span> <span class="n">counts</span><span class="p">,</span> <span class="n">total_interactions</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="n">similarity_matrix</span><span class="p">)</span>

<span class="c1"># Select items using softmax sampling</span>
<span class="n">selected_indices</span> <span class="o">=</span> <span class="n">softmax_sampling</span><span class="p">(</span><span class="n">ucb_scores</span><span class="p">,</span> <span class="n">temperature</span><span class="p">,</span> <span class="n">selection_size</span><span class="p">,</span> <span class="n">top_p</span><span class="p">,</span> <span class="n">top_k</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="exploration-with-explicit-uncertainty-modeling">
<h4>Exploration With Explicit Uncertainty Modeling<a class="headerlink" href="#exploration-with-explicit-uncertainty-modeling" title="Link to this heading"></a></h4>
</section>
<section id="upper-confidence-bound-ucb">
<h4>Upper Confidence Bound (UCB)<a class="headerlink" href="#upper-confidence-bound-ucb" title="Link to this heading"></a></h4>
<p>UCB algorithms select arms based on optimistic estimates of potential rewards:</p>
<div class="math notranslate nohighlight">
\[a_t = \arg\max_a \left( \hat{\mu}_a + c \sqrt{\frac{\ln t}{n_a}} \right)\]</div>
<p>Where:</p>
<ul class="simple">
<li><p>$hat{mu}_a$ is the estimated mean reward for arm $a$</p></li>
<li><p>$c$ controls the exploration level</p></li>
<li><p>$ln t$ represents the natural logarithm of the current round</p></li>
<li><p>$n_a$ is the number of times arm $a$ has been pulled</p></li>
</ul>
<p>The UCB term $c sqrt{frac{ln t}{n_a}}$ represents the uncertainty in the reward estimate, which decreases as an arm is pulled more frequently.</p>
</section>
<section id="thompson-sampling">
<h4>Thompson Sampling<a class="headerlink" href="#thompson-sampling" title="Link to this heading"></a></h4>
<p>Thompson Sampling adopts a Bayesian approach:</p>
<ol class="arabic simple">
<li><p>Maintain a probability distribution over the mean reward of each arm</p></li>
<li><p>Sample from each distribution to get a potential reward value for each arm</p></li>
<li><p>Select the arm with the highest sampled value</p></li>
</ol>
<p>For Bernoulli rewards (e.g., click/no-click), Beta distributions are typically used:</p>
<ul class="simple">
<li><p>Initialize each arm with prior Beta($alpha_0$, $beta_0$)</p></li>
<li><p>After observing outcome $r$ for arm $a$:
- If $r = 1$ (success): Update to Beta($alpha_a + 1$, $beta_a$)
- If $r = 0$ (failure): Update to Beta($alpha_a$, $beta_a + 1$)</p></li>
</ul>
<p>The selection rule becomes:</p>
<div class="math notranslate nohighlight">
\[a_t = \arg\max_a \theta_a,~\text{where}~\theta_a \sim \text{Beta}(\alpha_a, \beta_a)\]</div>
</section>
<section id="contextual-bandits">
<h4>4. Contextual Bandits<a class="headerlink" href="#contextual-bandits" title="Link to this heading"></a></h4>
<p>Contextual bandits extend the MAB framework by incorporating context information:</p>
<ul class="simple">
<li><p>In each round $t$, observe context $x_t in mathcal{X}$</p></li>
<li><p>Select arm $a_t$ based on both context and past rewards</p></li>
<li><p>Observe reward $r_t$</p></li>
</ul>
<p>The goal is to learn a policy $pi: mathcal{X} rightarrow mathcal{A}$ that maps contexts to arms.</p>
<p>For linear contextual bandits, the expected reward is modeled as:</p>
<div class="math notranslate nohighlight">
\[\mathbb{E}[r|a,x] = x^T\theta_a\]</div>
<p>Where $theta_a$ is an unknown parameter vector for arm $a$.</p>
<p>Popular contextual bandit algorithms include:</p>
<ul class="simple">
<li><p>LinUCB: $a_t = argmax_a (x_t^That{theta}_a + alphasqrt{x_t^T A_a^{-1} x_t})$</p></li>
<li><p>Neural Bandits: Use neural networks to model the relationship between contexts and rewards</p></li>
</ul>
</section>
<section id="advanced-mab-techniques">
<h4>Advanced MAB Techniques<a class="headerlink" href="#advanced-mab-techniques" title="Link to this heading"></a></h4>
<p>Batched Bandits</p>
<p>Process feedback in batches rather than individually, crucial for systems where immediate feedback is unavailable:</p>
<div class="math notranslate nohighlight">
\[\hat{\mu}_a^{(b)} = \frac{1}{n_a^{(b)}} \sum_{t \in \mathcal{B}_b} r_t \cdot \mathbb{I}[a_t = a]\]</div>
<p>Where $mathcal{B}_b$ represents the set of interactions in batch $b$.</p>
<p>Non-Stationary Bandits</p>
<p>Address environments where reward distributions change over time using techniques like:</p>
<ul class="simple">
<li><p>Sliding window averaging: $hat{mu}_a = frac{1}{<a href="#id4"><span class="problematic" id="id5">|\mathcal{W}|</span></a>} sum_{t in mathcal{W}} r_t cdot mathbb{I}[a_t = a]$</p></li>
<li><p>Exponential weighting: $hat{mu}_a = frac{sum_{i=1}^{n_a} gamma^{n_a-i} r_i}{sum_{i=1}^{n_a} gamma^{n_a-i}}$</p></li>
</ul>
<p>Where $mathcal{W}$ is a window of recent observations and $gamma in (0,1)$ is a decay factor.</p>
<p>Combinatorial Bandits</p>
<p>Select multiple arms simultaneously under various constraints:</p>
<ul class="simple">
<li><p>Budget constraints: $sum_{a in S_t} c_a leq B$</p></li>
<li><p>Diverse selection: $d(a_i, a_j) geq delta$ for $a_i, a_j in S_t$</p></li>
</ul>
<p>Where $S_t$ is the set of selected arms at round $t$.</p>
</section>
</section>
<section id="training-objectives">
<h3>Training Objectives<a class="headerlink" href="#training-objectives" title="Link to this heading"></a></h3>
<p>For Neural Thompson Sampling with continuous rewards, the primary training objective is to minimize the prediction error while capturing uncertainty:</p>
<ol class="arabic simple" start="2">
<li><p><strong>Heteroscedastic Loss</strong>: When modeling uncertainty, the network can predict both mean and variance:</p></li>
</ol>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\mathcal{L}_{het}(\theta) = \frac{1}{N}\sum_{i=1}^{N}\left(\frac{(r_i - \mu_i)^2}{2\sigma_i^2} + \frac{1}{2}\log \sigma_i^2\right)\]</div>
<p>Where:
- <span class="math notranslate nohighlight">\(\mu_i = f_\theta^{\mu}(\mathbf{u}_i, \mathbf{I}_i, \mathbf{H}_i)\)</span> is the predicted mean reward
- <span class="math notranslate nohighlight">\(\sigma_i^2 = f_\theta^{\sigma}(\mathbf{u}_i, \mathbf{I}_i, \mathbf{H}_i)\)</span> is the predicted variance</p>
</div></blockquote>
<ol class="arabic simple" start="3">
<li><p><strong>Regularization Terms</strong>: To prevent overfitting and encourage exploration:</p></li>
</ol>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\mathcal{L}_{reg}(\theta) = \lambda_1 \|\theta\|_2^2 + \lambda_2 \|\nabla_{\mathbf{x}}f_\theta(\mathbf{x})\|_2^2\]</div>
<p>Where the second term (gradient norm) encourages smoother predictions</p>
</div></blockquote>
<ol class="arabic simple" start="4">
<li><p><strong>Ensemble Diversity Loss</strong>: When training ensemble models:</p></li>
</ol>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\mathcal{L}_{div}(\theta_1,...,\theta_K) = -\lambda_d \sum_{i \neq j} \text{dist}(f_{\theta_i}, f_{\theta_j})\]</div>
<p>Where <span class="math notranslate nohighlight">\(\text{dist}\)</span> measures functional diversity between ensemble members</p>
</div></blockquote>
<p>The final training objective combines these components:</p>
<div class="math notranslate nohighlight">
\[\mathcal{L}(\theta) = \mathcal{L}_{MSE}(\theta) + \mathcal{L}_{reg}(\theta) + \mathcal{L}_{div}(\theta_1,...,\theta_K)\]</div>
<p>Or, when modeling uncertainty directly:</p>
<div class="math notranslate nohighlight">
\[\mathcal{L}(\theta) = \mathcal{L}_{het}(\theta) + \mathcal{L}_{reg}(\theta) + \mathcal{L}_{div}(\theta_1,...,\theta_K)\]</div>
</section>
<section id="implementation-with-neural-thompson-sampling">
<h3>Implementation with Neural Thompson Sampling<a class="headerlink" href="#implementation-with-neural-thompson-sampling" title="Link to this heading"></a></h3>
<ol class="arabic simple">
<li><p><strong>Data Collection</strong>: For each user-item pair, store:</p></li>
</ol>
<blockquote>
<div><ul class="simple">
<li><p>User features (<span class="math notranslate nohighlight">\(\mathbf{u}\)</span>)</p></li>
<li><p>Item features (<span class="math notranslate nohighlight">\(\mathbf{I}\)</span>)</p></li>
<li><p>Sequential interaction history (<span class="math notranslate nohighlight">\(\mathbf{H}\)</span>)</p></li>
<li><p>Observed reward values (<span class="math notranslate nohighlight">\(r\)</span>)</p></li>
<li><p>Timestamps and contextual metadata</p></li>
</ul>
</div></blockquote>
<ol class="arabic simple" start="2">
<li><p><strong>Bayesian Neural Network Training</strong>: Train the model to estimate both reward and uncertainty:</p></li>
</ol>
<blockquote>
<div><div class="math notranslate nohighlight">
\[P(r|\mathbf{u}, \mathbf{I}, \mathbf{H}, \theta) = \mathcal{N}(f_\theta^{\mu}(\mathbf{u}, \mathbf{I}, \mathbf{H}), f_\theta^{\sigma}(\mathbf{u}, \mathbf{I}, \mathbf{H})^2)\]</div>
<p>This distribution represents the model’s confidence about reward predictions for each user-item pair.</p>
</div></blockquote>
<ol class="arabic simple" start="3">
<li><p><strong>Uncertainty Estimation Methods</strong>: Different approaches for approximating posterior sampling:</p></li>
</ol>
<blockquote>
<div><ol class="loweralpha">
<li><p><strong>MC Dropout</strong>: Apply dropout during inference to sample from an approximate posterior:</p>
<div class="math notranslate nohighlight">
\[\hat{r}_{\mathbf{u},\mathbf{I}}^{(s)} = f_{\theta, \text{dropout}}(\mathbf{u}, \mathbf{I}, \mathbf{H})\]</div>
<p>for <span class="math notranslate nohighlight">\(s = 1, 2, \ldots, S\)</span> samples</p>
</li>
<li><p><strong>Ensemble Sampling</strong>: Maintain <span class="math notranslate nohighlight">\(K\)</span> neural networks trained with different initializations:</p>
<div class="math notranslate nohighlight">
\[\hat{r}_{\mathbf{u},\mathbf{I}}^{(k)} = f_{\theta_k}(\mathbf{u}, \mathbf{I}, \mathbf{H})\]</div>
<p>For inference, randomly select one network <span class="math notranslate nohighlight">\(k \in \{1,2,...,K\}\)</span> to generate predictions</p>
</li>
</ol>
</div></blockquote>
<ol class="arabic simple" start="4">
<li><p><strong>Real-time Serving Process</strong>:</p></li>
</ol>
<blockquote>
<div><ol class="loweralpha simple">
<li><p>Retrieve user context and interaction history</p></li>
<li><p>Generate candidate items for recommendation</p></li>
<li><p>For each candidate item set:
- Sample from the posterior distribution using chosen uncertainty method
- Rank items based on sampled rewards</p></li>
<li><p>Apply diversity adjustments to create the final recommendation slate</p></li>
<li><p>Serve recommendations and collect new interaction data for continuous learning</p></li>
</ol>
</div></blockquote>
<ol class="arabic simple" start="3">
<li><p><strong>Uncertainty Estimation</strong>: For Thompson Sampling with neural networks, we need posterior uncertainty estimates. This can be done through:</p></li>
</ol>
<blockquote>
<div><ol class="loweralpha">
<li><p><strong>MC Dropout</strong>: Apply dropout during inference to sample from an approximate posterior</p>
<div class="math notranslate nohighlight">
\[\hat{r}_{u,i}^{(s)} = f_{\theta, \text{dropout}}(\mathbf{x}_{u,i})\]</div>
<p>for <span class="math notranslate nohighlight">\(s = 1, 2, \ldots, S\)</span> samples</p>
</li>
<li><p><strong>Neural Ensemble</strong>: Maintain <span class="math notranslate nohighlight">\(K\)</span> neural networks trained on bootstrap samples or with different initializations</p>
<div class="math notranslate nohighlight">
\[\hat{r}_{u,i}^{(k)} = f_{\theta_k}(\mathbf{x}_{u,i})\]</div>
<p>for <span class="math notranslate nohighlight">\(k = 1, 2, \ldots, K\)</span> ensemble members</p>
</li>
</ol>
</div></blockquote>
<ol class="arabic simple" start="4">
<li><p><strong>Real-time Serving</strong>:</p></li>
</ol>
<blockquote>
<div><ul class="simple">
<li><p>When user <span class="math notranslate nohighlight">\(u\)</span> views product <span class="math notranslate nohighlight">\(p\)</span>, generate context features including user profile, current item, and interaction history</p></li>
<li><p>For each candidate product <span class="math notranslate nohighlight">\(a\)</span> in the catalog:
- Sample a reward prediction using MC dropout or by selecting one random network from the ensemble
- Calculate expected reward with uncertainty</p></li>
<li><p>Recommend top 5 products by sampled reward, ensuring diversity</p></li>
</ul>
</div></blockquote>
</section>
<section id="handling-combinatorial-actions-and-exploration">
<h3>Handling Combinatorial Actions and Exploration<a class="headerlink" href="#handling-combinatorial-actions-and-exploration" title="Link to this heading"></a></h3>
<p>For recommending multiple items simultaneously (slate recommendation):</p>
<ol class="arabic simple">
<li><p><strong>Diverse Sampling Strategy</strong>: Instead of greedily selecting top-k items:</p></li>
</ol>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\pi(\mathbf{a} | \mathbf{x}) = \prod_{i=1}^{k} \frac{\exp(\hat{r}_{u,a_i}/\tau)}{\sum_{j \notin \{a_1,...,a_{i-1}\}} \exp(\hat{r}_{u,j}/\tau)}\]</div>
<p>Where <span class="math notranslate nohighlight">\(\tau\)</span> is a temperature parameter controlling exploration</p>
</div></blockquote>
<ol class="arabic simple" start="2">
<li><p><strong>Submodular Diversity Function</strong>: Incorporate a diversity term to maximize slate coverage:</p></li>
</ol>
<blockquote>
<div><div class="math notranslate nohighlight">
\[S(\mathbf{a}) = \sum_{i=1}^{k} \hat{r}_{u,a_i} - \lambda \sum_{i=1}^{k}\sum_{j=1, j \neq i}^{k} \text{sim}(a_i, a_j)\]</div>
<p>Where <span class="math notranslate nohighlight">\(\text{sim}(a_i, a_j)\)</span> measures similarity between items and <span class="math notranslate nohighlight">\(\lambda\)</span> controls diversity weight</p>
</div></blockquote>
<ol class="arabic simple" start="3">
<li><p><strong>Thompson Sampling with IPS Correction</strong>: For partial feedback (user engages with only some recommendations):</p></li>
</ol>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\hat{r}_{IPS} = \frac{r_{observed}}{\pi(a_{observed} | \mathbf{x})}\]</div>
<p>This inverse propensity scoring allows learning from partial observations</p>
</div></blockquote>
</section>
<section id="continuous-learning-and-adaptation">
<h3>Continuous Learning and Adaptation<a class="headerlink" href="#continuous-learning-and-adaptation" title="Link to this heading"></a></h3>
<p>The transformer-based neural bandit implements advanced techniques for online learning:</p>
<ol class="arabic simple">
<li><p><strong>Streaming Batch Updates</strong>: The model parameters are updated in mini-batches as new data arrives:</p></li>
</ol>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\theta_{t+1} = \theta_t - \eta \nabla_\theta \mathcal{L}(\theta_t, \mathcal{D}_t)\]</div>
<p>Where <span class="math notranslate nohighlight">\(\mathcal{D}_t\)</span> is a batch of recent interactions</p>
</div></blockquote>
<ol class="arabic simple" start="2">
<li><p><strong>Catastrophic Forgetting Mitigation</strong>: Implement experience replay to maintain performance on older patterns:</p></li>
</ol>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\mathcal{L}_{replay}(\theta) = (1-\alpha)\mathcal{L}(\theta, \mathcal{D}_{recent}) + \alpha\mathcal{L}(\theta, \mathcal{D}_{memory})\]</div>
<p>Where <span class="math notranslate nohighlight">\(\alpha\)</span> controls the mixture of recent and memory samples</p>
</div></blockquote>
<ol class="arabic simple" start="3">
<li><p><strong>Distributional Shift Detection</strong>: The system monitors feature distribution changes and triggers retraining:</p></li>
</ol>
<blockquote>
<div><div class="math notranslate nohighlight">
\[D_{KL}(P_{t}(\mathbf{x}) || P_{t-\Delta}(\mathbf{x})) &gt; \tau_{shift}\]</div>
<p>Where <span class="math notranslate nohighlight">\(D_{KL}\)</span> is the Kullback-Leibler divergence between current and historical feature distributions</p>
</div></blockquote>
<section id="policy-networks">
<h4>Policy Networks<a class="headerlink" href="#policy-networks" title="Link to this heading"></a></h4>
</section>
<section id="deep-q-networks">
<h4>Deep Q-Networks<a class="headerlink" href="#deep-q-networks" title="Link to this heading"></a></h4>
</section>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="03_supervised_learning.html" class="btn btn-neutral float-left" title="Supervised Learning" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="../../evaluation/index.html" class="btn btn-neutral float-right" title="Evaluation" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2025, Tony.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>