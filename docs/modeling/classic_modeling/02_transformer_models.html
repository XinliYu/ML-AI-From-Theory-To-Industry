

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Transformer Models &mdash; MLAI 0.1 documentation</title>
      <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=80d5e7a1" />
      <link rel="stylesheet" type="text/css" href="../../_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="../../_static/custom.css?v=3eba48d4" />

  
    <link rel="canonical" href="https://xinliyu.github.io/ML-AI-From-Theory-To-Industry/modeling/classic_modeling/02_transformer_models.html" />
      <script src="../../_static/jquery.js?v=5d32c60e"></script>
      <script src="../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../../_static/documentation_options.js?v=2709fde1"></script>
      <script src="../../_static/doctools.js?v=9bcbadda"></script>
      <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
      <script src="../../_static/foldable_admonitions.js?v=351fa817"></script>
      <script>window.MathJax = {"tex": {"inlineMath": [["$", "$"]], "displayMath": [["$$", "$$"]]}, "options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
      <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      <script src="../../_static/js/mathjax-config.js?v=c54ad740"></script>
      <script src="https://unpkg.com/react@17/umd/react.production.min.js"></script>
      <script src="https://unpkg.com/react-dom@17/umd/react-dom.production.min.js"></script>
    <script src="../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Supervised Learning" href="03_supervised_learning.html" />
    <link rel="prev" title="Data Preparation" href="01_data_preparation.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../index.html" class="icon icon-home">
            MLAI
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../system_design/index.html">ML/AI Systen Design</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../index.html">Modeling</a><ul class="current">
<li class="toctree-l2 current"><a class="reference internal" href="index.html">Classic Modeling</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="01_data_preparation.html">Data Preparation</a></li>
<li class="toctree-l3 current"><a class="current reference internal" href="#">Transformer Models</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#transformer-architecture">Transformer Architecture</a><ul>
<li class="toctree-l5"><a class="reference internal" href="#multi-head-attention">Multi-Head Attention</a></li>
<li class="toctree-l5"><a class="reference internal" href="#positional-encoding">Positional Encoding</a></li>
<li class="toctree-l5"><a class="reference internal" href="#positionwise-feed-forward-networks">Positionwise Feed-Forward Networks</a></li>
<li class="toctree-l5"><a class="reference internal" href="#layer-normalization">Layer Normalization</a></li>
<li class="toctree-l5"><a class="reference internal" href="#encoder-decoder-architecture">Encoder-Decoder Architecture</a></li>
</ul>
</li>
<li class="toctree-l4"><a class="reference internal" href="#gpt-architecture">GPT Architecture</a><ul>
<li class="toctree-l5"><a class="reference internal" href="#gpt-decoder">GPT Decoder</a></li>
<li class="toctree-l5"><a class="reference internal" href="#generation-strategies">Generation Strategies</a><ul>
<li class="toctree-l6"><a class="reference internal" href="#greedy-search">Greedy Search</a></li>
<li class="toctree-l6"><a class="reference internal" href="#sampling-based-generation">Sampling-Based Generation</a></li>
<li class="toctree-l6"><a class="reference internal" href="#beam-search">Beam Search</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="03_supervised_learning.html">Supervised Learning</a></li>
<li class="toctree-l3"><a class="reference internal" href="04_reinforcement_learning.html">Reinforcement Learning</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../evaluation/index.html">Evaluation</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">MLAI</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../index.html">Modeling</a></li>
          <li class="breadcrumb-item"><a href="index.html">Classic Modeling</a></li>
      <li class="breadcrumb-item active">Transformer Models</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../_sources/modeling/classic_modeling/02_transformer_models.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="transformer-models">
<h1>Transformer Models<a class="headerlink" href="#transformer-models" title="Link to this heading"></a></h1>
<section id="transformer-architecture">
<h2>Transformer Architecture<a class="headerlink" href="#transformer-architecture" title="Link to this heading"></a></h2>
<p>The <span class="target" id="newconcept-transformer_architecture"></span><span class="newconcept">Transformer Architecture</span> has become a foundational building block in modern deep learning, particularly for sequence modeling tasks in natural language processing, computer vision, and search/recommendation systems. Unlike the previously popular architecture <span class="target" id="newconcept-recurrent_neural_networks"></span><span class="newconcept">Recurrent Neural Networks (RNNs)</span>, transformers process all elements of a sequence in parallel through <span class="target" id="newconcept-self-attention"></span><span class="newconcept">self-attention</span> mechanisms. This approach enables more efficient training and better captures long-range dependencies within data. The following is an executable complete example code.</p>
<div class="folding highlight-python notranslate" id="complete-example-transformer-architecture"><div class="highlight"><pre><span></span><span class="linenos">  1</span><span class="kn">import</span> <span class="nn">torch</span>
<span class="linenos">  2</span><span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="linenos">  3</span><span class="kn">import</span> <span class="nn">math</span>
<span class="linenos">  4</span>
<span class="linenos">  5</span>
<span class="linenos">  6</span><span class="k">class</span> <span class="nc">MultiHeadAttention</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="linenos">  7</span><span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="linenos">  8</span><span class="sd">    Multi-head attention mechanism that splits the model dimension (d_model)</span>
<span class="linenos">  9</span><span class="sd">    into multiple heads, each head computing scaled dot-product attention in parallel.</span>
<span class="linenos"> 10</span><span class="sd">    The outputs of all heads are then concatenated and projected back to d_model.</span>
<span class="linenos"> 11</span>
<span class="linenos"> 12</span><span class="sd">    Motivation:</span>
<span class="linenos"> 13</span><span class="sd">        Having multiple heads allows the model to attend to different features</span>
<span class="linenos"> 14</span><span class="sd">        or subspaces of the input representation independently.</span>
<span class="linenos"> 15</span><span class="sd">    &quot;&quot;&quot;</span>
<span class="linenos"> 16</span>
<span class="linenos"> 17</span>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">):</span>
<span class="linenos"> 18</span>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="linenos"> 19</span>        <span class="k">assert</span> <span class="n">d_model</span> <span class="o">%</span> <span class="n">num_heads</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="s2">&quot;d_model must be divisible by num_heads&quot;</span>
<span class="linenos"> 20</span>
<span class="linenos"> 21</span>        <span class="bp">self</span><span class="o">.</span><span class="n">d_model</span> <span class="o">=</span> <span class="n">d_model</span>
<span class="linenos"> 22</span>        <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">num_heads</span>
<span class="linenos"> 23</span>        <span class="bp">self</span><span class="o">.</span><span class="n">d_head</span> <span class="o">=</span> <span class="n">d_model</span> <span class="o">//</span> <span class="n">num_heads</span>
<span class="linenos"> 24</span>
<span class="linenos"> 25</span>        <span class="bp">self</span><span class="o">.</span><span class="n">W_q</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
<span class="linenos"> 26</span>        <span class="bp">self</span><span class="o">.</span><span class="n">W_k</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
<span class="linenos"> 27</span>        <span class="bp">self</span><span class="o">.</span><span class="n">W_v</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
<span class="linenos"> 28</span>        <span class="bp">self</span><span class="o">.</span><span class="n">W_o</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
<span class="linenos"> 29</span>
<span class="linenos"> 30</span>    <span class="k">def</span> <span class="nf">scaled_dot_product_attention</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">V</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="linenos"> 31</span><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="linenos"> 32</span><span class="sd">        Compute scaled dot-product attention between query Q, key K, and value V.</span>
<span class="linenos"> 33</span><span class="sd">        Typically used inside each attention head.</span>
<span class="linenos"> 34</span>
<span class="linenos"> 35</span><span class="sd">        Scaled-doc product can be viewed as soft retrieval, where we obtain probabilistic scores for values in V through Q and K.</span>
<span class="linenos"> 36</span><span class="sd">        The probabilistic scores are the attention scores.</span>
<span class="linenos"> 37</span>
<span class="linenos"> 38</span><span class="sd">        Args:</span>
<span class="linenos"> 39</span><span class="sd">            Q (Tensor):</span>
<span class="linenos"> 40</span><span class="sd">                Shape (batch_size, num_heads, query_size, d_head).</span>
<span class="linenos"> 41</span><span class="sd">            K (Tensor):</span>
<span class="linenos"> 42</span><span class="sd">                Shape (batch_size, num_heads, index_size, d_head).</span>
<span class="linenos"> 43</span><span class="sd">            V (Tensor):</span>
<span class="linenos"> 44</span><span class="sd">                Shape (batch_size, num_heads, index_size, d_head).</span>
<span class="linenos"> 45</span><span class="sd">            mask (Tensor, optional):</span>
<span class="linenos"> 46</span><span class="sd">                Shape broadcastable to (batch_size, 1, query_size, index_size),</span>
<span class="linenos"> 47</span><span class="sd">                typically a padding of size (batch_size, 1, 1, index_size),</span>
<span class="linenos"> 48</span><span class="sd">                with zero (False) entries indicating positions to mask out.</span>
<span class="linenos"> 49</span>
<span class="linenos"> 50</span><span class="sd">        Returns:</span>
<span class="linenos"> 51</span><span class="sd">            output (Tensor):</span>
<span class="linenos"> 52</span><span class="sd">                Shape (batch_size, num_heads, query_size, d_head).</span>
<span class="linenos"> 53</span><span class="sd">            attn_weights (Tensor):</span>
<span class="linenos"> 54</span><span class="sd">                Shape (batch_size, num_heads, query_size, index_size).</span>
<span class="linenos"> 55</span><span class="sd">                These are the softmax-normalized attention scores.</span>
<span class="linenos"> 56</span><span class="sd">        &quot;&quot;&quot;</span>
<span class="linenos"> 57</span>
<span class="linenos"> 58</span>
<span class="linenos"> 59</span>        <span class="c1"># -----------------------</span>
<span class="linenos"> 60</span>        <span class="c1"># 1) Compute raw attention scores</span>
<span class="linenos"> 61</span>        <span class="c1"># -----------------------</span>
<span class="linenos"> 62</span>        <span class="c1"># Calculate raw attention scores (before applying softmax).</span>
<span class="linenos"> 63</span>        <span class="c1"># We use negative indexes to indicate transposing the last two dimension of K. We do not use positive indexes so the code can work in a more general way, as K might have 3 or 4 dimensions with &quot;multi-head&quot; and &quot;batch&quot;.</span>
<span class="linenos"> 64</span>        <span class="c1"># Dividing by the square root of `d_head` is recommended by the paper is to prevent the raw attention scores from becoming too large for large dimensions.</span>
<span class="linenos"> 65</span>        <span class="c1"># The `attn_scores` dimensions are (`batch_size`, `num_heads`, `query_size`, `index_size`)</span>
<span class="linenos"> 66</span>        <span class="c1"># (or (`batch_size`, `num_heads`, `seq_length`, `seq_length`) in the sequence processing case).</span>
<span class="linenos"> 67</span>        <span class="n">attn_scores</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">d_head</span><span class="p">)</span>
<span class="linenos"> 68</span>
<span class="linenos"> 69</span>        <span class="c1"># -----------------------</span>
<span class="linenos"> 70</span>        <span class="c1"># 2) Mask out invalid positions (if any)</span>
<span class="linenos"> 71</span>        <span class="c1"># -----------------------</span>
<span class="linenos"> 72</span>        <span class="k">if</span> <span class="n">mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
<span class="linenos"> 73</span>            <span class="c1"># We can mask out keys/values by replacing their attention scores to near-zero values</span>
<span class="linenos"> 74</span>            <span class="c1"># Typically a padding mask of size (batch_size, 1, 1, index_size),</span>
<span class="linenos"> 75</span>            <span class="c1">#  but any mask broadcastable to (batch_size, 1, query_size, index_size) should work here.</span>
<span class="linenos"> 76</span>            <span class="n">attn_scores</span> <span class="o">=</span> <span class="n">attn_scores</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="n">mask</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mf">1e9</span><span class="p">)</span>
<span class="linenos"> 77</span>
<span class="linenos"> 78</span>        <span class="c1"># -----------------------</span>
<span class="linenos"> 79</span>        <span class="c1"># 3) Apply softmax to get normalized weights</span>
<span class="linenos"> 80</span>        <span class="c1"># -----------------------</span>
<span class="linenos"> 81</span>        <span class="c1"># Apply softmax along the last dimension / feature dimension, i.e. normalizing each row)</span>
<span class="linenos"> 82</span>        <span class="n">attn_weights</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">attn_scores</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="linenos"> 83</span>
<span class="linenos"> 84</span>        <span class="c1"># -----------------------</span>
<span class="linenos"> 85</span>        <span class="c1"># 4) Multiply weights by values (weighted sum of V)</span>
<span class="linenos"> 86</span>        <span class="c1"># -----------------------</span>
<span class="linenos"> 87</span>        <span class="c1"># Apply attention weights to values.</span>
<span class="linenos"> 88</span>        <span class="c1"># The `output` dimensions are (`batch_size`, `num_heads`, `query_size`, `d_head`)</span>
<span class="linenos"> 89</span>        <span class="c1"># (or (`batch_size`, `num_heads`, `seq_length`, `d_head`) in the sequence processing case),</span>
<span class="linenos"> 90</span>        <span class="c1"># which can be viewed as a weighted sum of features (rows) of V, weighted by the `attn_weights`,</span>
<span class="linenos"> 91</span>        <span class="c1"># or as if it had &quot;retrieved&quot; features from V in a weighted way.</span>
<span class="linenos"> 92</span>        <span class="n">output</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">attn_weights</span><span class="p">,</span> <span class="n">V</span><span class="p">)</span>
<span class="linenos"> 93</span>        <span class="k">return</span> <span class="n">output</span><span class="p">,</span> <span class="n">attn_weights</span>
<span class="linenos"> 94</span>
<span class="linenos"> 95</span>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">V</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="linenos"> 96</span><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="linenos"> 97</span><span class="sd">        Forward pass of multi-head attention. Splits Q, K, and V into num_heads</span>
<span class="linenos"> 98</span><span class="sd">        chunks, applies scaled dot-product attention for each head, then</span>
<span class="linenos"> 99</span><span class="sd">        concatenates the heads back into d_model.</span>
<span class="linenos">100</span>
<span class="linenos">101</span><span class="sd">        Args:</span>
<span class="linenos">102</span><span class="sd">            Q (Tensor): (batch_size, query_size, d_model)</span>
<span class="linenos">103</span><span class="sd">            K (Tensor): (batch_size, index_size, d_model)</span>
<span class="linenos">104</span><span class="sd">            V (Tensor): (batch_size, index_size, d_model)</span>
<span class="linenos">105</span><span class="sd">            mask (Tensor, optional):</span>
<span class="linenos">106</span><span class="sd">                Shape broadcastable to (batch_size, 1, query_size, index_size),</span>
<span class="linenos">107</span><span class="sd">                typically a padding of size (batch_size, 1, 1, index_size),</span>
<span class="linenos">108</span><span class="sd">                with zero (False) entries indicating positions to mask out.</span>
<span class="linenos">109</span><span class="sd">        Returns:</span>
<span class="linenos">110</span><span class="sd">            output (Tensor):</span>
<span class="linenos">111</span><span class="sd">                (batch_size, query_size, d_model)</span>
<span class="linenos">112</span><span class="sd">            attn_weights (Tensor):</span>
<span class="linenos">113</span><span class="sd">                (batch_size, num_heads, query_size, index_size)</span>
<span class="linenos">114</span><span class="sd">        &quot;&quot;&quot;</span>
<span class="linenos">115</span>        <span class="n">batch_size</span> <span class="o">=</span> <span class="n">Q</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="linenos">116</span>
<span class="linenos">117</span>        <span class="c1"># -----------------------</span>
<span class="linenos">118</span>        <span class="c1"># 1) Linear projections + reshape into heads</span>
<span class="linenos">119</span>        <span class="c1"># -----------------------</span>
<span class="linenos">120</span>        <span class="c1"># 1. Keep the batch size.</span>
<span class="linenos">121</span>        <span class="c1"># 2. Breaking down the last dimension (the feature dimension) to two dimensions of size `num_heads` and `d_head`.</span>
<span class="linenos">122</span>        <span class="c1"># 3. The `-1` means the `view` function will infer the corresponding dimension (the index size dimension).</span>
<span class="linenos">123</span>        <span class="c1"># Transposing the index size dimension (dimension 1) and the `num_heads` dimension (dimension 2) is for computational convenience</span>
<span class="linenos">124</span>        <span class="c1"># because the scaled-doc product will be performed per batch per head.</span>
<span class="linenos">125</span>        <span class="c1"># After transposing, the dimensions are (`batch_size`, `num_heads`, `query_size`, `d_head`) for Q, and (`batch_size`, `num_heads`, `index_size`, `d_head`) for K, V</span>
<span class="linenos">126</span>        <span class="c1"># (or all (`batch_size`, `num_heads`, `seq_length`, `d_head`) in the sequence self-attention case).</span>
<span class="linenos">127</span>        <span class="n">Q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">W_q</span><span class="p">(</span><span class="n">Q</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_head</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="linenos">128</span>        <span class="n">K</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">W_k</span><span class="p">(</span><span class="n">K</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_head</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="linenos">129</span>        <span class="n">V</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">W_v</span><span class="p">(</span><span class="n">V</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_head</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="linenos">130</span>
<span class="linenos">131</span>        <span class="c1"># -----------------------</span>
<span class="linenos">132</span>        <span class="c1"># 2) Scaled dot-product attention</span>
<span class="linenos">133</span>        <span class="c1"># -----------------------</span>
<span class="linenos">134</span>        <span class="c1"># The `output` dimensions are (`batch_size`, `num_heads`, `query_size`, `d_head`)</span>
<span class="linenos">135</span>        <span class="c1"># (or (`batch_size`, `num_heads`, `seq_length`, `d_head`) in the sequence processing case),</span>
<span class="linenos">136</span>        <span class="c1"># The `attn_scores` dimensions are (`batch_size`, `num_heads`, `query_size`, `index_size`)</span>
<span class="linenos">137</span>        <span class="c1"># (or (`batch_size`, `num_heads`, `seq_length`, `seq_length`) in the sequence processing case).</span>
<span class="linenos">138</span>        <span class="n">output</span><span class="p">,</span> <span class="n">attn_weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">scaled_dot_product_attention</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">V</span><span class="p">,</span> <span class="n">mask</span><span class="p">)</span>
<span class="linenos">139</span>
<span class="linenos">140</span>        <span class="c1"># -----------------------</span>
<span class="linenos">141</span>        <span class="c1"># 3) Recombine heads</span>
<span class="linenos">142</span>        <span class="c1"># -----------------------</span>
<span class="linenos">143</span>        <span class="c1"># Combine the heads and reshape back to original input embedding dimension sizes.</span>
<span class="linenos">144</span>        <span class="c1"># 1. Transposing the dimensions back to (`batch_size`, `query_size`, `num_heads`, `d_head`) (or (`batch_size`, `seq_length`, `num_heads`, `d_head`) in the sequence processing case).</span>
<span class="linenos">145</span>        <span class="c1"># 2. The `contiguous` method enforces the underlying tensor representation be continuous.</span>
<span class="linenos">146</span>        <span class="c1"># 3. The `view` method merges the last two dimensions, so it becomes (`batch_size`, `query_size`, `d_model`) (or (`batch_size`, `seq_length`, `d_model`) in the sequence processing case).</span>
<span class="linenos">147</span>        <span class="n">output</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_model</span><span class="p">)</span>
<span class="linenos">148</span>
<span class="linenos">149</span>        <span class="c1"># -----------------------</span>
<span class="linenos">150</span>        <span class="c1"># 4) Final linear projection</span>
<span class="linenos">151</span>        <span class="c1"># -----------------------</span>
<span class="linenos">152</span>        <span class="c1"># Apply output transformation on the `d_model` dimension.</span>
<span class="linenos">153</span>        <span class="c1"># `output` dimensions unchanged.</span>
<span class="linenos">154</span>        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">W_o</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
<span class="linenos">155</span>
<span class="linenos">156</span>        <span class="c1"># NOTE:</span>
<span class="linenos">157</span>        <span class="c1"># The above approach is mathematically equivalent to perform multiple Q/K/V scaled-dot-products and then concatenate them.</span>
<span class="linenos">158</span>        <span class="c1"># However, above view/transpose operations are more efficient implementation.</span>
<span class="linenos">159</span>
<span class="linenos">160</span>        <span class="k">return</span> <span class="n">output</span><span class="p">,</span> <span class="n">attn_weights</span>
<span class="linenos">161</span>
<span class="linenos">162</span>
<span class="linenos">163</span><span class="k">class</span> <span class="nc">PositionWiseFeedForward</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="linenos">164</span><span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="linenos">165</span><span class="sd">    A feedforward network applied independently at each time step (position).</span>
<span class="linenos">166</span><span class="sd">    In other words, for each position in the sequence, the same two-layer MLP is used:</span>
<span class="linenos">167</span><span class="sd">      1) Linear transform: d_model -&gt; d_ff</span>
<span class="linenos">168</span><span class="sd">      2) Non-linear activation (ReLU)</span>
<span class="linenos">169</span><span class="sd">      3) Linear transform: d_ff -&gt; d_model</span>
<span class="linenos">170</span>
<span class="linenos">171</span><span class="sd">    d_ff is larger than d_model. For example, the classic setting is 4x larger.</span>
<span class="linenos">172</span><span class="sd">    This larger hidden dimension in the feed-forward sub-layer helps increase the representational capacity of the network,</span>
<span class="linenos">173</span><span class="sd">    allowing more complex transformations to be applied at each position.</span>
<span class="linenos">174</span><span class="sd">    After this larger intermediate projection, it then projects back down to d_model.</span>
<span class="linenos">175</span>
<span class="linenos">176</span><span class="sd">    Because it is applied &quot;position-wise,&quot; there&#39;s no interaction across different sequence positions here.</span>
<span class="linenos">177</span><span class="sd">    &quot;&quot;&quot;</span>
<span class="linenos">178</span>
<span class="linenos">179</span>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">):</span>
<span class="linenos">180</span>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="linenos">181</span>        <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">)</span>
<span class="linenos">182</span>        <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_ff</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
<span class="linenos">183</span>        <span class="bp">self</span><span class="o">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>
<span class="linenos">184</span>
<span class="linenos">185</span>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="linenos">186</span><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="linenos">187</span><span class="sd">        Forward pass through the position-wise feed-forward network.</span>
<span class="linenos">188</span>
<span class="linenos">189</span><span class="sd">        Args:</span>
<span class="linenos">190</span><span class="sd">            x (Tensor):</span>
<span class="linenos">191</span><span class="sd">                Input tensor of shape (batch_size, seq_length, d_model).</span>
<span class="linenos">192</span>
<span class="linenos">193</span><span class="sd">        Returns:</span>
<span class="linenos">194</span><span class="sd">            (Tensor):</span>
<span class="linenos">195</span><span class="sd">                Output tensor of shape (batch_size, seq_length, d_model).</span>
<span class="linenos">196</span><span class="sd">        &quot;&quot;&quot;</span>
<span class="linenos">197</span>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
<span class="linenos">198</span>
<span class="linenos">199</span>
<span class="linenos">200</span><span class="k">class</span> <span class="nc">PositionalEncoding</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="linenos">201</span><span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="linenos">202</span><span class="sd">    Positional encoding injects information about the absolute or relative position</span>
<span class="linenos">203</span><span class="sd">    of tokens in the sequence, enabling the model to capture ordering context.</span>
<span class="linenos">204</span>
<span class="linenos">205</span><span class="sd">    The sine/cosine pattern allows the model to generalize to positions beyond</span>
<span class="linenos">206</span><span class="sd">    the training range and provides unique encodings for each position.</span>
<span class="linenos">207</span>
<span class="linenos">208</span><span class="sd">    Implementation details:</span>
<span class="linenos">209</span><span class="sd">      - Sine values assigned to even indices.</span>
<span class="linenos">210</span><span class="sd">      - Cosine values assigned to odd indices.</span>
<span class="linenos">211</span><span class="sd">      - Dimensions with higher indices (features) oscillate faster.</span>
<span class="linenos">212</span><span class="sd">      - Final tensor is registered as a buffer, so it&#39;s not trainable.</span>
<span class="linenos">213</span><span class="sd">    &quot;&quot;&quot;</span>
<span class="linenos">214</span>
<span class="linenos">215</span>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">max_seq_length</span><span class="o">=</span><span class="mi">5000</span><span class="p">):</span>
<span class="linenos">216</span>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="linenos">217</span>
<span class="linenos">218</span>        <span class="n">pe</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">max_seq_length</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
<span class="linenos">219</span>        <span class="n">position</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">max_seq_length</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="linenos">220</span>        <span class="n">div_term</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span> <span class="o">*</span> <span class="p">(</span><span class="o">-</span><span class="n">math</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mf">10000.0</span><span class="p">)</span> <span class="o">/</span> <span class="n">d_model</span><span class="p">))</span>
<span class="linenos">221</span>
<span class="linenos">222</span>        <span class="c1"># the following uses start:stop:step notation</span>
<span class="linenos">223</span>        <span class="n">pe</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">position</span> <span class="o">*</span> <span class="n">div_term</span><span class="p">)</span>  <span class="c1"># Assigns sine values to even indices (0,2,4,...)</span>
<span class="linenos">224</span>        <span class="n">pe</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">position</span> <span class="o">*</span> <span class="n">div_term</span><span class="p">)</span>  <span class="c1"># Assigns cosine values to odd indices (1,3,5,...)</span>
<span class="linenos">225</span>
<span class="linenos">226</span>        <span class="c1"># The original pe tensor has shape [`max_seq_length`, `d_model`],</span>
<span class="linenos">227</span>        <span class="c1"># and `unsqueeze(0)` adds a new dimension at index 0.</span>
<span class="linenos">228</span>        <span class="c1"># After unsqueeze, shape becomes [1, `max_seq_length`, `d_model`],</span>
<span class="linenos">229</span>        <span class="c1"># and this extra dimension allows for batch processing.</span>
<span class="linenos">230</span>
<span class="linenos">231</span>        <span class="c1"># `self.register_buffer` in PyTorch is a method used to register a tensor as a buffer</span>
<span class="linenos">232</span>        <span class="c1"># that should be saved along with model parameters during model.state_dict() calls,</span>
<span class="linenos">233</span>        <span class="c1"># but is not considered a model parameter (meaning it doesn&#39;t receive gradients during backpropagation).</span>
<span class="linenos">234</span>        <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s1">&#39;pe&#39;</span><span class="p">,</span> <span class="n">pe</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>
<span class="linenos">235</span>
<span class="linenos">236</span>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="linenos">237</span><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="linenos">238</span><span class="sd">        Add positional encoding to input embeddings.</span>
<span class="linenos">239</span>
<span class="linenos">240</span><span class="sd">        Args:</span>
<span class="linenos">241</span><span class="sd">            x (Tensor):</span>
<span class="linenos">242</span><span class="sd">                Shape (batch_size, seq_length, d_model).</span>
<span class="linenos">243</span>
<span class="linenos">244</span><span class="sd">        Returns:</span>
<span class="linenos">245</span><span class="sd">            (Tensor):</span>
<span class="linenos">246</span><span class="sd">                Same shape as input, with positional encodings added elementwise.</span>
<span class="linenos">247</span><span class="sd">        &quot;&quot;&quot;</span>
<span class="linenos">248</span>
<span class="linenos">249</span>        <span class="c1"># If x has shape [`batch_size`, `seq_length`, `d_model`]</span>
<span class="linenos">250</span>        <span class="c1"># pe[:, :x.size(1)] will broadcast from [1, `seq_length`, `d_model`]</span>
<span class="linenos">251</span>        <span class="c1"># to match x&#39;s shape.</span>
<span class="linenos">252</span>        <span class="k">return</span> <span class="n">x</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">pe</span><span class="p">[:,</span> <span class="p">:</span><span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)]</span>
<span class="linenos">253</span>
<span class="linenos">254</span>
<span class="linenos">255</span><span class="k">class</span> <span class="nc">EncoderLayer</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="linenos">256</span><span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="linenos">257</span><span class="sd">    The Encoder layer is composed of:</span>
<span class="linenos">258</span><span class="sd">    1) Multi-head self-attention over the input sequence.</span>
<span class="linenos">259</span><span class="sd">    2) Position-wise feed-forward network.</span>
<span class="linenos">260</span><span class="sd">    3) Each sub-layer is followed by a residual connection and LayerNorm.</span>
<span class="linenos">261</span>
<span class="linenos">262</span><span class="sd">    The output size is the same as the input embeddings shape: (batch_size, src_seq_len, d_model).</span>
<span class="linenos">263</span><span class="sd">    &quot;&quot;&quot;</span>
<span class="linenos">264</span>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
<span class="linenos">265</span>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="linenos">266</span>        <span class="bp">self</span><span class="o">.</span><span class="n">self_attn</span> <span class="o">=</span> <span class="n">MultiHeadAttention</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">)</span>
<span class="linenos">267</span>        <span class="bp">self</span><span class="o">.</span><span class="n">feed_forward</span> <span class="o">=</span> <span class="n">PositionWiseFeedForward</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">)</span>
<span class="linenos">268</span>
<span class="linenos">269</span>        <span class="c1"># LayerNorm is normalizing the feature dimension using z-score (with perturbed variance)</span>
<span class="linenos">270</span>        <span class="bp">self</span><span class="o">.</span><span class="n">norm1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>
<span class="linenos">271</span>        <span class="bp">self</span><span class="o">.</span><span class="n">norm2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>
<span class="linenos">272</span>
<span class="linenos">273</span>        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">)</span>
<span class="linenos">274</span>
<span class="linenos">275</span>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="linenos">276</span><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="linenos">277</span><span class="sd">        Args:</span>
<span class="linenos">278</span><span class="sd">            x (Tensor):</span>
<span class="linenos">279</span><span class="sd">                The input embeddings/hidden states of shape (batch_size, src_seq_len, d_model).</span>
<span class="linenos">280</span><span class="sd">            mask (Tensor, optional):</span>
<span class="linenos">281</span><span class="sd">                A padding mask for the source sequence.</span>
<span class="linenos">282</span><span class="sd">                Typically shaped (batch_size, 1, 1, src_seq_len).</span>
<span class="linenos">283</span><span class="sd">                Defaults to None if no mask is used.</span>
<span class="linenos">284</span>
<span class="linenos">285</span><span class="sd">        Returns:</span>
<span class="linenos">286</span><span class="sd">            x (Tensor):</span>
<span class="linenos">287</span><span class="sd">                The output of this encoder layer, shape (batch_size, src_seq_len, d_model).</span>
<span class="linenos">288</span><span class="sd">        &quot;&quot;&quot;</span>
<span class="linenos">289</span>
<span class="linenos">290</span>        <span class="c1"># -----------------------</span>
<span class="linenos">291</span>        <span class="c1"># 1) Self-Attention</span>
<span class="linenos">292</span>        <span class="c1"># -----------------------</span>
<span class="linenos">293</span>        <span class="c1"># Each position in &#39;x&#39; attends to other positions in the same sequence.</span>
<span class="linenos">294</span>        <span class="c1"># The shapes inside attention temporarily become (batch_size, num_heads, src_seq_len, d_head).</span>
<span class="linenos">295</span>        <span class="n">attn_output</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">self_attn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">mask</span><span class="p">)</span>
<span class="linenos">296</span>
<span class="linenos">297</span>        <span class="c1"># Residual connection + dropout + LayerNorm</span>
<span class="linenos">298</span>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm1</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">attn_output</span><span class="p">))</span>
<span class="linenos">299</span>        <span class="c1"># x remains (batch_size, src_seq_len, d_model)</span>
<span class="linenos">300</span>
<span class="linenos">301</span>        <span class="c1"># -----------------------</span>
<span class="linenos">302</span>        <span class="c1"># 2) Position-Wise Feed-Forward</span>
<span class="linenos">303</span>        <span class="c1"># -----------------------</span>
<span class="linenos">304</span>        <span class="c1"># Transform each position from d_model -&gt; d_ff -&gt; d_model independently.</span>
<span class="linenos">305</span>        <span class="n">ff_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">feed_forward</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="linenos">306</span>
<span class="linenos">307</span>        <span class="c1"># Residual connection + dropout + LayerNorm</span>
<span class="linenos">308</span>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm2</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">ff_output</span><span class="p">))</span>
<span class="linenos">309</span>        <span class="c1"># x remains (batch_size, src_seq_len, d_model)</span>
<span class="linenos">310</span>
<span class="linenos">311</span>        <span class="k">return</span> <span class="n">x</span>
<span class="linenos">312</span>
<span class="linenos">313</span>
<span class="linenos">314</span><span class="k">class</span> <span class="nc">DecoderLayer</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="linenos">315</span><span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="linenos">316</span><span class="sd">    The Decoder layer contains:</span>
<span class="linenos">317</span><span class="sd">    1. Masked self-attention over the (partial) target sequence.</span>
<span class="linenos">318</span><span class="sd">    2. Cross-attention over the encoder&#39;s output.</span>
<span class="linenos">319</span><span class="sd">    3. Position-wise feed-forward network.</span>
<span class="linenos">320</span><span class="sd">    4. Each above sub-layer is followed by a residual connection and LayerNorm.</span>
<span class="linenos">321</span>
<span class="linenos">322</span><span class="sd">    The output size is the same as the decoder input embeddings, both (batch_size, target_seq_len, d_model).</span>
<span class="linenos">323</span><span class="sd">    &quot;&quot;&quot;</span>
<span class="linenos">324</span>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
<span class="linenos">325</span>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="linenos">326</span>        <span class="bp">self</span><span class="o">.</span><span class="n">self_attn</span> <span class="o">=</span> <span class="n">MultiHeadAttention</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">)</span>
<span class="linenos">327</span>        <span class="bp">self</span><span class="o">.</span><span class="n">cross_attn</span> <span class="o">=</span> <span class="n">MultiHeadAttention</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">)</span>
<span class="linenos">328</span>        <span class="bp">self</span><span class="o">.</span><span class="n">feed_forward</span> <span class="o">=</span> <span class="n">PositionWiseFeedForward</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">)</span>
<span class="linenos">329</span>
<span class="linenos">330</span>        <span class="bp">self</span><span class="o">.</span><span class="n">norm1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>
<span class="linenos">331</span>        <span class="bp">self</span><span class="o">.</span><span class="n">norm2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>
<span class="linenos">332</span>        <span class="bp">self</span><span class="o">.</span><span class="n">norm3</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>
<span class="linenos">333</span>
<span class="linenos">334</span>        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">)</span>
<span class="linenos">335</span>
<span class="linenos">336</span>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">enc_output</span><span class="p">,</span> <span class="n">src_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">tgt_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="linenos">337</span><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="linenos">338</span><span class="sd">        Args:</span>
<span class="linenos">339</span><span class="sd">            x (Tensor): Decoder input embeddings of shape (batch_size, target_seq_len, d_model).</span>
<span class="linenos">340</span><span class="sd">            enc_output (Tensor): Final encoder outputs of shape (batch_size, source_seq_len, d_model).</span>
<span class="linenos">341</span><span class="sd">            src_mask (Tensor or None): Padding mask for encoder outputs, shape (batch_size, 1, 1, source_seq_len).</span>
<span class="linenos">342</span><span class="sd">            tgt_mask (Tensor or None): Combined padding + causal (future) mask for decoder inputs,</span>
<span class="linenos">343</span><span class="sd">                                       shape (batch_size, 1, target_seq_len, target_seq_len).</span>
<span class="linenos">344</span>
<span class="linenos">345</span><span class="sd">        Returns:</span>
<span class="linenos">346</span><span class="sd">            x (Tensor): Decoder layer output of shape (batch_size, target_seq_len, d_model).</span>
<span class="linenos">347</span><span class="sd">        &quot;&quot;&quot;</span>
<span class="linenos">348</span>
<span class="linenos">349</span>        <span class="c1"># -----------------------</span>
<span class="linenos">350</span>        <span class="c1"># 1) Masked Self-Attention</span>
<span class="linenos">351</span>        <span class="c1"># -----------------------</span>
<span class="linenos">352</span>        <span class="c1"># Decoder attends to itself (partial target sequence).</span>
<span class="linenos">353</span>        <span class="c1"># Causal mask ensures a token can’t attend to future positions.</span>
<span class="linenos">354</span>        <span class="n">self_attn_output</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">self_attn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">tgt_mask</span><span class="p">)</span>
<span class="linenos">355</span>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm1</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">self_attn_output</span><span class="p">))</span>
<span class="linenos">356</span>
<span class="linenos">357</span>        <span class="c1"># -----------------------</span>
<span class="linenos">358</span>        <span class="c1"># 2) Cross-Attention</span>
<span class="linenos">359</span>        <span class="c1"># -----------------------</span>
<span class="linenos">360</span>        <span class="c1"># Each decoder input embedding retrieves a mixture from encoder output</span>
<span class="linenos">361</span>        <span class="n">cross_attn_output</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cross_attn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">enc_output</span><span class="p">,</span> <span class="n">enc_output</span><span class="p">,</span> <span class="n">src_mask</span><span class="p">)</span>
<span class="linenos">362</span>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm2</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">cross_attn_output</span><span class="p">))</span>
<span class="linenos">363</span>
<span class="linenos">364</span>        <span class="c1"># -----------------------</span>
<span class="linenos">365</span>        <span class="c1"># 3) Position-Wise Feed-Forward</span>
<span class="linenos">366</span>        <span class="c1"># -----------------------</span>
<span class="linenos">367</span>        <span class="c1"># Applies two linear layers (d_model -&gt; d_ff -&gt; d_model)</span>
<span class="linenos">368</span>        <span class="c1"># at each position independently.</span>
<span class="linenos">369</span>        <span class="n">ff_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">feed_forward</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="linenos">370</span>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm3</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">ff_output</span><span class="p">))</span>
<span class="linenos">371</span>
<span class="linenos">372</span>        <span class="k">return</span> <span class="n">x</span>
<span class="linenos">373</span>
<span class="linenos">374</span>
<span class="linenos">375</span><span class="k">class</span> <span class="nc">Transformer</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="linenos">376</span><span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="linenos">377</span><span class="sd">    The default Transformer model adopts encoder-decoder architecture, where the encoder input and decode input could be different (e.g., translation).</span>
<span class="linenos">378</span><span class="sd">    &quot;&quot;&quot;</span>
<span class="linenos">379</span>
<span class="linenos">380</span>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">src_vocab_size</span><span class="p">,</span> <span class="n">tgt_vocab_size</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">num_layers</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">,</span> <span class="n">max_seq_length</span><span class="o">=</span><span class="mi">5000</span><span class="p">,</span>
<span class="linenos">381</span>                 <span class="n">dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
<span class="linenos">382</span>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="linenos">383</span>
<span class="linenos">384</span>        <span class="bp">self</span><span class="o">.</span><span class="n">encoder_embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">src_vocab_size</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
<span class="linenos">385</span>        <span class="bp">self</span><span class="o">.</span><span class="n">decoder_embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">tgt_vocab_size</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
<span class="linenos">386</span>        <span class="bp">self</span><span class="o">.</span><span class="n">positional_encoding</span> <span class="o">=</span> <span class="n">PositionalEncoding</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">max_seq_length</span><span class="p">)</span>
<span class="linenos">387</span>
<span class="linenos">388</span>        <span class="bp">self</span><span class="o">.</span><span class="n">encoder_layers</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">([</span>
<span class="linenos">389</span>            <span class="n">EncoderLayer</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">,</span> <span class="n">dropout</span><span class="p">)</span>
<span class="linenos">390</span>            <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_layers</span><span class="p">)</span>
<span class="linenos">391</span>        <span class="p">])</span>
<span class="linenos">392</span>
<span class="linenos">393</span>        <span class="bp">self</span><span class="o">.</span><span class="n">decoder_layers</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">([</span>
<span class="linenos">394</span>            <span class="n">DecoderLayer</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">,</span> <span class="n">dropout</span><span class="p">)</span>
<span class="linenos">395</span>            <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_layers</span><span class="p">)</span>
<span class="linenos">396</span>        <span class="p">])</span>
<span class="linenos">397</span>
<span class="linenos">398</span>        <span class="bp">self</span><span class="o">.</span><span class="n">fc</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">tgt_vocab_size</span><span class="p">)</span>
<span class="linenos">399</span>        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">)</span>
<span class="linenos">400</span>
<span class="linenos">401</span>    <span class="k">def</span> <span class="nf">generate_mask</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">src</span><span class="p">,</span> <span class="n">tgt</span><span class="p">):</span>
<span class="linenos">402</span><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="linenos">403</span><span class="sd">        Generate:</span>
<span class="linenos">404</span><span class="sd">         - src_mask to hide padding tokens in the encoder input.</span>
<span class="linenos">405</span><span class="sd">         - tgt_mask to hide padding and future positions in the decoder input.</span>
<span class="linenos">406</span>
<span class="linenos">407</span><span class="sd">        Args:</span>
<span class="linenos">408</span><span class="sd">            src (Tensor): (batch_size, src_seq_len) of token indices.</span>
<span class="linenos">409</span><span class="sd">            tgt (Tensor): (batch_size, tgt_seq_len) of token indices.</span>
<span class="linenos">410</span>
<span class="linenos">411</span><span class="sd">        Returns:</span>
<span class="linenos">412</span><span class="sd">            src_mask (Tensor): (batch_size, 1, 1, src_seq_len) with 1/True = keep, 0/False = mask out.</span>
<span class="linenos">413</span><span class="sd">            tgt_mask (Tensor): (batch_size, 1, tgt_seq_len, tgt_seq_len) combined padding + causal mask.</span>
<span class="linenos">414</span><span class="sd">        &quot;&quot;&quot;</span>
<span class="linenos">415</span>
<span class="linenos">416</span>        <span class="c1"># The `unsqueeze(1).unsqueeze(2)` operations add extra dimensions to the `src` tensor, transforming its shape from [`batch_size`, `seq_length`] to [`batch_size`, 1, 1, `seq_length`]</span>
<span class="linenos">417</span>        <span class="n">src_mask</span> <span class="o">=</span> <span class="p">(</span><span class="n">src</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="linenos">418</span>
<span class="linenos">419</span>        <span class="c1"># The `unsqueeze(1).unsqueeze(3)` operations add extra dimensions to the `tgt` tensor, transforming its shape from [`batch_size`, `seq_length`] to [`batch_size`, 1, `seq_length`, 1]</span>
<span class="linenos">420</span>        <span class="n">tgt_mask</span> <span class="o">=</span> <span class="p">(</span><span class="n">tgt</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
<span class="linenos">421</span>        <span class="n">seq_length</span> <span class="o">=</span> <span class="n">tgt</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="linenos">422</span>        <span class="c1"># Prevent the decoder from accessing future tokens in the target sequence.</span>
<span class="linenos">423</span>        <span class="c1"># The resulting `causal_mask` is a boolean mask of shape [1, `seq_length`, `seq_length`], with lower triangle (diagonal included) being True and all others being False.</span>
<span class="linenos">424</span>        <span class="c1"># It looks like this for a sequence length of 5:</span>
<span class="linenos">425</span>        <span class="c1"># [[[ True, False, False, False, False],</span>
<span class="linenos">426</span>        <span class="c1">#   [ True,  True, False, False, False],</span>
<span class="linenos">427</span>        <span class="c1">#   [ True,  True,  True, False, False],</span>
<span class="linenos">428</span>        <span class="c1">#   [ True,  True,  True,  True, False],</span>
<span class="linenos">429</span>        <span class="c1">#   [ True,  True,  True,  True,  True]]]</span>
<span class="linenos">430</span>        <span class="c1"># The added extra dimension enables later `tgt_mask &amp; causal_mask` operation.</span>
<span class="linenos">431</span>        <span class="c1"># The `causal_mask`&#39;s new shape [1, `seq_length`, `seq_length`] is broadcastable to `tgt_mask`&#39;s shape [`batch_size`, 1, 1, `seq_length`]</span>
<span class="linenos">432</span>        <span class="n">causal_mask</span> <span class="o">=</span> <span class="o">~</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">triu</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">seq_length</span><span class="p">,</span> <span class="n">seq_length</span><span class="p">),</span> <span class="n">diagonal</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">bool</span><span class="p">())</span>
<span class="linenos">433</span>
<span class="linenos">434</span>        <span class="c1"># Consider the target is represented by single tensor tgt = [[5, 3, 7, 0, 0]] (i.e., batch size is 1), where 0 denotes padding tokens.</span>
<span class="linenos">435</span>        <span class="c1"># The bitwise AND operation results in:</span>
<span class="linenos">436</span>        <span class="c1"># [[[True, False, False, False, False],</span>
<span class="linenos">437</span>        <span class="c1">#    [True,  True, False, False, False],</span>
<span class="linenos">438</span>        <span class="c1">#    [True,  True,  True, False, False],</span>
<span class="linenos">439</span>        <span class="c1">#    [False, False, False, False, False],</span>
<span class="linenos">440</span>        <span class="c1">#    [False, False, False, False, False]]]</span>
<span class="linenos">441</span>        <span class="n">tgt_mask</span> <span class="o">=</span> <span class="n">tgt_mask</span> <span class="o">&amp;</span> <span class="n">causal_mask</span>
<span class="linenos">442</span>
<span class="linenos">443</span>        <span class="k">return</span> <span class="n">src_mask</span><span class="p">,</span> <span class="n">tgt_mask</span>
<span class="linenos">444</span>
<span class="linenos">445</span>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">src</span><span class="p">,</span> <span class="n">tgt</span><span class="p">):</span>
<span class="linenos">446</span><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="linenos">447</span><span class="sd">        Forward pass of the Transformer.</span>
<span class="linenos">448</span>
<span class="linenos">449</span><span class="sd">        Args:</span>
<span class="linenos">450</span><span class="sd">            src (Tensor): (batch_size, src_seq_len) of token indices for the encoder.</span>
<span class="linenos">451</span><span class="sd">            tgt (Tensor): (batch_size, tgt_seq_len) of token indices for the decoder.</span>
<span class="linenos">452</span>
<span class="linenos">453</span><span class="sd">        Returns:</span>
<span class="linenos">454</span><span class="sd">            logits (Tensor): (batch_size, tgt_seq_len, tgt_vocab_size)</span>
<span class="linenos">455</span><span class="sd">                             unnormalized scores for each token in the target sequence.</span>
<span class="linenos">456</span><span class="sd">        &quot;&quot;&quot;</span>
<span class="linenos">457</span>        <span class="c1"># -----------------------</span>
<span class="linenos">458</span>        <span class="c1"># 1) Generate Attention Masks</span>
<span class="linenos">459</span>        <span class="c1"># -----------------------</span>
<span class="linenos">460</span>        <span class="n">src_mask</span><span class="p">,</span> <span class="n">tgt_mask</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">generate_mask</span><span class="p">(</span><span class="n">src</span><span class="p">,</span> <span class="n">tgt</span><span class="p">)</span>
<span class="linenos">461</span>
<span class="linenos">462</span>        <span class="c1"># -----------------------</span>
<span class="linenos">463</span>        <span class="c1"># 2) Encoder</span>
<span class="linenos">464</span>        <span class="c1"># -----------------------</span>
<span class="linenos">465</span>
<span class="linenos">466</span>        <span class="c1"># Embed and add positional encoding (early fusion of position and sequence embeddings)</span>
<span class="linenos">467</span>        <span class="n">enc_input</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">positional_encoding</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">encoder_embedding</span><span class="p">(</span><span class="n">src</span><span class="p">)))</span>
<span class="linenos">468</span>
<span class="linenos">469</span>        <span class="c1"># Pass through several encoder layers</span>
<span class="linenos">470</span>        <span class="n">enc_output</span> <span class="o">=</span> <span class="n">enc_input</span>
<span class="linenos">471</span>        <span class="k">for</span> <span class="n">enc_layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder_layers</span><span class="p">:</span>
<span class="linenos">472</span>            <span class="n">enc_output</span> <span class="o">=</span> <span class="n">enc_layer</span><span class="p">(</span><span class="n">enc_output</span><span class="p">,</span> <span class="n">src_mask</span><span class="p">)</span>
<span class="linenos">473</span>
<span class="linenos">474</span>        <span class="c1"># -----------------------</span>
<span class="linenos">475</span>        <span class="c1"># 3) Decoder</span>
<span class="linenos">476</span>        <span class="c1"># -----------------------</span>
<span class="linenos">477</span>
<span class="linenos">478</span>        <span class="c1"># Embed and add positional encoding (early fusion of position and sequence embeddings)</span>
<span class="linenos">479</span>        <span class="n">dec_input</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">positional_encoding</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">decoder_embedding</span><span class="p">(</span><span class="n">tgt</span><span class="p">)))</span>
<span class="linenos">480</span>        <span class="n">dec_output</span> <span class="o">=</span> <span class="n">dec_input</span>
<span class="linenos">481</span>
<span class="linenos">482</span>        <span class="c1"># Pass through several decoder layers</span>
<span class="linenos">483</span>        <span class="k">for</span> <span class="n">dec_layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder_layers</span><span class="p">:</span>
<span class="linenos">484</span>            <span class="n">dec_output</span> <span class="o">=</span> <span class="n">dec_layer</span><span class="p">(</span><span class="n">dec_output</span><span class="p">,</span> <span class="n">enc_output</span><span class="p">,</span> <span class="n">src_mask</span><span class="p">,</span> <span class="n">tgt_mask</span><span class="p">)</span>
<span class="linenos">485</span>
<span class="linenos">486</span>        <span class="c1"># -----------------------</span>
<span class="linenos">487</span>        <span class="c1"># 4) Final Output Projection</span>
<span class="linenos">488</span>        <span class="c1"># -----------------------</span>
<span class="linenos">489</span>        <span class="c1"># Project decoder output to vocab dimension</span>
<span class="linenos">490</span>        <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc</span><span class="p">(</span><span class="n">dec_output</span><span class="p">)</span>
<span class="linenos">491</span>        <span class="k">return</span> <span class="n">logits</span>
<span class="linenos">492</span>
<span class="linenos">493</span>
<span class="linenos">494</span><span class="c1"># Example usage:</span>
<span class="linenos">495</span><span class="k">def</span> <span class="nf">create_transformer</span><span class="p">(</span><span class="n">src_vocab_size</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">tgt_vocab_size</span><span class="o">=</span><span class="mi">1000</span><span class="p">):</span>
<span class="linenos">496</span>    <span class="n">model</span> <span class="o">=</span> <span class="n">Transformer</span><span class="p">(</span>
<span class="linenos">497</span>        <span class="n">src_vocab_size</span><span class="o">=</span><span class="n">src_vocab_size</span><span class="p">,</span>
<span class="linenos">498</span>        <span class="n">tgt_vocab_size</span><span class="o">=</span><span class="n">tgt_vocab_size</span><span class="p">,</span>
<span class="linenos">499</span>        <span class="n">d_model</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span>
<span class="linenos">500</span>        <span class="n">num_heads</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
<span class="linenos">501</span>        <span class="n">num_layers</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span>
<span class="linenos">502</span>        <span class="n">d_ff</span><span class="o">=</span><span class="mi">2048</span><span class="p">,</span>
<span class="linenos">503</span>        <span class="n">dropout</span><span class="o">=</span><span class="mf">0.1</span>
<span class="linenos">504</span>    <span class="p">)</span>
<span class="linenos">505</span>    <span class="k">return</span> <span class="n">model</span>
<span class="linenos">506</span>
<span class="linenos">507</span>
<span class="linenos">508</span><span class="c1"># Text preprocessing utilities</span>
<span class="linenos">509</span><span class="c1"># Text preprocessing utilities</span>
<span class="linenos">510</span><span class="k">class</span> <span class="nc">SimpleTokenizer</span><span class="p">:</span>
<span class="linenos">511</span>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="linenos">512</span>        <span class="bp">self</span><span class="o">.</span><span class="n">word2idx</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;&lt;PAD&gt;&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;&lt;START&gt;&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;&lt;END&gt;&#39;</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span> <span class="s1">&#39;&lt;UNK&gt;&#39;</span><span class="p">:</span> <span class="mi">3</span><span class="p">}</span>
<span class="linenos">513</span>        <span class="bp">self</span><span class="o">.</span><span class="n">idx2word</span> <span class="o">=</span> <span class="p">{</span><span class="n">v</span><span class="p">:</span> <span class="n">k</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">word2idx</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
<span class="linenos">514</span>        <span class="bp">self</span><span class="o">.</span><span class="n">next_idx</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">word2idx</span><span class="p">)</span>
<span class="linenos">515</span>
<span class="linenos">516</span>    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">texts</span><span class="p">):</span>
<span class="linenos">517</span>        <span class="k">for</span> <span class="n">text</span> <span class="ow">in</span> <span class="n">texts</span><span class="p">:</span>
<span class="linenos">518</span>            <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">text</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span><span class="o">.</span><span class="n">split</span><span class="p">():</span>
<span class="linenos">519</span>                <span class="k">if</span> <span class="n">word</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">word2idx</span><span class="p">:</span>
<span class="linenos">520</span>                    <span class="bp">self</span><span class="o">.</span><span class="n">word2idx</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">next_idx</span>
<span class="linenos">521</span>                    <span class="bp">self</span><span class="o">.</span><span class="n">idx2word</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">next_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">word</span>
<span class="linenos">522</span>                    <span class="bp">self</span><span class="o">.</span><span class="n">next_idx</span> <span class="o">+=</span> <span class="mi">1</span>
<span class="linenos">523</span>
<span class="linenos">524</span>    <span class="k">def</span> <span class="nf">encode</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">text</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="linenos">525</span>        <span class="n">tokens</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">word2idx</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">word</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">word2idx</span><span class="p">[</span><span class="s1">&#39;&lt;UNK&gt;&#39;</span><span class="p">])</span>
<span class="linenos">526</span>                  <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">text</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span><span class="o">.</span><span class="n">split</span><span class="p">()]</span>
<span class="linenos">527</span>        <span class="n">tokens</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">word2idx</span><span class="p">[</span><span class="s1">&#39;&lt;START&gt;&#39;</span><span class="p">]]</span> <span class="o">+</span> <span class="n">tokens</span> <span class="o">+</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">word2idx</span><span class="p">[</span><span class="s1">&#39;&lt;END&gt;&#39;</span><span class="p">]]</span>
<span class="linenos">528</span>
<span class="linenos">529</span>        <span class="k">if</span> <span class="n">max_length</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
<span class="linenos">530</span>            <span class="n">tokens</span> <span class="o">=</span> <span class="n">tokens</span><span class="p">[:</span><span class="n">max_length</span><span class="p">]</span>
<span class="linenos">531</span>            <span class="n">tokens</span> <span class="o">=</span> <span class="n">tokens</span> <span class="o">+</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">word2idx</span><span class="p">[</span><span class="s1">&#39;&lt;PAD&gt;&#39;</span><span class="p">]]</span> <span class="o">*</span> <span class="p">(</span><span class="n">max_length</span> <span class="o">-</span> <span class="nb">len</span><span class="p">(</span><span class="n">tokens</span><span class="p">))</span>
<span class="linenos">532</span>
<span class="linenos">533</span>        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span>
<span class="linenos">534</span>
<span class="linenos">535</span>    <span class="k">def</span> <span class="nf">decode</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tokens</span><span class="p">):</span>
<span class="linenos">536</span>        <span class="k">return</span> <span class="s1">&#39; &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">idx2word</span><span class="p">[</span><span class="n">token</span><span class="o">.</span><span class="n">item</span><span class="p">()]</span>
<span class="linenos">537</span>                         <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">tokens</span>
<span class="linenos">538</span>                         <span class="k">if</span> <span class="n">token</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">word2idx</span><span class="p">[</span><span class="s1">&#39;&lt;PAD&gt;&#39;</span><span class="p">],</span>
<span class="linenos">539</span>                                                 <span class="bp">self</span><span class="o">.</span><span class="n">word2idx</span><span class="p">[</span><span class="s1">&#39;&lt;START&gt;&#39;</span><span class="p">],</span>
<span class="linenos">540</span>                                                 <span class="bp">self</span><span class="o">.</span><span class="n">word2idx</span><span class="p">[</span><span class="s1">&#39;&lt;END&gt;&#39;</span><span class="p">]]])</span>
<span class="linenos">541</span>
<span class="linenos">542</span>
<span class="linenos">543</span><span class="c1"># Example data and training setup</span>
<span class="linenos">544</span><span class="k">def</span> <span class="nf">create_dummy_summarization_data</span><span class="p">(</span><span class="n">num_examples</span><span class="o">=</span><span class="mi">10</span><span class="p">):</span>
<span class="linenos">545</span>    <span class="n">articles</span> <span class="o">=</span> <span class="p">[</span>
<span class="linenos">546</span>                   <span class="s2">&quot;the cat sat on the mat and watched the birds fly by in the clear blue sky&quot;</span><span class="p">,</span>
<span class="linenos">547</span>                   <span class="s2">&quot;scientists discover new species of butterfly in the amazon rainforest last summer&quot;</span><span class="p">,</span>
<span class="linenos">548</span>                   <span class="s2">&quot;local restaurant wins award for best pizza in the city fifth year in row&quot;</span><span class="p">,</span>
<span class="linenos">549</span>                   <span class="s2">&quot;students develop innovative app to help reduce food waste in school cafeterias&quot;</span><span class="p">,</span>
<span class="linenos">550</span>                   <span class="s2">&quot;new study shows benefits of regular exercise on mental health and productivity&quot;</span><span class="p">,</span>
<span class="linenos">551</span>                   <span class="s2">&quot;artist creates stunning mural celebrating community diversity and unity&quot;</span><span class="p">,</span>
<span class="linenos">552</span>                   <span class="s2">&quot;volunteer group organizes successful beach cleanup removing plastic waste&quot;</span><span class="p">,</span>
<span class="linenos">553</span>                   <span class="s2">&quot;tech company launches eco friendly laptop made from recycled materials&quot;</span><span class="p">,</span>
<span class="linenos">554</span>                   <span class="s2">&quot;urban garden project transforms abandoned lot into community vegetable garden&quot;</span><span class="p">,</span>
<span class="linenos">555</span>                   <span class="s2">&quot;music festival brings together local talents raising funds for education&quot;</span>
<span class="linenos">556</span>               <span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="n">num_examples</span> <span class="o">//</span> <span class="mi">10</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
<span class="linenos">557</span>
<span class="linenos">558</span>    <span class="n">summaries</span> <span class="o">=</span> <span class="p">[</span>
<span class="linenos">559</span>                    <span class="s2">&quot;cat watches birds from mat&quot;</span><span class="p">,</span>
<span class="linenos">560</span>                    <span class="s2">&quot;new butterfly species found in amazon&quot;</span><span class="p">,</span>
<span class="linenos">561</span>                    <span class="s2">&quot;restaurant wins best pizza award again&quot;</span><span class="p">,</span>
<span class="linenos">562</span>                    <span class="s2">&quot;students create food waste reduction app&quot;</span><span class="p">,</span>
<span class="linenos">563</span>                    <span class="s2">&quot;exercise improves mental health study finds&quot;</span><span class="p">,</span>
<span class="linenos">564</span>                    <span class="s2">&quot;artist paints community unity mural&quot;</span><span class="p">,</span>
<span class="linenos">565</span>                    <span class="s2">&quot;volunteers clean beach of plastic&quot;</span><span class="p">,</span>
<span class="linenos">566</span>                    <span class="s2">&quot;eco friendly laptop launches&quot;</span><span class="p">,</span>
<span class="linenos">567</span>                    <span class="s2">&quot;community garden replaces empty lot&quot;</span><span class="p">,</span>
<span class="linenos">568</span>                    <span class="s2">&quot;local music festival supports education&quot;</span>
<span class="linenos">569</span>                <span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="n">num_examples</span> <span class="o">//</span> <span class="mi">10</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
<span class="linenos">570</span>
<span class="linenos">571</span>    <span class="k">return</span> <span class="n">articles</span><span class="p">[:</span><span class="n">num_examples</span><span class="p">],</span> <span class="n">summaries</span><span class="p">[:</span><span class="n">num_examples</span><span class="p">]</span>
<span class="linenos">572</span>
<span class="linenos">573</span>
<span class="linenos">574</span><span class="k">def</span> <span class="nf">train_epoch</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span> <span class="n">src_data</span><span class="p">,</span> <span class="n">tgt_data</span><span class="p">,</span> <span class="n">src_tokenizer</span><span class="p">,</span> <span class="n">tgt_tokenizer</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">50</span><span class="p">):</span>
<span class="linenos">575</span>    <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
<span class="linenos">576</span>    <span class="n">total_loss</span> <span class="o">=</span> <span class="mi">0</span>
<span class="linenos">577</span>
<span class="linenos">578</span>    <span class="k">for</span> <span class="n">src_text</span><span class="p">,</span> <span class="n">tgt_text</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">src_data</span><span class="p">,</span> <span class="n">tgt_data</span><span class="p">):</span>
<span class="linenos">579</span>        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
<span class="linenos">580</span>
<span class="linenos">581</span>        <span class="c1"># Prepare input data</span>
<span class="linenos">582</span>        <span class="n">src</span> <span class="o">=</span> <span class="n">src_tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">src_text</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="n">max_length</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="linenos">583</span>        <span class="n">tgt</span> <span class="o">=</span> <span class="n">tgt_tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">tgt_text</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="n">max_length</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="linenos">584</span>
<span class="linenos">585</span>        <span class="c1"># Create target for loss calculation (shifted right)</span>
<span class="linenos">586</span>        <span class="n">tgt_input</span> <span class="o">=</span> <span class="n">tgt</span><span class="p">[:,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
<span class="linenos">587</span>        <span class="n">tgt_output</span> <span class="o">=</span> <span class="n">tgt</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:]</span>
<span class="linenos">588</span>
<span class="linenos">589</span>        <span class="c1"># Forward pass</span>
<span class="linenos">590</span>        <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">src</span><span class="p">,</span> <span class="n">tgt_input</span><span class="p">)</span>
<span class="linenos">591</span>
<span class="linenos">592</span>        <span class="c1"># Calculate loss</span>
<span class="linenos">593</span>        <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">output</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)),</span> <span class="n">tgt_output</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>
<span class="linenos">594</span>
<span class="linenos">595</span>        <span class="c1"># Backward pass</span>
<span class="linenos">596</span>        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="linenos">597</span>        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
<span class="linenos">598</span>
<span class="linenos">599</span>        <span class="n">total_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
<span class="linenos">600</span>
<span class="linenos">601</span>    <span class="k">return</span> <span class="n">total_loss</span>
<span class="linenos">602</span>
<span class="linenos">603</span>
<span class="linenos">604</span><span class="k">def</span> <span class="nf">generate_summary</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">src_text</span><span class="p">,</span> <span class="n">src_tokenizer</span><span class="p">,</span> <span class="n">tgt_tokenizer</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">50</span><span class="p">):</span>
<span class="linenos">605</span>    <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
<span class="linenos">606</span>
<span class="linenos">607</span>    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
<span class="linenos">608</span>        <span class="c1"># Encode input text</span>
<span class="linenos">609</span>        <span class="n">src</span> <span class="o">=</span> <span class="n">src_tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">src_text</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="n">max_length</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="linenos">610</span>
<span class="linenos">611</span>        <span class="c1"># Initialize target with START token</span>
<span class="linenos">612</span>        <span class="n">tgt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="n">tgt_tokenizer</span><span class="o">.</span><span class="n">word2idx</span><span class="p">[</span><span class="s1">&#39;&lt;START&gt;&#39;</span><span class="p">]]])</span>
<span class="linenos">613</span>
<span class="linenos">614</span>        <span class="c1"># Generate summary token by token</span>
<span class="linenos">615</span>        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_length</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):</span>
<span class="linenos">616</span>            <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">src</span><span class="p">,</span> <span class="n">tgt</span><span class="p">)</span>
<span class="linenos">617</span>            <span class="n">next_token</span> <span class="o">=</span> <span class="n">output</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="linenos">618</span>            <span class="n">tgt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">tgt</span><span class="p">,</span> <span class="n">next_token</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="linenos">619</span>
<span class="linenos">620</span>            <span class="k">if</span> <span class="n">next_token</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">==</span> <span class="n">tgt_tokenizer</span><span class="o">.</span><span class="n">word2idx</span><span class="p">[</span><span class="s1">&#39;&lt;END&gt;&#39;</span><span class="p">]:</span>
<span class="linenos">621</span>                <span class="k">break</span>
<span class="linenos">622</span>
<span class="linenos">623</span>    <span class="k">return</span> <span class="n">tgt_tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">tgt</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="linenos">624</span>
<span class="linenos">625</span>
<span class="linenos">626</span><span class="c1"># Training example</span>
<span class="linenos">627</span><span class="k">def</span> <span class="nf">train_summarization_model</span><span class="p">():</span>
<span class="linenos">628</span>    <span class="c1"># Create dummy data</span>
<span class="linenos">629</span>    <span class="n">articles</span><span class="p">,</span> <span class="n">summaries</span> <span class="o">=</span> <span class="n">create_dummy_summarization_data</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
<span class="linenos">630</span>
<span class="linenos">631</span>    <span class="c1"># Initialize tokenizers</span>
<span class="linenos">632</span>    <span class="n">src_tokenizer</span> <span class="o">=</span> <span class="n">SimpleTokenizer</span><span class="p">()</span>
<span class="linenos">633</span>    <span class="n">tgt_tokenizer</span> <span class="o">=</span> <span class="n">SimpleTokenizer</span><span class="p">()</span>
<span class="linenos">634</span>
<span class="linenos">635</span>    <span class="c1"># Fit tokenizers</span>
<span class="linenos">636</span>    <span class="n">src_tokenizer</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">articles</span><span class="p">)</span>
<span class="linenos">637</span>    <span class="n">tgt_tokenizer</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">summaries</span><span class="p">)</span>
<span class="linenos">638</span>
<span class="linenos">639</span>    <span class="c1"># Create model</span>
<span class="linenos">640</span>    <span class="n">model</span> <span class="o">=</span> <span class="n">create_transformer</span><span class="p">(</span>
<span class="linenos">641</span>        <span class="n">src_vocab_size</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">src_tokenizer</span><span class="o">.</span><span class="n">word2idx</span><span class="p">),</span>
<span class="linenos">642</span>        <span class="n">tgt_vocab_size</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">tgt_tokenizer</span><span class="o">.</span><span class="n">word2idx</span><span class="p">)</span>
<span class="linenos">643</span>    <span class="p">)</span>
<span class="linenos">644</span>
<span class="linenos">645</span>    <span class="c1"># Training setup</span>
<span class="linenos">646</span>    <span class="c1"># `nn.CrossEntropyLoss` expects logits — the raw, unnormalized scores output by the network—rather than probabilities.</span>
<span class="linenos">647</span>    <span class="c1">#  Internally, `nn.CrossEntropyLoss` applies a logarithm and a softmax</span>
<span class="linenos">648</span>    <span class="c1">#  (i.e., it uses the log-softmax formulation) before computing the loss.</span>
<span class="linenos">649</span>    <span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">(</span><span class="n">ignore_index</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># ignore padding index</span>
<span class="linenos">650</span>    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.0001</span><span class="p">)</span>
<span class="linenos">651</span>
<span class="linenos">652</span>    <span class="c1"># Training loop</span>
<span class="linenos">653</span>    <span class="n">num_epochs</span> <span class="o">=</span> <span class="mi">50</span>
<span class="linenos">654</span>    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>
<span class="linenos">655</span>        <span class="n">loss</span> <span class="o">=</span> <span class="n">train_epoch</span><span class="p">(</span>
<span class="linenos">656</span>            <span class="n">model</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span>
<span class="linenos">657</span>            <span class="n">articles</span><span class="p">,</span> <span class="n">summaries</span><span class="p">,</span>
<span class="linenos">658</span>            <span class="n">src_tokenizer</span><span class="p">,</span> <span class="n">tgt_tokenizer</span>
<span class="linenos">659</span>        <span class="p">)</span>
<span class="linenos">660</span>
<span class="linenos">661</span>        <span class="k">if</span> <span class="p">(</span><span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="mi">10</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
<span class="linenos">662</span>            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Epoch </span><span class="si">{</span><span class="n">epoch</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">num_epochs</span><span class="si">}</span><span class="s2">, Loss: </span><span class="si">{</span><span class="n">loss</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="linenos">663</span>
<span class="linenos">664</span>    <span class="k">return</span> <span class="n">model</span><span class="p">,</span> <span class="n">src_tokenizer</span><span class="p">,</span> <span class="n">tgt_tokenizer</span>
<span class="linenos">665</span>
<span class="linenos">666</span>
<span class="linenos">667</span><span class="c1"># Example usage</span>
<span class="linenos">668</span><span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
<span class="linenos">669</span>    <span class="c1"># Train model</span>
<span class="linenos">670</span>    <span class="n">model</span><span class="p">,</span> <span class="n">src_tokenizer</span><span class="p">,</span> <span class="n">tgt_tokenizer</span> <span class="o">=</span> <span class="n">train_summarization_model</span><span class="p">()</span>
<span class="linenos">671</span>
<span class="linenos">672</span>    <span class="c1"># Test on training examples</span>
<span class="linenos">673</span>    <span class="n">test_articles</span> <span class="o">=</span> <span class="p">[</span>
<span class="linenos">674</span>        <span class="s2">&quot;the cat sat on the mat and watched the birds fly by in the clear blue sky&quot;</span><span class="p">,</span>
<span class="linenos">675</span>        <span class="s2">&quot;scientists discover new species of butterfly in the amazon rainforest last summer&quot;</span>
<span class="linenos">676</span>    <span class="p">]</span>
<span class="linenos">677</span>
<span class="linenos">678</span>    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Generated Summaries:&quot;</span><span class="p">)</span>
<span class="linenos">679</span>    <span class="k">for</span> <span class="n">article</span> <span class="ow">in</span> <span class="n">test_articles</span><span class="p">:</span>
<span class="linenos">680</span>        <span class="n">summary</span> <span class="o">=</span> <span class="n">generate_summary</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">article</span><span class="p">,</span> <span class="n">src_tokenizer</span><span class="p">,</span> <span class="n">tgt_tokenizer</span><span class="p">)</span>
<span class="linenos">681</span>        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Article: </span><span class="si">{</span><span class="n">article</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="linenos">682</span>        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Summary: </span><span class="si">{</span><span class="n">summary</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<section id="multi-head-attention">
<h3>Multi-Head Attention<a class="headerlink" href="#multi-head-attention" title="Link to this heading"></a></h3>
<p>At the core of Transformer architecture is the <span class="target" id="newconcept-multi-head_attention"></span><span class="newconcept">Multi-Head Attention</span> mechanism, which allows the model to jointly attend to information from different representation subspaces.</p>
<ul>
<li><p><strong>Motivation</strong>: Multi-head attention is motivated by the idea that a value’s embedding might have multiple “components” - part of the embedding might be about topic A, another part about topic B, and for different components the weighting might be different.</p></li>
<li><p><strong>Implementation</strong>: The multi-head attention divides the model dimension (<code class="docutils literal notranslate"><span class="pre">d_model</span></code>) into multiple heads (<code class="docutils literal notranslate"><span class="pre">num_heads</span></code>), where each head focuses on a different aspect of the representation:</p>
<div class="folding highlight-python notranslate" id="multi-head-attention-init"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">MultiHeadAttention</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>

   <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">):</span>
       <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
       <span class="k">assert</span> <span class="n">d_model</span> <span class="o">%</span> <span class="n">num_heads</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="s2">&quot;d_model must be divisible by num_heads&quot;</span>

       <span class="bp">self</span><span class="o">.</span><span class="n">d_model</span> <span class="o">=</span> <span class="n">d_model</span>
       <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">num_heads</span>
       <span class="bp">self</span><span class="o">.</span><span class="n">d_head</span> <span class="o">=</span> <span class="n">d_model</span> <span class="o">//</span> <span class="n">num_heads</span>

       <span class="c1"># Linear projections for query, key, and value transformations</span>
       <span class="bp">self</span><span class="o">.</span><span class="n">W_q</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
       <span class="bp">self</span><span class="o">.</span><span class="n">W_k</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
       <span class="bp">self</span><span class="o">.</span><span class="n">W_v</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>

       <span class="c1"># Final output projection after concatenating heads</span>
       <span class="bp">self</span><span class="o">.</span><span class="n">W_o</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p><strong>Scaled Dot-Product Attention</strong>: Within each attention head, a <span class="target" id="newconcept-scaled_dot-product_attention"></span><span class="newconcept">Scaled Dot-Product Attention</span> mechanism computes attention weights:</p>
<div class="folding highlight-python notranslate" id="scaled-dot-product-attention"><div class="highlight"><pre><span></span>   <span class="k">def</span> <span class="nf">scaled_dot_product_attention</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">V</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">       </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">       Compute scaled dot-product attention between query Q, key K, and value V.</span>
<span class="sd">       Typically used inside each attention head.</span>

<span class="sd">       Scaled-doc product can be viewed as soft retrieval, where we obtain probabilistic scores for values in V through Q and K.</span>
<span class="sd">       The probabilistic scores are the attention scores.</span>

<span class="sd">       Args:</span>
<span class="sd">           Q (Tensor):</span>
<span class="sd">               Shape (batch_size, num_heads, query_size, d_head).</span>
<span class="sd">           K (Tensor):</span>
<span class="sd">               Shape (batch_size, num_heads, index_size, d_head).</span>
<span class="sd">           V (Tensor):</span>
<span class="sd">               Shape (batch_size, num_heads, index_size, d_head).</span>
<span class="sd">           mask (Tensor, optional):</span>
<span class="sd">               Shape broadcastable to (batch_size, 1, query_size, index_size),</span>
<span class="sd">               typically a padding of size (batch_size, 1, 1, index_size),</span>
<span class="sd">               with zero (False) entries indicating positions to mask out.</span>

<span class="sd">       Returns:</span>
<span class="sd">           output (Tensor):</span>
<span class="sd">               Shape (batch_size, num_heads, query_size, d_head).</span>
<span class="sd">           attn_weights (Tensor):</span>
<span class="sd">               Shape (batch_size, num_heads, query_size, index_size).</span>
<span class="sd">               These are the softmax-normalized attention scores.</span>
<span class="sd">       &quot;&quot;&quot;</span>


       <span class="c1"># -----------------------</span>
       <span class="c1"># 1) Compute raw attention scores</span>
       <span class="c1"># -----------------------</span>
       <span class="c1"># Calculate raw attention scores (before applying softmax).</span>
       <span class="c1"># We use negative indexes to indicate transposing the last two dimension of K. We do not use positive indexes so the code can work in a more general way, as K might have 3 or 4 dimensions with &quot;multi-head&quot; and &quot;batch&quot;.</span>
       <span class="c1"># Dividing by the square root of `d_head` is recommended by the paper is to prevent the raw attention scores from becoming too large for large dimensions.</span>
       <span class="c1"># The `attn_scores` dimensions are (`batch_size`, `num_heads`, `query_size`, `index_size`)</span>
       <span class="c1"># (or (`batch_size`, `num_heads`, `seq_length`, `seq_length`) in the sequence processing case).</span>
       <span class="n">attn_scores</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">d_head</span><span class="p">)</span>

       <span class="c1"># -----------------------</span>
       <span class="c1"># 2) Mask out invalid positions (if any)</span>
       <span class="c1"># -----------------------</span>
       <span class="k">if</span> <span class="n">mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
           <span class="c1"># We can mask out keys/values by replacing their attention scores to near-zero values</span>
           <span class="c1"># Typically a padding mask of size (batch_size, 1, 1, index_size),</span>
           <span class="c1">#  but any mask broadcastable to (batch_size, 1, query_size, index_size) should work here.</span>
           <span class="n">attn_scores</span> <span class="o">=</span> <span class="n">attn_scores</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="n">mask</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mf">1e9</span><span class="p">)</span>

       <span class="c1"># -----------------------</span>
       <span class="c1"># 3) Apply softmax to get normalized weights</span>
       <span class="c1"># -----------------------</span>
       <span class="c1"># Apply softmax along the last dimension / feature dimension, i.e. normalizing each row)</span>
       <span class="n">attn_weights</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">attn_scores</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

       <span class="c1"># -----------------------</span>
       <span class="c1"># 4) Multiply weights by values (weighted sum of V)</span>
       <span class="c1"># -----------------------</span>
       <span class="c1"># Apply attention weights to values.</span>
       <span class="c1"># The `output` dimensions are (`batch_size`, `num_heads`, `query_size`, `d_head`)</span>
       <span class="c1"># (or (`batch_size`, `num_heads`, `seq_length`, `d_head`) in the sequence processing case),</span>
       <span class="c1"># which can be viewed as a weighted sum of features (rows) of V, weighted by the `attn_weights`,</span>
       <span class="c1"># or as if it had &quot;retrieved&quot; features from V in a weighted way.</span>
       <span class="n">output</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">attn_weights</span><span class="p">,</span> <span class="n">V</span><span class="p">)</span>
       <span class="k">return</span> <span class="n">output</span><span class="p">,</span> <span class="n">attn_weights</span>
</pre></div>
</div>
<ul class="simple">
<li><p>This can be viewed as a <strong class="underline-bold">soft retrieval operation</strong>, obtaining probabilistic scores for values in V through query-key interactions</p></li>
<li><p>Dividing by <code class="docutils literal notranslate"><span class="pre">sqrt(d_head)</span></code> stabilizes gradients by preventing attention scores from becoming too large for high dimensions</p></li>
</ul>
</li>
<li><p><strong>Forward Pass Implementation</strong>: The forward method demonstrates how multi-head attention is actually computed:</p>
<div class="folding highlight-python notranslate" id="multi-head-attention-forward"><div class="highlight"><pre><span></span>   <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">V</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">       </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">       Forward pass of multi-head attention. Splits Q, K, and V into num_heads</span>
<span class="sd">       chunks, applies scaled dot-product attention for each head, then</span>
<span class="sd">       concatenates the heads back into d_model.</span>

<span class="sd">       Args:</span>
<span class="sd">           Q (Tensor): (batch_size, query_size, d_model)</span>
<span class="sd">           K (Tensor): (batch_size, index_size, d_model)</span>
<span class="sd">           V (Tensor): (batch_size, index_size, d_model)</span>
<span class="sd">           mask (Tensor, optional):</span>
<span class="sd">               Shape broadcastable to (batch_size, 1, query_size, index_size),</span>
<span class="sd">               typically a padding of size (batch_size, 1, 1, index_size),</span>
<span class="sd">               with zero (False) entries indicating positions to mask out.</span>
<span class="sd">       Returns:</span>
<span class="sd">           output (Tensor):</span>
<span class="sd">               (batch_size, query_size, d_model)</span>
<span class="sd">           attn_weights (Tensor):</span>
<span class="sd">               (batch_size, num_heads, query_size, index_size)</span>
<span class="sd">       &quot;&quot;&quot;</span>
       <span class="n">batch_size</span> <span class="o">=</span> <span class="n">Q</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

       <span class="c1"># -----------------------</span>
       <span class="c1"># 1) Linear projections + reshape into heads</span>
       <span class="c1"># -----------------------</span>
       <span class="c1"># 1. Keep the batch size.</span>
       <span class="c1"># 2. Breaking down the last dimension (the feature dimension) to two dimensions of size `num_heads` and `d_head`.</span>
       <span class="c1"># 3. The `-1` means the `view` function will infer the corresponding dimension (the index size dimension).</span>
       <span class="c1"># Transposing the index size dimension (dimension 1) and the `num_heads` dimension (dimension 2) is for computational convenience</span>
       <span class="c1"># because the scaled-doc product will be performed per batch per head.</span>
       <span class="c1"># After transposing, the dimensions are (`batch_size`, `num_heads`, `query_size`, `d_head`) for Q, and (`batch_size`, `num_heads`, `index_size`, `d_head`) for K, V</span>
       <span class="c1"># (or all (`batch_size`, `num_heads`, `seq_length`, `d_head`) in the sequence self-attention case).</span>
       <span class="n">Q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">W_q</span><span class="p">(</span><span class="n">Q</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_head</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
       <span class="n">K</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">W_k</span><span class="p">(</span><span class="n">K</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_head</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
       <span class="n">V</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">W_v</span><span class="p">(</span><span class="n">V</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_head</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

       <span class="c1"># -----------------------</span>
       <span class="c1"># 2) Scaled dot-product attention</span>
       <span class="c1"># -----------------------</span>
       <span class="c1"># The `output` dimensions are (`batch_size`, `num_heads`, `query_size`, `d_head`)</span>
       <span class="c1"># (or (`batch_size`, `num_heads`, `seq_length`, `d_head`) in the sequence processing case),</span>
       <span class="c1"># The `attn_scores` dimensions are (`batch_size`, `num_heads`, `query_size`, `index_size`)</span>
       <span class="c1"># (or (`batch_size`, `num_heads`, `seq_length`, `seq_length`) in the sequence processing case).</span>
       <span class="n">output</span><span class="p">,</span> <span class="n">attn_weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">scaled_dot_product_attention</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">V</span><span class="p">,</span> <span class="n">mask</span><span class="p">)</span>

       <span class="c1"># -----------------------</span>
       <span class="c1"># 3) Recombine heads</span>
       <span class="c1"># -----------------------</span>
       <span class="c1"># Combine the heads and reshape back to original input embedding dimension sizes.</span>
       <span class="c1"># 1. Transposing the dimensions back to (`batch_size`, `query_size`, `num_heads`, `d_head`) (or (`batch_size`, `seq_length`, `num_heads`, `d_head`) in the sequence processing case).</span>
       <span class="c1"># 2. The `contiguous` method enforces the underlying tensor representation be continuous.</span>
       <span class="c1"># 3. The `view` method merges the last two dimensions, so it becomes (`batch_size`, `query_size`, `d_model`) (or (`batch_size`, `seq_length`, `d_model`) in the sequence processing case).</span>
       <span class="n">output</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_model</span><span class="p">)</span>

       <span class="c1"># -----------------------</span>
       <span class="c1"># 4) Final linear projection</span>
       <span class="c1"># -----------------------</span>
       <span class="c1"># Apply output transformation on the `d_model` dimension.</span>
       <span class="c1"># `output` dimensions unchanged.</span>
       <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">W_o</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>

       <span class="c1"># NOTE:</span>
       <span class="c1"># The above approach is mathematically equivalent to perform multiple Q/K/V scaled-dot-products and then concatenate them.</span>
       <span class="c1"># However, above view/transpose operations are more efficient implementation.</span>

       <span class="k">return</span> <span class="n">output</span><span class="p">,</span> <span class="n">attn_weights</span>
</pre></div>
</div>
<ul class="simple">
<li><p>The key operations include:</p>
<ol class="arabic simple">
<li><p>Projecting inputs through linear layers <span class="math notranslate nohighlight">\(Q, K, V\)</span>.</p></li>
<li><p>Splitting the model dimension across multiple heads.</p></li>
<li><p>Applying attention independently to each head.</p></li>
<li><p>Concatenating outputs from all heads.</p></li>
<li><p>Applying the final output projection <span class="math notranslate nohighlight">\(W_o\)</span> to produce the combined attention result.</p></li>
</ol>
</li>
<li><p>The code implementation is mathematically equivalent to perform multiple Q/K/V scaled-dot-products and then concatenate them; however, <strong class="underline-bold">the view/transpose operations are more efficient implementation</strong>.</p></li>
</ul>
</li>
</ul>
</section>
<section id="positional-encoding">
<h3>Positional Encoding<a class="headerlink" href="#positional-encoding" title="Link to this heading"></a></h3>
<p>Since multi-head attention process input tokens simultaneously (in parallel), they lack inherent sequential information. <span class="target" id="newconcept-positional_encoding"></span><span class="newconcept">Positional Encoding</span> addresses this limitation:</p>
<ul>
<li><p><strong>Purpose</strong>: Positional encoding can be viewed as a type of <strong class="underline-bold">multi-modal learning</strong>.</p>
<ul class="simple">
<li><p>It combined position information with the token embeddings, allowing the model to distinguish between different positions in the sequence.</p></li>
<li><p>In the canonical transformer architecture, it is by simply adding the positional encodings to the token embeddings before input to the encoder layers, as well as adding to the encoder output before input to the decoder layers.</p></li>
</ul>
</li>
<li><p><strong>Implementation</strong>: Uses sine and cosine functions of different frequencies to create unique position identifiers:</p>
<div class="folding highlight-python notranslate" id="transformer-positional-encoding"><div class="highlight"><pre><span></span>   <span class="k">class</span> <span class="nc">PositionalEncoding</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">       </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">       Positional encoding injects information about the absolute or relative position</span>
<span class="sd">       of tokens in the sequence, enabling the model to capture ordering context.</span>

<span class="sd">       The sine/cosine pattern allows the model to generalize to positions beyond</span>
<span class="sd">       the training range and provides unique encodings for each position.</span>

<span class="sd">       Implementation details:</span>
<span class="sd">         - Sine values assigned to even indices.</span>
<span class="sd">         - Cosine values assigned to odd indices.</span>
<span class="sd">         - Dimensions with higher indices (features) oscillate faster.</span>
<span class="sd">         - Final tensor is registered as a buffer, so it&#39;s not trainable.</span>
<span class="sd">       &quot;&quot;&quot;</span>

       <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">max_seq_length</span><span class="o">=</span><span class="mi">5000</span><span class="p">):</span>
           <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

           <span class="n">pe</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">max_seq_length</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
           <span class="n">position</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">max_seq_length</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
           <span class="n">div_term</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span> <span class="o">*</span> <span class="p">(</span><span class="o">-</span><span class="n">math</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mf">10000.0</span><span class="p">)</span> <span class="o">/</span> <span class="n">d_model</span><span class="p">))</span>

           <span class="c1"># the following uses start:stop:step notation</span>
           <span class="n">pe</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">position</span> <span class="o">*</span> <span class="n">div_term</span><span class="p">)</span>  <span class="c1"># Assigns sine values to even indices (0,2,4,...)</span>
           <span class="n">pe</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">position</span> <span class="o">*</span> <span class="n">div_term</span><span class="p">)</span>  <span class="c1"># Assigns cosine values to odd indices (1,3,5,...)</span>

           <span class="c1"># The original pe tensor has shape [`max_seq_length`, `d_model`],</span>
           <span class="c1"># and `unsqueeze(0)` adds a new dimension at index 0.</span>
           <span class="c1"># After unsqueeze, shape becomes [1, `max_seq_length`, `d_model`],</span>
           <span class="c1"># and this extra dimension allows for batch processing.</span>

           <span class="c1"># `self.register_buffer` in PyTorch is a method used to register a tensor as a buffer</span>
           <span class="c1"># that should be saved along with model parameters during model.state_dict() calls,</span>
           <span class="c1"># but is not considered a model parameter (meaning it doesn&#39;t receive gradients during backpropagation).</span>
           <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s1">&#39;pe&#39;</span><span class="p">,</span> <span class="n">pe</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>

       <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="w">           </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">           Add positional encoding to input embeddings.</span>

<span class="sd">           Args:</span>
<span class="sd">               x (Tensor):</span>
<span class="sd">                   Shape (batch_size, seq_length, d_model).</span>

<span class="sd">           Returns:</span>
<span class="sd">               (Tensor):</span>
<span class="sd">                   Same shape as input, with positional encodings added elementwise.</span>
<span class="sd">           &quot;&quot;&quot;</span>

           <span class="c1"># If x has shape [`batch_size`, `seq_length`, `d_model`]</span>
           <span class="c1"># pe[:, :x.size(1)] will broadcast from [1, `seq_length`, `d_model`]</span>
           <span class="c1"># to match x&#39;s shape.</span>
           <span class="k">return</span> <span class="n">x</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">pe</span><span class="p">[:,</span> <span class="p">:</span><span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)]</span>
</pre></div>
</div>
</li>
<li><p><strong>Key Properties</strong>:</p>
<ul class="simple">
<li><p>Projects integer positions onto a circle in 2D dimension, using sine/cosine values to identify positions</p></li>
<li><p>Includes dimension index in the formula to:
1. Match the model feature dimension size (<code class="docutils literal notranslate"><span class="pre">d_model</span></code>)
2. Help each positional encoding uniquely identify a position</p></li>
</ul>
</li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The positional encoding has deep connection with <span class="target" id="newconcept-fourier_transform"></span><span class="newconcept">Fourier Transform</span> in mathematics &amp; signal processing (Fourier transform decomposes a signal into different frequency components). Given the formula of positional encoding $text{position} cdot text{div_term}$ where</p>
<div class="math notranslate nohighlight">
\[\text{div_term} = \text{exp}(\text{arange}(0, \text{d_model}, 2) \cdot (-\text{log}(10000.0) / \text{d_model}))\]</div>
<p>We can see it is using sine and cosine functions with different frequencies for each dimension, as illustrated in the figure below.</p>
<figure class="align-center" id="id2">
<a class="reference internal image-reference" href="../../_images/transformer_positional_encoding.png"><img alt="Transformer Positional Encoding Interpretation" src="../../_images/transformer_positional_encoding.png" style="width: 80%;" />
</a>
<figcaption>
<p><span class="caption-number">Figure 6 </span><span class="caption-text">Visualization of positional encoding frequencies</span><a class="headerlink" href="#id2" title="Link to this image"></a></p>
</figcaption>
</figure>
<p>Lower dimensions use lower frequencies (changing slowly across positions, so it is more distinguishing long-range positional difference), while higher dimensions use higher frequencies (changing rapidly across positions, more distinguishing short-range local positional difference). This multi-scale approach gives the model the <strong class="underline-bold">ability to reason about positional relationships at different levels of granularity</strong>.</p>
</div>
</section>
<section id="positionwise-feed-forward-networks">
<h3>Positionwise Feed-Forward Networks<a class="headerlink" href="#positionwise-feed-forward-networks" title="Link to this heading"></a></h3>
<p>Between attention layers, the Transformer employs <span class="target" id="newconcept-positionwise_feed-forward_networks"></span><span class="newconcept">Positionwise Feed-Forward Networks</span>, a fully connected feedforward neural network applied independently and identically to each data point in the sequence. This means that the same feedforward network is applied to each data point’s representation without any interaction between different positions at this stage.</p>
<ul>
<li><p><strong>Implementation</strong>: Two linear transformations with a ReLU activation in between. The hidden layer size <code class="docutils literal notranslate"><span class="pre">d_ff</span></code> is usually 2x to 4x larger than <code class="docutils literal notranslate"><span class="pre">d_model</span></code>. Both non-linear activation and larger hidden layer size enhance the model’s representational capacity:</p>
<div class="folding highlight-python notranslate" id="transformer-positionwise-feedforward"><div class="highlight"><pre><span></span>   <span class="k">class</span> <span class="nc">PositionWiseFeedForward</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">       </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">       A feedforward network applied independently at each time step (position).</span>
<span class="sd">       In other words, for each position in the sequence, the same two-layer MLP is used:</span>
<span class="sd">         1) Linear transform: d_model -&gt; d_ff</span>
<span class="sd">         2) Non-linear activation (ReLU)</span>
<span class="sd">         3) Linear transform: d_ff -&gt; d_model</span>

<span class="sd">       d_ff is larger than d_model. For example, the classic setting is 4x larger.</span>
<span class="sd">       This larger hidden dimension in the feed-forward sub-layer helps increase the representational capacity of the network,</span>
<span class="sd">       allowing more complex transformations to be applied at each position.</span>
<span class="sd">       After this larger intermediate projection, it then projects back down to d_model.</span>

<span class="sd">       Because it is applied &quot;position-wise,&quot; there&#39;s no interaction across different sequence positions here.</span>
<span class="sd">       &quot;&quot;&quot;</span>

       <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">):</span>
           <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
           <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">)</span>
           <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_ff</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
           <span class="bp">self</span><span class="o">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>

       <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="w">           </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">           Forward pass through the position-wise feed-forward network.</span>

<span class="sd">           Args:</span>
<span class="sd">               x (Tensor):</span>
<span class="sd">                   Input tensor of shape (batch_size, seq_length, d_model).</span>

<span class="sd">           Returns:</span>
<span class="sd">               (Tensor):</span>
<span class="sd">                   Output tensor of shape (batch_size, seq_length, d_model).</span>
<span class="sd">           &quot;&quot;&quot;</span>
           <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
</pre></div>
</div>
</li>
</ul>
</section>
<section id="layer-normalization">
<h3>Layer Normalization<a class="headerlink" href="#layer-normalization" title="Link to this heading"></a></h3>
<p><span class="target" id="newconcept-layer_normalization"></span><span class="newconcept">Layer Normalization</span> is applied after each sub-layer within both encoder and decoder stacks to stabilize the learning process (see <a class="reference external" href="https://en.wikipedia.org/wiki/Numerical_stability">Numerical Stability</a>):</p>
<ul>
<li><p>Layer Normalization <strong class="underline-bold">applies z-score normalization</strong> across the feature dimension for each sample in the batch (i.e., applied across the feature dimension). For an input vector <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> with features <span class="math notranslate nohighlight">\(x_1, x_2, ..., x_d\)</span>, the formula is:</p>
<div class="math notranslate nohighlight">
\[\text{LayerNorm}(\mathbf{x}) = \gamma \cdot \frac{\mathbf{x} - \mu}{\sqrt{\sigma^2 + \epsilon}} + \beta\]</div>
<p>Where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\mu = \frac{1}{d} \sum_{i=1}^{d} x_i\)</span> is the mean across features</p></li>
<li><p><span class="math notranslate nohighlight">\(\sigma^2 = \frac{1}{d} \sum_{i=1}^{d} (x_i - \mu)^2\)</span> is the variance across features</p></li>
<li><p><span class="math notranslate nohighlight">\(\epsilon\)</span> is a small constant added for numerical stability (preventing division by zero)</p></li>
<li><p><span class="math notranslate nohighlight">\(\gamma\)</span> and <span class="math notranslate nohighlight">\(\beta\)</span> are learnable parameters for scaling and shifting</p></li>
</ul>
</li>
<li><p><strong>Implementation</strong>: In PyTorch, this is implemented as <code class="docutils literal notranslate"><span class="pre">nn.LayerNorm</span></code> under <code class="docutils literal notranslate"><span class="pre">torch.nn</span></code>.</p></li>
<li><p><strong>Purpose</strong>:</p>
<ul class="simple">
<li><p><strong class="underline-bold">Preventing covariate shift</strong> and thus stabilizing the distribution of activations.</p></li>
<li><p><strong class="underline-bold">Enables higher learning rates</strong> and faster convergence.</p></li>
<li><p><strong class="underline-bold">Reduces sensitivity to initialization</strong> and helps gradient flow through deep networks</p></li>
<li><p>Unlike batch normalization, works effectively with <strong class="underline-bold">variable-length sequences and small batch sizes</strong></p></li>
</ul>
</li>
<li><p><strong class="underline-bold">Applied with random dropout and residual connection</strong>: In Transformer implementations, Layer Normalization is applied together with <span class="target" id="newconcept-residual_connections"></span><span class="newconcept">residual connections</span> using the formula <code class="docutils literal notranslate"><span class="pre">LayerNorm(x</span> <span class="pre">+</span> <span class="pre">Dropout(Sublayer(x)))</span></code>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># In the encoder layer</span>
<span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="c1"># Self-attention with residual connection and layer normalization</span>
    <span class="n">attn_output</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">self_attn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">mask</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm1</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">attn_output</span><span class="p">))</span>  <span class="c1"># Add &amp; Norm</span>

    <span class="c1"># Feed-forward with residual connection and layer normalization</span>
    <span class="n">ff_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">feed_forward</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm2</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">ff_output</span><span class="p">))</span>  <span class="c1"># Add &amp; Norm</span>

    <span class="k">return</span> <span class="n">x</span>
</pre></div>
</div>
<p>This “Add &amp; Norm” pattern (residual connection followed by layer normalization) is empirically found effective for training deep Transformer networks.</p>
<ul class="simple">
<li><p>Improved gradient flow through direct pathways</p></li>
<li><p>Easier optimization of residual mappings compared to direct mappings</p></li>
<li><p>Ability to train much deeper networks without performance degradation</p></li>
<li><p>Ensemble-like behavior that improves generalization</p></li>
</ul>
</li>
</ul>
</section>
<section id="encoder-decoder-architecture">
<h3>Encoder-Decoder Architecture<a class="headerlink" href="#encoder-decoder-architecture" title="Link to this heading"></a></h3>
<p>The Transformer follows an <span class="target" id="newconcept-encoder-decoder_architecture"></span><span class="newconcept">Encoder-Decoder Architecture</span>, with the encoder processing the input sequence and the decoder generating the output sequence.</p>
<ul id="code-transformer-encoder">
<li><p><strong>Encoder Layer</strong>: Each encoder layer consists of:</p>
<ol class="arabic simple">
<li><p>Multi-head <strong class="underline-bold">self-attention</strong> mechanism to mix the encoder input embeddings.</p></li>
<li><p><strong class="underline-bold">Position-wise feed-forward</strong> network.</p></li>
<li><p><strong class="underline-bold">Layer norm with dropout and residuals</strong> twice after each of above self-attention and feed-forward layers.</p></li>
</ol>
<p>Also note that <strong class="underline-bold">positional encoding is not applied inside encoder/decoder layers</strong>. They have been added to the encoder/decoder input token embeddings (early fusion).</p>
<blockquote>
<div><div class="folding highlight-python notranslate" id="transformer-encoder-layer"><div class="highlight"><pre><span></span>  <span class="k">class</span> <span class="nc">EncoderLayer</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">      </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">      The Encoder layer is composed of:</span>
<span class="sd">      1) Multi-head self-attention over the input sequence.</span>
<span class="sd">      2) Position-wise feed-forward network.</span>
<span class="sd">      3) Each sub-layer is followed by a residual connection and LayerNorm.</span>

<span class="sd">      The output size is the same as the input embeddings shape: (batch_size, src_seq_len, d_model).</span>
<span class="sd">      &quot;&quot;&quot;</span>
      <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
          <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">self_attn</span> <span class="o">=</span> <span class="n">MultiHeadAttention</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">feed_forward</span> <span class="o">=</span> <span class="n">PositionWiseFeedForward</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">)</span>

          <span class="c1"># LayerNorm is normalizing the feature dimension using z-score (with perturbed variance)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">norm1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">norm2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>

          <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">)</span>

      <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">          </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">          Args:</span>
<span class="sd">              x (Tensor):</span>
<span class="sd">                  The input embeddings/hidden states of shape (batch_size, src_seq_len, d_model).</span>
<span class="sd">              mask (Tensor, optional):</span>
<span class="sd">                  A padding mask for the source sequence.</span>
<span class="sd">                  Typically shaped (batch_size, 1, 1, src_seq_len).</span>
<span class="sd">                  Defaults to None if no mask is used.</span>

<span class="sd">          Returns:</span>
<span class="sd">              x (Tensor):</span>
<span class="sd">                  The output of this encoder layer, shape (batch_size, src_seq_len, d_model).</span>
<span class="sd">          &quot;&quot;&quot;</span>

          <span class="c1"># -----------------------</span>
          <span class="c1"># 1) Self-Attention</span>
          <span class="c1"># -----------------------</span>
          <span class="c1"># Each position in &#39;x&#39; attends to other positions in the same sequence.</span>
          <span class="c1"># The shapes inside attention temporarily become (batch_size, num_heads, src_seq_len, d_head).</span>
          <span class="n">attn_output</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">self_attn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">mask</span><span class="p">)</span>

          <span class="c1"># Residual connection + dropout + LayerNorm</span>
          <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm1</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">attn_output</span><span class="p">))</span>
          <span class="c1"># x remains (batch_size, src_seq_len, d_model)</span>

          <span class="c1"># -----------------------</span>
          <span class="c1"># 2) Position-Wise Feed-Forward</span>
          <span class="c1"># -----------------------</span>
          <span class="c1"># Transform each position from d_model -&gt; d_ff -&gt; d_model independently.</span>
          <span class="n">ff_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">feed_forward</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

          <span class="c1"># Residual connection + dropout + LayerNorm</span>
          <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm2</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">ff_output</span><span class="p">))</span>
          <span class="c1"># x remains (batch_size, src_seq_len, d_model)</span>

          <span class="k">return</span> <span class="n">x</span>
</pre></div>
</div>
</div></blockquote>
</li>
<li><p><strong>Decoder Layer</strong>: Each decoder layer has:</p>
<ol class="arabic simple">
<li><p>Causal-masked multi-head <strong class="underline-bold">self-attention</strong> on the decoder inputs</p>
<ul class="simple">
<li><p>Decoder inputs are usually the training targets.</p></li>
<li><p>The mask is a combined padding mask and causal mask to prevent attending to future positions.</p></li>
<li><p>This causal style inference needs a “seed” at the beginning the decoder targets, therefore a <span class="target" id="newconcept-start-of-sequence_special_token"></span><span class="newconcept">start-of-sequence special token</span> is needed at the beginning of decoder input sequence.</p></li>
<li><p>During inference, the decoder needs to identify when to stop, therefore a <span class="target" id="newconcept-end-of-sequence_special_token"></span><span class="newconcept">end-of-sequence special token</span> is also needed at the end of decoder input sequence.</p></li>
</ul>
</li>
<li><p>Multi-head <strong class="underline-bold">cross-attention</strong> over encoder outputs</p></li>
<li><p><strong class="underline-bold">Position-wise feed-forward</strong> network.</p></li>
<li><p><strong class="underline-bold">Layer norm with dropout and residuals</strong> three times after each of above layers.</p></li>
</ol>
<p>Decoder input embeddings usually represent the learning targets. The <span class="target" id="newconcept-decoder_vocabulary"></span><span class="newconcept">decoder vocabulary</span> consists of all possible decoder learning targets. Under transformer, <strong class="underline-bold">each decoder input embedding functions like a query to retrieve a mix from encoder output embeddings</strong> to represent learning target.</p>
<blockquote id="code-transformer-decoder">
<div><div class="folding highlight-python notranslate" id="transformer-decoder-layer"><div class="highlight"><pre><span></span>  <span class="k">class</span> <span class="nc">DecoderLayer</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">      </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">      The Decoder layer contains:</span>
<span class="sd">      1. Masked self-attention over the (partial) target sequence.</span>
<span class="sd">      2. Cross-attention over the encoder&#39;s output.</span>
<span class="sd">      3. Position-wise feed-forward network.</span>
<span class="sd">      4. Each above sub-layer is followed by a residual connection and LayerNorm.</span>

<span class="sd">      The output size is the same as the decoder input embeddings, both (batch_size, target_seq_len, d_model).</span>
<span class="sd">      &quot;&quot;&quot;</span>
      <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
          <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">self_attn</span> <span class="o">=</span> <span class="n">MultiHeadAttention</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">cross_attn</span> <span class="o">=</span> <span class="n">MultiHeadAttention</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">feed_forward</span> <span class="o">=</span> <span class="n">PositionWiseFeedForward</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">)</span>

          <span class="bp">self</span><span class="o">.</span><span class="n">norm1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">norm2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">norm3</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>

          <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">)</span>

      <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">enc_output</span><span class="p">,</span> <span class="n">src_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">tgt_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">          </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">          Args:</span>
<span class="sd">              x (Tensor): Decoder input embeddings of shape (batch_size, target_seq_len, d_model).</span>
<span class="sd">              enc_output (Tensor): Final encoder outputs of shape (batch_size, source_seq_len, d_model).</span>
<span class="sd">              src_mask (Tensor or None): Padding mask for encoder outputs, shape (batch_size, 1, 1, source_seq_len).</span>
<span class="sd">              tgt_mask (Tensor or None): Combined padding + causal (future) mask for decoder inputs,</span>
<span class="sd">                                         shape (batch_size, 1, target_seq_len, target_seq_len).</span>

<span class="sd">          Returns:</span>
<span class="sd">              x (Tensor): Decoder layer output of shape (batch_size, target_seq_len, d_model).</span>
<span class="sd">          &quot;&quot;&quot;</span>

          <span class="c1"># -----------------------</span>
          <span class="c1"># 1) Masked Self-Attention</span>
          <span class="c1"># -----------------------</span>
          <span class="c1"># Decoder attends to itself (partial target sequence).</span>
          <span class="c1"># Causal mask ensures a token can’t attend to future positions.</span>
          <span class="n">self_attn_output</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">self_attn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">tgt_mask</span><span class="p">)</span>
          <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm1</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">self_attn_output</span><span class="p">))</span>

          <span class="c1"># -----------------------</span>
          <span class="c1"># 2) Cross-Attention</span>
          <span class="c1"># -----------------------</span>
          <span class="c1"># Each decoder input embedding retrieves a mixture from encoder output</span>
          <span class="n">cross_attn_output</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cross_attn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">enc_output</span><span class="p">,</span> <span class="n">enc_output</span><span class="p">,</span> <span class="n">src_mask</span><span class="p">)</span>
          <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm2</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">cross_attn_output</span><span class="p">))</span>

          <span class="c1"># -----------------------</span>
          <span class="c1"># 3) Position-Wise Feed-Forward</span>
          <span class="c1"># -----------------------</span>
          <span class="c1"># Applies two linear layers (d_model -&gt; d_ff -&gt; d_model)</span>
          <span class="c1"># at each position independently.</span>
          <span class="n">ff_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">feed_forward</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
          <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm3</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">ff_output</span><span class="p">))</span>

          <span class="k">return</span> <span class="n">x</span>
</pre></div>
</div>
</div></blockquote>
</li>
<li><p><strong>Attention Masking</strong>: Two types of masks are employed:</p>
<ol class="arabic simple">
<li><p><strong>Source Padding Mask</strong>: Handles padding in the input sequence</p></li>
<li><p><strong>Target Mask</strong>: Combines padding mask with a future-masking mechanism to prevent the decoder from accessing future tokens</p></li>
</ol>
<div class="folding highlight-python notranslate" id="transformer-masking"><span id="code-transformer-masking"></span><div class="highlight"><pre><span></span> <span class="k">def</span> <span class="nf">generate_mask</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">src</span><span class="p">,</span> <span class="n">tgt</span><span class="p">):</span>
<span class="w">     </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">     Generate:</span>
<span class="sd">      - src_mask to hide padding tokens in the encoder input.</span>
<span class="sd">      - tgt_mask to hide padding and future positions in the decoder input.</span>

<span class="sd">     Args:</span>
<span class="sd">         src (Tensor): (batch_size, src_seq_len) of token indices.</span>
<span class="sd">         tgt (Tensor): (batch_size, tgt_seq_len) of token indices.</span>

<span class="sd">     Returns:</span>
<span class="sd">         src_mask (Tensor): (batch_size, 1, 1, src_seq_len) with 1/True = keep, 0/False = mask out.</span>
<span class="sd">         tgt_mask (Tensor): (batch_size, 1, tgt_seq_len, tgt_seq_len) combined padding + causal mask.</span>
<span class="sd">     &quot;&quot;&quot;</span>

     <span class="c1"># The `unsqueeze(1).unsqueeze(2)` operations add extra dimensions to the `src` tensor, transforming its shape from [`batch_size`, `seq_length`] to [`batch_size`, 1, 1, `seq_length`]</span>
     <span class="n">src_mask</span> <span class="o">=</span> <span class="p">(</span><span class="n">src</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>

     <span class="c1"># The `unsqueeze(1).unsqueeze(3)` operations add extra dimensions to the `tgt` tensor, transforming its shape from [`batch_size`, `seq_length`] to [`batch_size`, 1, `seq_length`, 1]</span>
     <span class="n">tgt_mask</span> <span class="o">=</span> <span class="p">(</span><span class="n">tgt</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
     <span class="n">seq_length</span> <span class="o">=</span> <span class="n">tgt</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
     <span class="c1"># Prevent the decoder from accessing future tokens in the target sequence.</span>
     <span class="c1"># The resulting `causal_mask` is a boolean mask of shape [1, `seq_length`, `seq_length`], with lower triangle (diagonal included) being True and all others being False.</span>
     <span class="c1"># It looks like this for a sequence length of 5:</span>
     <span class="c1"># [[[ True, False, False, False, False],</span>
     <span class="c1">#   [ True,  True, False, False, False],</span>
     <span class="c1">#   [ True,  True,  True, False, False],</span>
     <span class="c1">#   [ True,  True,  True,  True, False],</span>
     <span class="c1">#   [ True,  True,  True,  True,  True]]]</span>
     <span class="c1"># The added extra dimension enables later `tgt_mask &amp; causal_mask` operation.</span>
     <span class="c1"># The `causal_mask`&#39;s new shape [1, `seq_length`, `seq_length`] is broadcastable to `tgt_mask`&#39;s shape [`batch_size`, 1, 1, `seq_length`]</span>
     <span class="n">causal_mask</span> <span class="o">=</span> <span class="o">~</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">triu</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">seq_length</span><span class="p">,</span> <span class="n">seq_length</span><span class="p">),</span> <span class="n">diagonal</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">bool</span><span class="p">())</span>

     <span class="c1"># Consider the target is represented by single tensor tgt = [[5, 3, 7, 0, 0]] (i.e., batch size is 1), where 0 denotes padding tokens.</span>
     <span class="c1"># The bitwise AND operation results in:</span>
     <span class="c1"># [[[True, False, False, False, False],</span>
     <span class="c1">#    [True,  True, False, False, False],</span>
     <span class="c1">#    [True,  True,  True, False, False],</span>
     <span class="c1">#    [False, False, False, False, False],</span>
     <span class="c1">#    [False, False, False, False, False]]]</span>
     <span class="n">tgt_mask</span> <span class="o">=</span> <span class="n">tgt_mask</span> <span class="o">&amp;</span> <span class="n">causal_mask</span>

     <span class="k">return</span> <span class="n">src_mask</span><span class="p">,</span> <span class="n">tgt_mask</span>
</pre></div>
</div>
</li>
<li><p><strong>Complete Transformer Model</strong>: The full Transformer combines multiple encoder and decoder layers.</p>
<ul>
<li><p>An example task is summary, where you have input tokens for both the encoder layers and decoder layers.</p>
<div class="folding highlight-python notranslate" id="transformer-input-example-summary"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">create_dummy_summarization_data</span><span class="p">(</span><span class="n">num_examples</span><span class="o">=</span><span class="mi">10</span><span class="p">):</span>
    <span class="n">articles</span> <span class="o">=</span> <span class="p">[</span>
                   <span class="s2">&quot;the cat sat on the mat and watched the birds fly by in the clear blue sky&quot;</span><span class="p">,</span>
                   <span class="s2">&quot;scientists discover new species of butterfly in the amazon rainforest last summer&quot;</span><span class="p">,</span>
                   <span class="s2">&quot;local restaurant wins award for best pizza in the city fifth year in row&quot;</span><span class="p">,</span>
                   <span class="s2">&quot;students develop innovative app to help reduce food waste in school cafeterias&quot;</span><span class="p">,</span>
                   <span class="s2">&quot;new study shows benefits of regular exercise on mental health and productivity&quot;</span><span class="p">,</span>
                   <span class="s2">&quot;artist creates stunning mural celebrating community diversity and unity&quot;</span><span class="p">,</span>
                   <span class="s2">&quot;volunteer group organizes successful beach cleanup removing plastic waste&quot;</span><span class="p">,</span>
                   <span class="s2">&quot;tech company launches eco friendly laptop made from recycled materials&quot;</span><span class="p">,</span>
                   <span class="s2">&quot;urban garden project transforms abandoned lot into community vegetable garden&quot;</span><span class="p">,</span>
                   <span class="s2">&quot;music festival brings together local talents raising funds for education&quot;</span>
               <span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="n">num_examples</span> <span class="o">//</span> <span class="mi">10</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>

    <span class="n">summaries</span> <span class="o">=</span> <span class="p">[</span>
                    <span class="s2">&quot;cat watches birds from mat&quot;</span><span class="p">,</span>
                    <span class="s2">&quot;new butterfly species found in amazon&quot;</span><span class="p">,</span>
                    <span class="s2">&quot;restaurant wins best pizza award again&quot;</span><span class="p">,</span>
                    <span class="s2">&quot;students create food waste reduction app&quot;</span><span class="p">,</span>
                    <span class="s2">&quot;exercise improves mental health study finds&quot;</span><span class="p">,</span>
                    <span class="s2">&quot;artist paints community unity mural&quot;</span><span class="p">,</span>
                    <span class="s2">&quot;volunteers clean beach of plastic&quot;</span><span class="p">,</span>
                    <span class="s2">&quot;eco friendly laptop launches&quot;</span><span class="p">,</span>
                    <span class="s2">&quot;community garden replaces empty lot&quot;</span><span class="p">,</span>
                    <span class="s2">&quot;local music festival supports education&quot;</span>
                <span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="n">num_examples</span> <span class="o">//</span> <span class="mi">10</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">articles</span><span class="p">[:</span><span class="n">num_examples</span><span class="p">],</span> <span class="n">summaries</span><span class="p">[:</span><span class="n">num_examples</span><span class="p">]</span>
</pre></div>
</div>
</li>
<li><p>This is a complete transformer model assembling the encoder/decoder layers.</p>
<div class="folding highlight-python notranslate" id="transformer-assembled-encoder-decoder-layers"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">Transformer</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    The default Transformer model adopts encoder-decoder architecture, where the encoder input and decode input could be different (e.g., translation).</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">src_vocab_size</span><span class="p">,</span> <span class="n">tgt_vocab_size</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">num_layers</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">,</span> <span class="n">max_seq_length</span><span class="o">=</span><span class="mi">5000</span><span class="p">,</span>
                 <span class="n">dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">encoder_embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">src_vocab_size</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">decoder_embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">tgt_vocab_size</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">positional_encoding</span> <span class="o">=</span> <span class="n">PositionalEncoding</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">max_seq_length</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">encoder_layers</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">([</span>
            <span class="n">EncoderLayer</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">,</span> <span class="n">dropout</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_layers</span><span class="p">)</span>
        <span class="p">])</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">decoder_layers</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">([</span>
            <span class="n">DecoderLayer</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">,</span> <span class="n">dropout</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_layers</span><span class="p">)</span>
        <span class="p">])</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">fc</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">tgt_vocab_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">generate_mask</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">src</span><span class="p">,</span> <span class="n">tgt</span><span class="p">):</span>
        <span class="c1"># ... as implemented in above ...</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">src</span><span class="p">,</span> <span class="n">tgt</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Forward pass of the Transformer.</span>

<span class="sd">        Args:</span>
<span class="sd">            src (Tensor): (batch_size, src_seq_len) of token indices for the encoder.</span>
<span class="sd">            tgt (Tensor): (batch_size, tgt_seq_len) of token indices for the decoder.</span>

<span class="sd">        Returns:</span>
<span class="sd">            logits (Tensor): (batch_size, tgt_seq_len, tgt_vocab_size)</span>
<span class="sd">                             unnormalized scores for each token in the target sequence.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># -----------------------</span>
        <span class="c1"># 1) Generate Attention Masks</span>
        <span class="c1"># -----------------------</span>
        <span class="n">src_mask</span><span class="p">,</span> <span class="n">tgt_mask</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">generate_mask</span><span class="p">(</span><span class="n">src</span><span class="p">,</span> <span class="n">tgt</span><span class="p">)</span>

        <span class="c1"># -----------------------</span>
        <span class="c1"># 2) Encoder</span>
        <span class="c1"># -----------------------</span>

        <span class="c1"># Embed and add positional encoding (early fusion of position and sequence embeddings)</span>
        <span class="n">enc_input</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">positional_encoding</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">encoder_embedding</span><span class="p">(</span><span class="n">src</span><span class="p">)))</span>

        <span class="c1"># Pass through several encoder layers</span>
        <span class="n">enc_output</span> <span class="o">=</span> <span class="n">enc_input</span>
        <span class="k">for</span> <span class="n">enc_layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder_layers</span><span class="p">:</span>
            <span class="n">enc_output</span> <span class="o">=</span> <span class="n">enc_layer</span><span class="p">(</span><span class="n">enc_output</span><span class="p">,</span> <span class="n">src_mask</span><span class="p">)</span>

        <span class="c1"># -----------------------</span>
        <span class="c1"># 3) Decoder</span>
        <span class="c1"># -----------------------</span>

        <span class="c1"># Embed and add positional encoding (early fusion of position and sequence embeddings)</span>
        <span class="n">dec_input</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">positional_encoding</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">decoder_embedding</span><span class="p">(</span><span class="n">tgt</span><span class="p">)))</span>
        <span class="n">dec_output</span> <span class="o">=</span> <span class="n">dec_input</span>

        <span class="c1"># Pass through several decoder layers</span>
        <span class="k">for</span> <span class="n">dec_layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder_layers</span><span class="p">:</span>
            <span class="n">dec_output</span> <span class="o">=</span> <span class="n">dec_layer</span><span class="p">(</span><span class="n">dec_output</span><span class="p">,</span> <span class="n">enc_output</span><span class="p">,</span> <span class="n">src_mask</span><span class="p">,</span> <span class="n">tgt_mask</span><span class="p">)</span>

        <span class="c1"># -----------------------</span>
        <span class="c1"># 4) Final Output Projection</span>
        <span class="c1"># -----------------------</span>
        <span class="c1"># Project decoder output to vocab dimension</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc</span><span class="p">(</span><span class="n">dec_output</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">logits</span>
</pre></div>
</div>
</li>
</ul>
</li>
</ul>
</section>
</section>
<section id="gpt-architecture">
<h2>GPT Architecture<a class="headerlink" href="#gpt-architecture" title="Link to this heading"></a></h2>
<p>The <span class="target" id="newconcept-gpt"></span><span class="newconcept">GPT (Generative Pre-Trained Transformer)</span> architecture represents a significant evolution of the original Transformer model, designed specifically for generative tasks. Unlike the standard encoder-decoder Transformer, GPT employs only the decoder component with modifications to <strong class="underline-bold">enable autoregressive text generation</strong> capabilities. This section is diving into the publicly disclosed GPT2 architecture.</p>
<p>GPT2 model architecture differs from the original Transformer in several ways:</p>
<ul class="simple">
<li><p><strong>Decoder-Only Design</strong>: GPT uses only a stack of modified decoder blocks without the encoder component.</p>
<ul>
<li><p><strong>No Cross-Attention</strong>: As a result, GPT’s decoder blocks lack the encoder-decoder cross-attention mechanism found in the original Transformer’s decoder.</p></li>
</ul>
</li>
<li><p><strong>Pre-LayerNorm Architecture</strong>: GPT found layer normalization before attention and feed-forward operations rather than after can enhance training stability.</p>
<ul>
<li><p><strong>Direct Residual Connections</strong>: As a result, the residual connections are outside the layer norm to improve gradient flow in deep networks.</p></li>
</ul>
</li>
<li><p><span class="target" id="newconcept-weight_tying"></span><span class="newconcept">Weight Tying</span>: GPT implements parameter sharing between the input embedding matrix and the output projection matrix.</p>
<ul>
<li><p>With “Weight Tying”, the GPT decoder can be interpreted as inferencing weights across token feature dimensions, so that the weighted sums of the features can serve as logits across the vocabulary.</p></li>
</ul>
</li>
<li><p><strong>Autoregressive Generation</strong>: GPT generates tokens sequentially, with each new token conditioned on all previously generated tokens.</p></li>
</ul>
<section id="gpt-decoder">
<h3>GPT Decoder<a class="headerlink" href="#gpt-decoder" title="Link to this heading"></a></h3>
<p>At the core of GPT is the <span class="target" id="newconcept-gpt_decoder_block"></span><span class="newconcept">GPT Decoder Block</span>, a modified version of the original <a class="reference external" href="02_transformer_models.html#code-transformer-decoder">Transformer decoder layer</a>.</p>
<blockquote>
<div><ul class="simple">
<li><p>For the original Transformer decoder, it is “self-attention $\rightarrow$ dropout + residual connection + layer norm $\rightarrow$ cross-attention $\rightarrow$ dropout + residual connection + layer norm $\rightarrow$ position-wise feedforward $\rightarrow$ dropout + residual connection + layer norm”.</p></li>
<li><p>For GPT decoder, it is “layer norm $\rightarrow$ self attention $\rightarrow$ dropout + residual connection $\rightarrow$ layer norm $\rightarrow$ position-wise feedforward $\rightarrow$ dropout + residual connection”.</p></li>
</ul>
</div></blockquote>
<div class="folding highlight-python notranslate" id="gpt2-decoder-block"><div class="highlight"><pre><span></span> <span class="k">class</span> <span class="nc">GPTDecoderBlock</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">     </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">     GPT style decoder block using transformer components.</span>

<span class="sd">     Key differences:</span>
<span class="sd">     1. Doesn&#39;t have cross-attention to attend to encoder outputs like in Transformer&#39;s `DecoderLayer`, because GPT employs encoder-only design.</span>
<span class="sd">     2. Uses pre-LayerNorm (before attention); Transformer&#39;s `DecoderLayer` uses post-LayerNorm (after attention).</span>
<span class="sd">     3. Applies direct residual connection, where the residual connections outside layer norm to improve gradient flow in deep networks.</span>
<span class="sd">     &quot;&quot;&quot;</span>

     <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
         <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
         <span class="bp">self</span><span class="o">.</span><span class="n">norm1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>
         <span class="bp">self</span><span class="o">.</span><span class="n">norm2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>

         <span class="c1"># Using the MultiHeadAttention from transformer</span>
         <span class="bp">self</span><span class="o">.</span><span class="n">self_attn</span> <span class="o">=</span> <span class="n">MultiHeadAttention</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">)</span>
         <span class="c1"># Using the PositionWiseFeedForward from transformer</span>
         <span class="bp">self</span><span class="o">.</span><span class="n">feed_forward</span> <span class="o">=</span> <span class="n">PositionWiseFeedForward</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">)</span>

         <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">)</span>

     <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
         <span class="c1"># Pre-LayerNorm architecture</span>
         <span class="c1"># Recent researches found Pre-LayerNorm (GPT-style) is generally more stable during training,</span>
         <span class="c1"># and helps avoid vanishing/exploding gradients in deep networks,</span>
         <span class="c1"># and therefore it allows for training deeper models and using larger learning rates.</span>
         <span class="n">x_norm</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

         <span class="c1"># MultiHeadAttention expects Q, K, V, they all equal to `x_norm` here for post-norm self-attention.</span>
         <span class="n">attn_output</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">self_attn</span><span class="p">(</span><span class="n">x_norm</span><span class="p">,</span> <span class="n">x_norm</span><span class="p">,</span> <span class="n">x_norm</span><span class="p">,</span> <span class="n">mask</span><span class="p">)</span>
         <span class="c1"># The previous `x` is added to the transformed output, creating a residual/skip path.</span>
         <span class="c1"># 1. Provides direct paths for gradients to flow backward.</span>
         <span class="c1"># 2. Helps combat vanishing gradients in deep networks, and therefore allows for training much deeper networks effectively.</span>
         <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">attn_output</span><span class="p">)</span>

         <span class="c1"># Feed-forward with pre-LayerNorm</span>
         <span class="n">x_norm</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
         <span class="n">ff_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">feed_forward</span><span class="p">(</span><span class="n">x_norm</span><span class="p">)</span>
         <span class="c1"># The previous `x` is added to the transformed output, creating a residual/skip path.</span>
         <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">ff_output</span><span class="p">)</span>

         <span class="k">return</span> <span class="n">x</span>
</pre></div>
</div>
<p>The full GPT model, represented by the <code class="docutils literal notranslate"><span class="pre">GPTDecoder</span></code> class, assembles multiple decoder blocks into a cohesive architecture that output logits in its <code class="docutils literal notranslate"><span class="pre">forward</span></code> function.</p>
<ul class="simple">
<li><p>The <code class="docutils literal notranslate"><span class="pre">generate_mask</span></code> function is consistent with the <a class="reference external" href="02_transformer_models.html#code-transformer-masking">Transformer causal mask</a>.</p></li>
<li><p>The output logits will be further processed for next-token generation, with various hyper-parameters (e.g., temperature, top_p, top_k) and different strategies (e.g., beam search).</p></li>
</ul>
<div class="folding highlight-python notranslate" id="gpt2-decoder-class"><div class="highlight"><pre><span></span> <span class="k">class</span> <span class="nc">GPTDecoder</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
     <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                  <span class="n">vocab_size</span><span class="p">,</span>           <span class="c1"># Size of vocabulary (number of possible tokens)</span>
                  <span class="n">d_model</span><span class="o">=</span><span class="mi">768</span><span class="p">,</span>          <span class="c1"># Dimension of embeddings and hidden states</span>
                  <span class="n">num_heads</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span>         <span class="c1"># Number of attention heads</span>
                  <span class="n">num_layers</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span>        <span class="c1"># Number of decoder layers</span>
                  <span class="n">d_ff</span><span class="o">=</span><span class="mi">3072</span><span class="p">,</span>            <span class="c1"># Dimension of feed-forward layer</span>
                  <span class="n">max_seq_length</span><span class="o">=</span><span class="mi">1024</span><span class="p">,</span>  <span class="c1"># Maximum sequence length supported</span>
                  <span class="n">dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>          <span class="c1"># Dropout rate</span>
                  <span class="n">pad_token_id</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>      <span class="c1"># ID of padding token</span>
         <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

         <span class="bp">self</span><span class="o">.</span><span class="n">vocab_size</span> <span class="o">=</span> <span class="n">vocab_size</span>
         <span class="bp">self</span><span class="o">.</span><span class="n">max_seq_length</span> <span class="o">=</span> <span class="n">max_seq_length</span>
         <span class="bp">self</span><span class="o">.</span><span class="n">pad_token_id</span> <span class="o">=</span> <span class="n">pad_token_id</span>

         <span class="c1"># Token embedding</span>
         <span class="c1"># Input shape: (batch_size, seq_length) of token IDs</span>
         <span class="c1"># Output shape: (batch_size, seq_length, d_model)</span>
         <span class="bp">self</span><span class="o">.</span><span class="n">token_embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>

         <span class="c1"># Positional encoding</span>
         <span class="c1"># Input shape: (batch_size, seq_length, d_model)</span>
         <span class="c1"># Output shape: (batch_size, seq_length, d_model)</span>
         <span class="bp">self</span><span class="o">.</span><span class="n">positional_encoding</span> <span class="o">=</span> <span class="n">PositionalEncoding</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">max_seq_length</span><span class="p">)</span>

         <span class="c1"># Dropout layer - same shape in and out</span>
         <span class="c1"># Input/Output shape: (batch_size, seq_length, d_model)</span>
         <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">)</span>

         <span class="c1"># Decoder layers - stack of identical transformer blocks</span>
         <span class="c1"># Each layer:</span>
         <span class="c1"># Input shape: (batch_size, seq_length, d_model)</span>
         <span class="c1"># Output shape: (batch_size, seq_length, d_model)</span>
         <span class="bp">self</span><span class="o">.</span><span class="n">decoder_layers</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">([</span>
             <span class="n">GPTDecoderBlock</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">,</span> <span class="n">dropout</span><span class="p">)</span>
             <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_layers</span><span class="p">)</span>
         <span class="p">])</span>

         <span class="c1"># Final layer normalization</span>
         <span class="c1"># Input shape: (batch_size, seq_length, d_model) or (batch_size, d_model) during inference</span>
         <span class="c1"># Output shape: Same as input</span>
         <span class="bp">self</span><span class="o">.</span><span class="n">norm_final</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>

         <span class="c1"># Linear projection to vocabulary (language model head)</span>
         <span class="c1"># Input shape: (batch_size, seq_length, d_model) or (batch_size, d_model) during inference</span>
         <span class="c1"># Output shape: (batch_size, seq_length, vocab_size) or (batch_size, vocab_size) during inference</span>
         <span class="bp">self</span><span class="o">.</span><span class="n">lm_head</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

         <span class="c1"># Weight tying: Share parameters between token embedding and output projection</span>
         <span class="c1"># Saves vocab_size × d_model parameters (e.g., 50,257 × 768 ≈ 38.6M parameters for GPT-2)</span>
         <span class="bp">self</span><span class="o">.</span><span class="n">lm_head</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">token_embedding</span><span class="o">.</span><span class="n">weight</span>

     <span class="k">def</span> <span class="nf">generate_mask</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_ids</span><span class="p">):</span>
<span class="w">         </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">         Generate a mask for self-attention that handles both padding tokens and causal masking.</span>
<span class="sd">         Adapted from the Transformer&#39;s generate_mask method.</span>

<span class="sd">         Args:</span>
<span class="sd">             input_ids (Tensor): Input token ids of shape (batch_size, seq_length)</span>

<span class="sd">         Returns:</span>
<span class="sd">             mask (Tensor): Combined padding and causal mask</span>
<span class="sd">         &quot;&quot;&quot;</span>
         <span class="c1"># Create padding mask (batch_size, 1, 1, seq_length)</span>
         <span class="c1"># True = keep, False = mask out (padding)</span>
         <span class="n">padding_mask</span> <span class="o">=</span> <span class="p">(</span><span class="n">input_ids</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_token_id</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>

         <span class="c1"># Create causal mask (1, seq_length, seq_length)</span>
         <span class="n">seq_length</span> <span class="o">=</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
         <span class="n">causal_mask</span> <span class="o">=</span> <span class="o">~</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">triu</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">seq_length</span><span class="p">,</span> <span class="n">seq_length</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">input_ids</span><span class="o">.</span><span class="n">device</span><span class="p">),</span> <span class="n">diagonal</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">bool</span><span class="p">())</span>

         <span class="c1"># Combine masks with bitwise AND (broadcasting handles the different shapes)</span>
         <span class="c1"># Both masks have True for positions to keep/attend to and False for positions to mask out</span>
         <span class="n">combined_mask</span> <span class="o">=</span> <span class="n">padding_mask</span> <span class="o">&amp;</span> <span class="n">causal_mask</span>

         <span class="k">return</span> <span class="n">combined_mask</span>

     <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_ids</span><span class="p">,</span> <span class="n">inference</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">         </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">         Forward pass through the GPT model.</span>

<span class="sd">         Args:</span>
<span class="sd">             input_ids: Tensor of shape (batch_size, seq_length) containing token IDs</span>
<span class="sd">             inference: Boolean flag for inference mode (only return last token&#39;s logits)</span>

<span class="sd">         Returns:</span>
<span class="sd">             - Training mode: logits of shape (batch_size, seq_length, vocab_size)</span>
<span class="sd">             - Inference mode: logits of shape (batch_size, vocab_size) for just the final token</span>
<span class="sd">         &quot;&quot;&quot;</span>
         <span class="c1"># Token embedding: (batch_size, seq_length) → (batch_size, seq_length, d_model)</span>
         <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">token_embedding</span><span class="p">(</span><span class="n">input_ids</span><span class="p">)</span>

         <span class="c1"># Add positional encoding: (batch_size, seq_length, d_model) → (batch_size, seq_length, d_model)</span>
         <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">positional_encoding</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

         <span class="c1"># Apply dropout: shape unchanged</span>
         <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

         <span class="c1"># Generate mask for attention</span>
         <span class="c1"># Shape: (batch_size, 1, seq_length, seq_length)</span>
         <span class="n">mask</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">generate_mask</span><span class="p">(</span><span class="n">input_ids</span><span class="p">)</span>

         <span class="c1"># Pass through decoder layers</span>
         <span class="c1"># Each layer maintains shape: (batch_size, seq_length, d_model)</span>
         <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder_layers</span><span class="p">:</span>
             <span class="n">x</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">mask</span><span class="p">)</span>

         <span class="c1"># Apply final normalization</span>
         <span class="k">if</span> <span class="n">inference</span><span class="p">:</span>
             <span class="c1"># During inference, only process the last token position</span>
             <span class="c1"># x[:, -1, :] will extract the last</span>
             <span class="c1"># (batch_size, seq_length, d_model) → (batch_size, d_model)</span>
             <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm_final</span><span class="p">(</span><span class="n">x</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:])</span>

             <span class="c1"># Project to vocabulary</span>
             <span class="c1"># (batch_size, d_model) → (batch_size, vocab_size)</span>
             <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lm_head</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
         <span class="k">else</span><span class="p">:</span>
             <span class="c1"># During training, process all positions</span>
             <span class="c1"># Shape maintained: (batch_size, seq_length, d_model)</span>
             <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm_final</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

             <span class="c1"># Project to vocabulary</span>
             <span class="c1"># (batch_size, seq_length, d_model) → (batch_size, seq_length, vocab_size)</span>
             <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lm_head</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

         <span class="k">return</span> <span class="n">logits</span>
</pre></div>
</div>
<div class="note admonition">
<p class="admonition-title">Weight Tying in Large Language Models</p>
<p><strong>Weight Tying</strong> is a parameter sharing technique where models constrain certain layers to use identical weights, most commonly between the input token embedding matrix and the output projection layer (LM head).</p>
<div class="math notranslate nohighlight">
\[\mathbf{W}_\text{embed} = \mathbf{W}_\text{lm_head}^T\]</div>
<p>Where <span class="math notranslate nohighlight">\(\mathbf{W}_\text{embed} \in \mathbb{R}^{V \times d}\)</span> is the embedding matrix mapping tokens to vector space, and <span class="math notranslate nohighlight">\(\mathbf{W}_\text{lm_head} \in \mathbb{R}^{d \times V}\)</span> projects hidden states back to vocabulary logits.</p>
<p><strong>Benefits:</strong></p>
<ul class="simple">
<li><p><strong>Parameter Efficiency</strong>: For a vocabulary size of 50K and embedding dimension of 4096, weight tying saves ~205M parameters.</p></li>
<li><p><strong>Regularization Effect</strong>: Enforces consistency between input representations and output predictions.</p></li>
<li><p><strong>Faster Convergence</strong>: May accelerate training by creating a more direct learning signal.</p></li>
</ul>
<p><strong>Current Implementation in Modern LLMs:</strong></p>
<table class="docutils align-default">
<colgroup>
<col style="width: 30.0%" />
<col style="width: 20.0%" />
<col style="width: 50.0%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Model</p></th>
<th class="head"><p>Weight Tying</p></th>
<th class="head"><p>Implementation Details</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>GPT-4</strong></p></td>
<td><p>Likely used</p></td>
<td><p>Not officially confirmed, but consistent with earlier GPT models</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Claude 3.5/3.7</strong></p></td>
<td><p>Unknown</p></td>
<td><p>Not disclosed in public documentation</p></td>
</tr>
<tr class="row-even"><td><p><strong>LLaMA 3.2</strong></p></td>
<td><p>Selectively implemented</p></td>
<td><p>Used in the 3B parameter model, status varies in larger variants</p></td>
</tr>
<tr class="row-odd"><td><p><strong>DeepSeek-V3</strong></p></td>
<td><p>Not used by default</p></td>
<td><p>Configurable via <code class="docutils literal notranslate"><span class="pre">tie_word_embeddings=False</span></code> default setting</p></td>
</tr>
</tbody>
</table>
<p><strong>Implementation Considerations:</strong></p>
<p>The decision to implement weight tying appears to be influenced by model size and architecture. Smaller models (like LLaMA 3.2 3B) tend to benefit more from the parameter efficiency, while larger models forgo weight tying to maximize performance as the benefit diminishes in comparison to the much larger overall model size due to increased number of decoder layers.</p>
<p><strong>Trade-offs:</strong></p>
<p>While weight tying enhances efficiency, it introduces a constraint that may limit representational flexibility. The optimal decision depends on whether priority is given to parameter efficiency or maximum model expressiveness.</p>
</div>
</section>
<section id="generation-strategies">
<h3>Generation Strategies<a class="headerlink" href="#generation-strategies" title="Link to this heading"></a></h3>
<p>After a GPT model produces logits from its forward pass, these logits must be converted into actual tokens through a <span class="target" id="newconcept-decoding_strategy"></span><span class="newconcept">decoding strategy</span>. The choice of decoding strategy significantly impacts the quality, diversity, and coherence of the generated text. GPT models support several approaches, ranging from simple deterministic methods to more sophisticated algorithms that <strong class="underline-bold">balance quality and diversity</strong>.</p>
<div class="folding highlight-python notranslate" id="unified-generate-with-strategy"><div class="highlight"><pre><span></span> <span class="k">def</span> <span class="nf">generate_with_strategy</span><span class="p">(</span>
         <span class="bp">self</span><span class="p">,</span>
         <span class="n">input_ids</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">,</span>
         <span class="n">max_new_tokens</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
         <span class="n">strategy</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;sample&quot;</span><span class="p">,</span>
         <span class="o">**</span><span class="n">kwargs</span>
 <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">:</span>
<span class="w">     </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">     Unified generation method supporting multiple decoding strategies.</span>

<span class="sd">     Args:</span>
<span class="sd">         input_ids: Input token ids</span>
<span class="sd">         max_new_tokens: Maximum number of new tokens to generate</span>
<span class="sd">         strategy: One of &quot;greedy&quot;, &quot;sample&quot;, or &quot;beam&quot;</span>
<span class="sd">         **kwargs: Additional arguments for specific strategies</span>
<span class="sd">     &quot;&quot;&quot;</span>
     <span class="k">if</span> <span class="n">strategy</span> <span class="o">==</span> <span class="s2">&quot;greedy&quot;</span><span class="p">:</span>
         <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">greedy_search</span><span class="p">(</span>
             <span class="n">input_ids</span><span class="p">,</span>
             <span class="n">max_new_tokens</span><span class="p">,</span>
             <span class="n">pad_token_id</span><span class="o">=</span><span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;pad_token_id&quot;</span><span class="p">),</span>
             <span class="n">eos_token_id</span><span class="o">=</span><span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;eos_token_id&quot;</span><span class="p">)</span>
         <span class="p">)</span>
     <span class="k">elif</span> <span class="n">strategy</span> <span class="o">==</span> <span class="s2">&quot;sample&quot;</span><span class="p">:</span>
         <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span>
             <span class="n">input_ids</span><span class="p">,</span>
             <span class="n">max_new_tokens</span><span class="p">,</span>
             <span class="n">temperature</span><span class="o">=</span><span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;temperature&quot;</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">),</span>
             <span class="n">top_k</span><span class="o">=</span><span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;top_k&quot;</span><span class="p">),</span>
             <span class="n">top_p</span><span class="o">=</span><span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;top_p&quot;</span><span class="p">),</span>
             <span class="n">pad_token_id</span><span class="o">=</span><span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;pad_token_id&quot;</span><span class="p">),</span>
             <span class="n">eos_token_id</span><span class="o">=</span><span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;eos_token_id&quot;</span><span class="p">)</span>
         <span class="p">)</span>
     <span class="k">elif</span> <span class="n">strategy</span> <span class="o">==</span> <span class="s2">&quot;beam&quot;</span><span class="p">:</span>
         <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">beam_search</span><span class="p">(</span>
             <span class="n">input_ids</span><span class="p">,</span>
             <span class="n">max_new_tokens</span><span class="p">,</span>
             <span class="n">num_beams</span><span class="o">=</span><span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;num_beams&quot;</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span>
             <span class="n">pad_token_id</span><span class="o">=</span><span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;pad_token_id&quot;</span><span class="p">),</span>
             <span class="n">eos_token_id</span><span class="o">=</span><span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;eos_token_id&quot;</span><span class="p">),</span>
             <span class="n">length_penalty</span><span class="o">=</span><span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;length_penalty&quot;</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">)</span>
         <span class="p">)</span>
     <span class="k">else</span><span class="p">:</span>
         <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Unknown decoding strategy: </span><span class="si">{</span><span class="n">strategy</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<section id="greedy-search">
<h4>Greedy Search<a class="headerlink" href="#greedy-search" title="Link to this heading"></a></h4>
<p><strong>Greedy Search</strong> represents the simplest decoding approach for autoregressive language models. At each generation step, it selects the single token with the highest predicted probability. This approach can be characterized as:</p>
<ul class="simple">
<li><p><strong>Locally Optimal Yet Myopic</strong>: [Pro &amp; Con] Makes the best choice at each individual step, but it doesn’t consider how current choices affect future options.</p></li>
<li><p><strong>Deterministic Yet Lacking Creativity</strong>: [Mostly Con] Given the same input and model, always produces identical output. No exploration of alternatives, even when multiple options have similar probabilities.</p></li>
<li><p><strong>Memory-Efficient</strong>: [Pro] Only needs to track a single candidate sequence.</p></li>
</ul>
<p>Before the transformer era, greedy decoding was often sufficient for simpler sequence-to-sequence tasks. As language models grew in capability and application domains expanded, its limitations became more apparent, driving the development of more sophisticated alternatives like sampling and beam search. Today its application is limited to</p>
<ul class="simple">
<li><p>Tasks requiring exact reproducibility and a single fixed answer is sufficient and diversity isn’t important.</p></li>
<li><p>Resource-constrained environments.</p></li>
</ul>
<div class="folding highlight-python notranslate" id="gpt-greedy-search"><div class="highlight"><pre><span></span> <span class="k">class</span> <span class="nc">GPTDecoder</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
     <span class="c1"># ... existing methods ...</span>

     <span class="k">def</span> <span class="nf">process_logits_for_finished_sequences</span><span class="p">(</span>
             <span class="bp">self</span><span class="p">,</span>
             <span class="n">logits</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
             <span class="n">finished_sequences_flags</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
             <span class="n">pad_token_id</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
     <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">         </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">         Process logits to ensure finished sequences only generate padding tokens.</span>

<span class="sd">         Args:</span>
<span class="sd">             logits: Tensor of logits with shape (batch_size, vocab_size)</span>
<span class="sd">             finished_sequences_flags: Boolean tensor of shape (batch_size) tracking which</span>
<span class="sd">                                sequences have already generated an EOS token</span>
<span class="sd">             pad_token_id: Token ID to use for padding (if None, no special handling occurs)</span>

<span class="sd">         Returns:</span>
<span class="sd">             Modified logits where finished sequences can only select padding token</span>
<span class="sd">         &quot;&quot;&quot;</span>
         <span class="c1"># If no sequences are finished or no pad token provided, return logits unchanged</span>
         <span class="k">if</span> <span class="n">pad_token_id</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="ow">not</span> <span class="n">finished_sequences_flags</span><span class="o">.</span><span class="n">any</span><span class="p">():</span>
             <span class="k">return</span> <span class="n">logits</span>

         <span class="c1"># torch.full_like` creates a new tensor with the same size and data type as the input tensor, but filled with a specified value</span>
         <span class="c1"># Below sets all token probabilities to -infinity (impossible to select)</span>
         <span class="c1"># Shape: [batch_size, vocab_size] - same as logits</span>
         <span class="n">pad_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">full_like</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="nb">float</span><span class="p">(</span><span class="s1">&#39;-inf&#39;</span><span class="p">))</span>
         <span class="c1"># Set only the padding token&#39;s logit to 0 (making it the only possible choice)</span>
         <span class="n">pad_mask</span><span class="p">[:,</span> <span class="n">pad_token_id</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>

         <span class="c1"># Apply this padding mask only to sequences that are marked as finished</span>
         <span class="c1"># For unfinished sequences, keep original logits</span>
         <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span>
                     <span class="n">finished_sequences_flags</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span>  <span class="c1"># Need to add a dimension for proper broadcasting with `logits`: [batch_size, 1]</span>
                     <span class="n">pad_mask</span><span class="p">,</span>                         <span class="c1"># Use pad_mask for finished sequences</span>
                     <span class="n">logits</span>                            <span class="c1"># Use original logits for unfinished sequences</span>
                 <span class="p">)</span>

     <span class="k">def</span> <span class="nf">update_finished_sequence_flags</span><span class="p">(</span>
             <span class="bp">self</span><span class="p">,</span>
             <span class="n">finished_sequences_flags</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
             <span class="n">next_token</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
             <span class="n">eos_token_id</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
     <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">         </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">         Update tracking for which sequences have finished generating.</span>

<span class="sd">         Args:</span>
<span class="sd">             finished_sequences_flags: Boolean tensor of shape (batch_size) tracking which</span>
<span class="sd">                                sequences have already generated an EOS token</span>
<span class="sd">             next_token: Tensor of shape (batch_size, 1) or (batch_size) containing</span>
<span class="sd">                        the next token generated for each sequence</span>
<span class="sd">             eos_token_id: Token ID that indicates end of sequence (if None, no sequences are marked as finished)</span>

<span class="sd">         Returns:</span>
<span class="sd">             Updated finished_sequences_flags tensor</span>
<span class="sd">         &quot;&quot;&quot;</span>
         <span class="c1"># If no EOS token specified, no sequences finish</span>
         <span class="k">if</span> <span class="n">eos_token_id</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
             <span class="k">return</span> <span class="n">finished_sequences_flags</span>

         <span class="c1"># Ensure next_token is a 1D tensor by removing the last dimension if present</span>
         <span class="k">if</span> <span class="n">next_token</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
             <span class="n">next_token</span> <span class="o">=</span> <span class="n">next_token</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>

         <span class="c1"># Identify which sequences generated an EOS token in this step</span>
         <span class="n">just_finished</span> <span class="o">=</span> <span class="p">(</span><span class="n">next_token</span> <span class="o">==</span> <span class="n">eos_token_id</span><span class="p">)</span>

         <span class="c1"># Update existing finished flags with newly finished sequences</span>
         <span class="k">return</span> <span class="n">finished_sequences_flags</span> <span class="o">|</span> <span class="n">just_finished</span>

     <span class="nd">@torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span>
     <span class="k">def</span> <span class="nf">greedy_search</span><span class="p">(</span>
             <span class="bp">self</span><span class="p">,</span>
             <span class="n">input_ids</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">,</span>
             <span class="n">max_new_tokens</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
             <span class="n">pad_token_id</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
             <span class="n">eos_token_id</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
     <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">:</span>
<span class="w">         </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">         Simplest decoding method: always choose the most likely next token.</span>

<span class="sd">         Handles sequences that finish at different times by tracking which sequences</span>
<span class="sd">         have generated an EOS token and padding future tokens for those sequences.</span>
<span class="sd">         &quot;&quot;&quot;</span>

         <span class="c1"># Get the batch size, i.e., the number of sequences</span>
         <span class="n">batch_size</span> <span class="o">=</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

         <span class="c1"># ----------------</span>
         <span class="c1"># 1. Creates an array to track completed sequences</span>
         <span class="c1"># ----------------</span>
         <span class="c1"># One-dimensional array (length is `batch_size`) to track which sequences are finished (have generated EOS)</span>
         <span class="c1">#   `GPTDecoder`&#39;s forward function is not aware of the sequence completion and will continue to generate the next token</span>
         <span class="c1">#   The sequence completion is tracked here in the generation function</span>
         <span class="n">finished_sequence_flags</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bool</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">input_ids</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

         <span class="c1"># For example, suppose the vocabulary has 5 tokens with the first token as the padding token, and the second token as the EOS token</span>
         <span class="c1"># logits = [</span>
         <span class="c1">#     [2.5, 1.0, 3.2, 0.8, 1.5],  # Sequence 1 (finished)</span>
         <span class="c1">#     [0.7, 4.1, 2.3, 3.0, 1.2]   # Sequence 2 (still generating)</span>
         <span class="c1"># ]</span>
         <span class="c1">#</span>
         <span class="c1"># finished_sequence_flags = [True, False]</span>

         <span class="c1"># The generation loop</span>
         <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_new_tokens</span><span class="p">):</span>
             <span class="c1"># Crop sequence if needed</span>
             <span class="n">input_ids_cond</span> <span class="o">=</span> <span class="n">input_ids</span> <span class="k">if</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_seq_length</span> <span class="k">else</span> <span class="n">input_ids</span><span class="p">[:,</span> <span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">max_seq_length</span><span class="p">:]</span>

             <span class="c1"># ----------------</span>
             <span class="c1"># 2. Get raw logits from the model</span>
             <span class="c1"># ----------------</span>
             <span class="c1"># Get model predictions - with inference=True, this directly returns logits for the last token</span>
             <span class="c1"># Shape: (batch_size, vocab_size)</span>
             <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span><span class="n">input_ids_cond</span><span class="p">,</span> <span class="n">inference</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

             <span class="c1"># For finished sequences, replace logits with padding-token-only logits</span>
             <span class="c1"># This ensures only padding tokens will be generated for these sequences</span>
             <span class="c1"># For the above example, we would have</span>
             <span class="c1"># pad_mask = [</span>
             <span class="c1">#    [0.0, -inf, -inf, -inf, -inf],  # Only token 0 (padding) can be selected, and same for all sequences</span>
             <span class="c1">#    [0.0, -inf, -inf, -inf, -inf]</span>
             <span class="c1"># ]</span>

             <span class="c1"># ----------------</span>
             <span class="c1"># 3. Process logits for finished sequences</span>
             <span class="c1"># ----------------</span>
             <span class="c1"># Handle finished sequences by forcing them to generate padding</span>
             <span class="c1"># For the above example, we would have</span>
             <span class="c1"># logits = [</span>
             <span class="c1">#     [0.0, -inf, -inf, -inf, -inf],  # Sequence 1 (can only generate padding)</span>
             <span class="c1">#     [0.7, 4.1, 2.3, 3.0, 1.2]       # Sequence 2 (unchanged)</span>
             <span class="c1"># ]</span>
             <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">process_logits_for_finished_sequences</span><span class="p">(</span>
                 <span class="n">logits</span><span class="p">,</span>
                 <span class="n">finished_sequence_flags</span><span class="p">,</span>
                 <span class="n">pad_token_id</span>
             <span class="p">)</span>

             <span class="c1"># ----------------</span>
             <span class="c1"># 4. Get next token and append them to input_ids</span>
             <span class="c1"># ----------------</span>
             <span class="c1"># Select the highest probability token for each sequence in batch</span>
             <span class="c1"># Shape: [batch_size, 1]</span>
             <span class="n">next_token</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

             <span class="c1"># ----------------</span>
             <span class="c1"># 5. Update finished_sequence_flags and break the loop if all sequences have completed</span>
             <span class="c1"># ----------------</span>
             <span class="c1"># Append the selected tokens to our running sequences for auto-regressive generation</span>
             <span class="c1"># Shape after concatenation: [batch_size, sequence_length + 1]</span>
             <span class="n">input_ids</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">next_token</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

             <span class="c1"># Check for and handle sequences that just generated an EOS token</span>
             <span class="n">finished_sequence_flags</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">update_finished_sequence_flags</span><span class="p">(</span>
                 <span class="n">finished_sequence_flags</span><span class="p">,</span>
                 <span class="n">next_token</span><span class="p">,</span>
                 <span class="n">eos_token_id</span>
             <span class="p">)</span>

             <span class="c1"># Break the loop if all sequences have completed</span>
             <span class="k">if</span> <span class="n">finished_sequence_flags</span><span class="o">.</span><span class="n">all</span><span class="p">():</span>
                 <span class="k">break</span>

         <span class="k">return</span> <span class="n">input_ids</span>
</pre></div>
</div>
</section>
<section id="sampling-based-generation">
<h4>Sampling-Based Generation<a class="headerlink" href="#sampling-based-generation" title="Link to this heading"></a></h4>
<p><strong>Sampling-Based Generation</strong> introduces controlled randomness into text generation by treating the model’s output logits as a probability distribution. Instead of always selecting the token with the highest probability (as in greedy search), sampling draws tokens from this distribution based on their relative likelihoods. This produces more diverse and creative outputs while maintaining coherence.</p>
<div class="folding highlight-python notranslate" id="gpt-sampling-generation"><div class="highlight"><pre><span></span> <span class="k">class</span> <span class="nc">GPTDecoder</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
     <span class="c1"># ... existing methods ...</span>

     <span class="k">def</span> <span class="nf">apply_temperature</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">logits</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">temperature</span><span class="p">:</span> <span class="nb">float</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">         </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">         Apply temperature scaling to logits.</span>

<span class="sd">         Args:</span>
<span class="sd">             logits: Raw logits from model output, shape (batch_size, vocab_size)</span>
<span class="sd">             temperature: Scaling factor to control randomness</span>
<span class="sd">                          Lower values make distribution more peaked (deterministic)</span>
<span class="sd">                          Higher values make distribution more uniform (random)</span>

<span class="sd">         Returns:</span>
<span class="sd">             Temperature-scaled logits with shape (batch_size, vocab_size)</span>
<span class="sd">         &quot;&quot;&quot;</span>
         <span class="c1"># Avoid division by zero or negative temperature</span>
         <span class="k">if</span> <span class="n">temperature</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
             <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Temperature must be positive&quot;</span><span class="p">)</span>

         <span class="c1"># Apply temperature scaling - shape remains (batch_size, vocab_size)</span>
         <span class="k">return</span> <span class="n">logits</span> <span class="o">/</span> <span class="n">temperature</span>

     <span class="k">def</span> <span class="nf">apply_top_k_filtering</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">logits</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">top_k</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">         </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">         Apply top-k filtering to restrict sampling to top k tokens.</span>

<span class="sd">         Args:</span>
<span class="sd">             logits: Logits after temperature scaling, shape (batch_size, vocab_size)</span>
<span class="sd">             top_k: Number of highest probability tokens to consider</span>

<span class="sd">         Returns:</span>
<span class="sd">             Filtered logits with only top-k tokens having finite values</span>
<span class="sd">             Shape (batch_size, vocab_size)</span>
<span class="sd">         &quot;&quot;&quot;</span>
         <span class="k">if</span> <span class="n">top_k</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
             <span class="k">return</span> <span class="n">logits</span>  <span class="c1"># No filtering, shape (batch_size, vocab_size)</span>

         <span class="c1"># Find values and indices of the k largest elements (per batch)</span>
         <span class="c1"># top_k_values shape: (batch_size, min(top_k, vocab_size))</span>
         <span class="n">top_k_values</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">topk</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="nb">min</span><span class="p">(</span><span class="n">top_k</span><span class="p">,</span> <span class="n">logits</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)))</span>

         <span class="c1"># Get the threshold value for each item in the batch</span>
         <span class="c1"># This selects the smallest value among the top-k values in each batch</span>
         <span class="c1"># min_values shape: (batch_size, 1)</span>
         <span class="n">min_values</span> <span class="o">=</span> <span class="n">top_k_values</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

         <span class="c1"># Create a mask for values to filter out (below the threshold)</span>
         <span class="c1"># filter_mask shape: (batch_size, vocab_size)</span>
         <span class="c1"># True where logits &lt; min_values (these will be masked out)</span>
         <span class="n">filter_mask</span> <span class="o">=</span> <span class="n">logits</span> <span class="o">&lt;</span> <span class="n">min_values</span>

         <span class="c1"># Apply the mask by setting filtered values to -infinity</span>
         <span class="c1"># Return shape: (batch_size, vocab_size)</span>
         <span class="k">return</span> <span class="n">logits</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="n">filter_mask</span><span class="p">,</span> <span class="nb">float</span><span class="p">(</span><span class="s1">&#39;-inf&#39;</span><span class="p">))</span>

     <span class="k">def</span> <span class="nf">apply_top_p_filtering</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">logits</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">top_p</span><span class="p">:</span> <span class="nb">float</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">         </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">         Apply nucleus (top-p) sampling to dynamically filter tokens based on</span>
<span class="sd">         cumulative probability mass.</span>

<span class="sd">         Args:</span>
<span class="sd">             logits: Logits after temperature scaling (and possibly top-k filtering)</span>
<span class="sd">                    Shape (batch_size, vocab_size)</span>
<span class="sd">             top_p: Probability threshold (0.0 to 1.0)</span>
<span class="sd">                    Only tokens comprising cumulative probability mass &lt;= top_p</span>
<span class="sd">                    will be kept for sampling</span>

<span class="sd">         Returns:</span>
<span class="sd">             Filtered logits with only tokens in the nucleus having finite values</span>
<span class="sd">             Shape (batch_size, vocab_size)</span>
<span class="sd">         &quot;&quot;&quot;</span>
         <span class="k">if</span> <span class="n">top_p</span> <span class="o">&lt;=</span> <span class="mf">0.0</span> <span class="ow">or</span> <span class="n">top_p</span> <span class="o">&gt;=</span> <span class="mf">1.0</span><span class="p">:</span>
             <span class="k">return</span> <span class="n">logits</span>  <span class="c1"># No filtering for invalid values, shape (batch_size, vocab_size)</span>

         <span class="c1"># Sort logits in descending order</span>
         <span class="c1"># sorted_logits shape: (batch_size, vocab_size)</span>
         <span class="c1"># sorted_indices shape: (batch_size, vocab_size)</span>
         <span class="c1"># For example,</span>
         <span class="c1"># Let&#39;s say these were the original token IDs before sorting:</span>
         <span class="c1"># Token IDs:        [ A,    B,    C,    D,    E,    F ]</span>
         <span class="c1"># Original logits:  [1.4,  2.7,  3.0,  1.9,  1.0,  0.0]</span>
         <span class="c1">#                     D     B     A     C     E     F</span>
         <span class="c1">#</span>
         <span class="c1"># After sorting by descending logit values:</span>
         <span class="c1"># Sorted logits:     [3.0,  2.7,  1.9,  1.4,  1.0,  0.0]</span>
         <span class="c1"># Sorted Token IDs:  [ C,    B,    D,    A,    E,    F ]</span>
         <span class="c1"># sorted_indices:    [ 2,    1,    3,    0,    4,    5 ]  (positions in original array)</span>
         <span class="c1">#</span>
         <span class="c1"># The sorted_indices tell us where each position in our sorted arrays</span>
         <span class="n">sorted_logits</span><span class="p">,</span> <span class="n">sorted_indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">descending</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

         <span class="c1"># Calculate probabilities from sorted logits</span>
         <span class="c1"># sorted_probs shape: (batch_size, vocab_size)</span>
         <span class="n">sorted_probs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">sorted_logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

         <span class="c1"># Calculate cumulative probabilities</span>
         <span class="c1"># cumulative_probs shape: (batch_size, vocab_size)</span>
         <span class="n">cumulative_probs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">sorted_probs</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

         <span class="c1"># Create nucleus mask where cumulative probability exceeds threshold</span>
         <span class="c1"># Each row of the mask identifies tokens outside the nucleus</span>
         <span class="c1"># nucleus_mask shape: (batch_size, vocab_size)</span>
         <span class="c1"># True where tokens are outside the nucleus</span>
         <span class="n">nucleus_mask</span> <span class="o">=</span> <span class="n">cumulative_probs</span> <span class="o">&gt;</span> <span class="n">top_p</span>

         <span class="c1"># Always keep at least one token (the highest probability one)</span>
         <span class="c1"># First column of nucleus_mask is set to False</span>
         <span class="n">nucleus_mask</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span>

         <span class="c1"># Shift mask by one position to exclude the token that pushed us over top_p</span>
         <span class="c1"># This ensures we keep all tokens up to and including the one that crosses threshold</span>
         <span class="c1"># Shape remains (batch_size, vocab_size)</span>
         <span class="c1">#</span>
         <span class="c1"># Continuing above example,</span>
         <span class="c1"># we have these sorted token probabilities for top_p = 0.7:</span>
         <span class="c1"># Token Index:     0     1     2     3     4     5</span>
         <span class="c1"># Probability:    0.4   0.3   0.15  0.08  0.05  0.02</span>
         <span class="c1"># Cumulative:     0.4   0.7   0.85  0.93  0.98  1.00</span>
         <span class="c1">#                       ^</span>
         <span class="c1">#                       |</span>
         <span class="c1">#             Crosses 0.7 threshold here</span>
         <span class="c1">#</span>
         <span class="c1"># Initial `nucleus_mask` (where cumulative &gt; 0.7):</span>
         <span class="c1"># [False, False, True,  True,  True,  True]</span>
         <span class="c1">#   |      |</span>
         <span class="c1">#   Keep   Keep</span>
         <span class="c1"># After shifting:</span>
         <span class="c1"># [False, False, False, True,  True,  True]</span>
         <span class="c1">#   |      |      |</span>
         <span class="c1">#   Keep   Keep   Keep</span>
         <span class="c1">#</span>
         <span class="c1"># This keeps tokens 0, 1, and 2 (the threshold-crossing token),</span>
         <span class="c1"># with total probability mass of 0.85 (&gt; 0.7)</span>
         <span class="n">nucleus_mask</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:]</span> <span class="o">=</span> <span class="n">nucleus_mask</span><span class="p">[:,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>

         <span class="c1"># Convert the sorted mask back to the original token ordering</span>
         <span class="c1"># `to_remove` shape: (batch_size, vocab_size)</span>
         <span class="n">to_remove</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bool</span><span class="p">)</span>

         <span class="c1"># Continuing our example:</span>
         <span class="c1"># `nucleus_mask`: [False, False, False, True, True, True]</span>
         <span class="c1"># `to_remove` will be initialized as: [F, F, F, F, F, F]</span>
         <span class="c1">#</span>
         <span class="c1"># `scatter_` puts values from nucleus_mask into to_remove at positions specified by sorted_indices</span>
         <span class="c1"># After `scatter_`:</span>
         <span class="c1"># to_remove:         [True, False, False, False, True, True]</span>
         <span class="c1"># Token IDs:         [  A,    B,     C,     D,     E,    F ]</span>
         <span class="c1"># Original logits:   [ 1.4,  2.7,   3.0,   1.9,   1.0,  0.0]</span>
         <span class="c1">#</span>
         <span class="c1"># This correctly maps our decision to keep the top 3 tokens by probability (i.e. C, B, D)</span>
         <span class="c1">#   back to their original positions</span>
         <span class="n">to_remove</span> <span class="o">=</span> <span class="n">to_remove</span><span class="o">.</span><span class="n">scatter_</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">sorted_indices</span><span class="p">,</span> <span class="n">nucleus_mask</span><span class="p">)</span>

         <span class="c1"># Apply the mask by setting filtered tokens to -infinity</span>
         <span class="c1"># Return shape: (batch_size, vocab_size)</span>
         <span class="c1">#</span>
         <span class="c1"># Continuing our example:</span>
         <span class="c1"># Original logits:   [1.4,   2.7,   3.0,   1.9,   1.0,  0.0]</span>
         <span class="c1"># to_remove:         [True, False, False, False, True, True]</span>
         <span class="c1">#</span>
         <span class="c1"># After applying mask:</span>
         <span class="c1"># Filtered logits:   [-inf, 2.7,  3.0,  1.9, -inf, -inf]</span>
         <span class="c1">#</span>
         <span class="c1"># When these logits are converted to probabilities via softmax:</span>
         <span class="c1"># Final probs:       [0.0,  0.35, 0.47, 0.18, 0.0,  0.0]</span>
         <span class="c1">#</span>
         <span class="c1"># Now only tokens B, C, and D have non-zero probabilities,</span>
         <span class="c1"># which are properly normalized to sum to 1.0.</span>
         <span class="c1"># These three tokens correspond exactly to the top 0.85 probability mass</span>
         <span class="c1"># from our sorted distribution, as we intended with top_p = 0.7.</span>
         <span class="k">return</span> <span class="n">logits</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="n">to_remove</span><span class="p">,</span> <span class="nb">float</span><span class="p">(</span><span class="s1">&#39;-inf&#39;</span><span class="p">))</span>

     <span class="nd">@torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span>
     <span class="k">def</span> <span class="nf">sample</span><span class="p">(</span>
             <span class="bp">self</span><span class="p">,</span>
             <span class="n">input_ids</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">,</span>
             <span class="n">max_new_tokens</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
             <span class="n">temperature</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span>
             <span class="n">top_k</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
             <span class="n">top_p</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
             <span class="n">pad_token_id</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
             <span class="n">eos_token_id</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
     <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">:</span>
<span class="w">         </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">         Generate new tokens autoregressively using sampling strategies.</span>
<span class="sd">         Combines temperature scaling with optional top-k and top-p filtering.</span>

<span class="sd">         Args:</span>
<span class="sd">             input_ids: Starting token IDs of shape (batch_size, seq_length)</span>
<span class="sd">             max_new_tokens: Maximum number of tokens to generate</span>
<span class="sd">             temperature: Controls randomness (lower = more deterministic)</span>
<span class="sd">             top_k: If set, only sample from the top k most likely tokens</span>
<span class="sd">             top_p: If set, only sample from the smallest set of tokens whose</span>
<span class="sd">                    cumulative probability exceeds p</span>
<span class="sd">             pad_token_id: Token ID used for padding</span>
<span class="sd">             eos_token_id: Token ID that signals sequence completion</span>

<span class="sd">         Returns:</span>
<span class="sd">             Tensor of shape (batch_size, seq_length + generated_length)</span>
<span class="sd">         &quot;&quot;&quot;</span>
         <span class="c1"># ----------------</span>
         <span class="c1"># 1. Use greedy search for temperature=0</span>
         <span class="c1"># ----------------</span>
         <span class="k">if</span> <span class="n">temperature</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
             <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">greedy_search</span><span class="p">(</span>
                 <span class="n">input_ids</span><span class="p">,</span>
                 <span class="n">max_new_tokens</span><span class="p">,</span>
                 <span class="n">pad_token_id</span><span class="o">=</span><span class="n">pad_token_id</span><span class="p">,</span>
                 <span class="n">eos_token_id</span><span class="o">=</span><span class="n">eos_token_id</span>
             <span class="p">)</span>

         <span class="c1"># Get the batch size from the input</span>
         <span class="c1"># input_ids shape: (batch_size, seq_length)</span>
         <span class="n">batch_size</span> <span class="o">=</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

         <span class="c1"># ----------------</span>
         <span class="c1"># 2. Create tracking for completed sequences</span>
         <span class="c1"># ----------------</span>
         <span class="c1"># finished_sequence_flags shape: (batch_size)</span>
         <span class="c1"># Initialized with zeros (False) - no sequences are finished yet</span>
         <span class="n">finished_sequence_flags</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bool</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">input_ids</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

         <span class="c1"># The generation loop - will run for max_new_tokens iterations unless all sequences finish early</span>
         <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_new_tokens</span><span class="p">):</span>
             <span class="c1"># ----------------</span>
             <span class="c1"># 3. Prepare inputs for current iteration</span>
             <span class="c1"># ----------------</span>
             <span class="c1"># Crop sequence if needed to respect model&#39;s maximum sequence length</span>
             <span class="c1"># input_ids_cond shape: (batch_size, seq_length&#39;)</span>
             <span class="c1"># where seq_length&#39; = min(seq_length, max_seq_length)</span>
             <span class="n">input_ids_cond</span> <span class="o">=</span> <span class="n">input_ids</span> <span class="k">if</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_seq_length</span> <span class="k">else</span> <span class="n">input_ids</span><span class="p">[:,</span> <span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">max_seq_length</span><span class="p">:]</span>

             <span class="c1"># ----------------</span>
             <span class="c1"># 4. Get raw logits from the model</span>
             <span class="c1"># ----------------</span>
             <span class="c1"># logits shape: (batch_size, vocab_size)</span>
             <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span><span class="n">input_ids_cond</span><span class="p">,</span> <span class="n">inference</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

             <span class="c1"># ----------------</span>
             <span class="c1"># 5. Process logits for finished sequences</span>
             <span class="c1"># ----------------</span>
             <span class="c1"># Ensure sequences that have already generated EOS only produce padding tokens</span>
             <span class="c1"># logits shape remains (batch_size, vocab_size)</span>
             <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">process_logits_for_finished_sequences</span><span class="p">(</span>
                 <span class="n">logits</span><span class="p">,</span>
                 <span class="n">finished_sequence_flags</span><span class="p">,</span>
                 <span class="n">pad_token_id</span>
             <span class="p">)</span>

             <span class="c1"># ----------------</span>
             <span class="c1"># 6. Apply sampling strategies to filter logits</span>
             <span class="c1"># ----------------</span>
             <span class="c1"># First adjust with temperature</span>
             <span class="k">if</span> <span class="n">temperature</span> <span class="o">!=</span> <span class="mf">1.0</span><span class="p">:</span>
                 <span class="c1"># logits shape remains (batch_size, vocab_size)</span>
                 <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">apply_temperature</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">temperature</span><span class="p">)</span>

             <span class="c1"># Then apply top-k filtering (if specified)</span>
             <span class="k">if</span> <span class="n">top_k</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">top_k</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                 <span class="c1"># logits shape remains (batch_size, vocab_size)</span>
                 <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">apply_top_k_filtering</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">top_k</span><span class="p">)</span>

             <span class="c1"># Finally apply top-p filtering (if specified)</span>
             <span class="k">if</span> <span class="n">top_p</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="mf">0.0</span> <span class="o">&lt;</span> <span class="n">top_p</span> <span class="o">&lt;</span> <span class="mf">1.0</span><span class="p">:</span>
                 <span class="c1"># logits shape remains (batch_size, vocab_size)</span>
                 <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">apply_top_p_filtering</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">top_p</span><span class="p">)</span>

             <span class="c1"># ----------------</span>
             <span class="c1"># 7. Sample from the filtered distribution</span>
             <span class="c1"># ----------------</span>
             <span class="c1"># Convert logits to probabilities</span>
             <span class="c1"># probs shape: (batch_size, vocab_size)</span>
             <span class="n">probs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

             <span class="c1"># Sample from the probability distribution (one token per sequence)</span>
             <span class="c1"># next_token shape: (batch_size, 1)</span>
             <span class="n">next_token</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">multinomial</span><span class="p">(</span><span class="n">probs</span><span class="p">,</span> <span class="n">num_samples</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

             <span class="c1"># ----------------</span>
             <span class="c1"># 8. Append sampled tokens and update tracking</span>
             <span class="c1"># ----------------</span>
             <span class="c1"># Add new tokens to sequences</span>
             <span class="c1"># Updated input_ids shape: (batch_size, seq_length + 1)</span>
             <span class="n">input_ids</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">next_token</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

             <span class="c1"># Update tracking of which sequences have finished</span>
             <span class="c1"># finished_sequence_flags shape remains (batch_size)</span>
             <span class="n">finished_sequence_flags</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">update_finished_sequence_flags</span><span class="p">(</span>
                 <span class="n">finished_sequence_flags</span><span class="p">,</span>
                 <span class="n">next_token</span><span class="p">,</span>
                 <span class="n">eos_token_id</span>
             <span class="p">)</span>

             <span class="c1"># Break if all sequences have finished</span>
             <span class="k">if</span> <span class="n">finished_sequence_flags</span><span class="o">.</span><span class="n">all</span><span class="p">():</span>
                 <span class="k">break</span>

         <span class="c1"># Return the complete sequences with generated tokens</span>
         <span class="c1"># Final shape: (batch_size, original_seq_length + generated_length)</span>
         <span class="k">return</span> <span class="n">input_ids</span>
</pre></div>
</div>
<p>The above sampling approach introduces several key hyperparameters that control the generation behavior:</p>
<ul>
<li><p><span class="target" id="newconcept-temperature"></span><span class="newconcept">Temperature</span> controls how concentrated or spread out the probability distribution becomes before sampling:</p>
<div class="math notranslate nohighlight">
\[p_i = \frac{\exp(z_i/T)}{\sum_j \exp(z_j/T)}\]</div>
<p>where <span class="math notranslate nohighlight">\(z_i\)</span> are the logits and <span class="math notranslate nohighlight">\(T\)</span> is the temperature parameter.</p>
<p>This seemingly simple operation has profound effects on the distribution’s entropy due to the <strong class="underline-bold">mathematical interplay between the linear scaling on logits and the exponential scaling by the softmax operation</strong>:</p>
<ul class="simple">
<li><p><strong>Temperature = 1.0</strong>: Uses the model’s raw probabilities without adjustment</p></li>
<li><p><strong>Temperature &lt; 1.0</strong>: Sharpens the distribution, making high-probability tokens more likely</p>
<ul>
<li><p><strong>Low values (0.3-0.7)</strong>: Produce more focused, deterministic text.</p></li>
<li><p><strong class="underline-bold">As T approaches 0, sampling approaches greedy search behavior</strong>.</p></li>
</ul>
</li>
<li><p><strong>Temperature &gt; 1.0</strong>: Flattens the distribution, giving more weight to low-probability tokens.</p>
<ul>
<li><p><strong>High values (1.2-2.0)</strong>: Generate more random, diverse, and unexpected text.</p></li>
<li><p><strong class="underline-bold">Very high values Can produce incoherent or ungrammatical output</strong>.</p></li>
</ul>
</li>
</ul>
<p>On the high level, <strong class="underline-bold">Temperature is essentially a creativity-coherence trade-off parameter</strong> - lower values prioritize accuracy and coherence, while higher values enable more creative exploration of the distribution.</p>
<div class="note admonition">
<p class="admonition-title">Mathematical Insights on Temperature</p>
<p>The power of temperature sampling comes from several key mathematical properties:</p>
<p><strong>1. Exponential Amplification/Suppression</strong></p>
<p>The exponential function magnifies small changes in input into large changes in output:</p>
<ul class="simple">
<li><p>When <strong>T &lt; 1</strong>: Differences between logits get amplified exponentially</p></li>
<li><p>When <strong>T &gt; 1</strong>: Differences between logits get compressed exponentially</p></li>
</ul>
<p>For example, with logits [5.0, 3.0, 1.0]:</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 16.7%" />
<col style="width: 27.8%" />
<col style="width: 27.8%" />
<col style="width: 27.8%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Temperature</p></th>
<th class="head"><p>T = 0.5 (Low)</p></th>
<th class="head"><p>T = 1.0 (Normal)</p></th>
<th class="head"><p>T = 2.0 (High)</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Scaled Logits</p></td>
<td><p>[10.0, 6.0, 2.0]</p></td>
<td><p>[5.0, 3.0, 1.0]</p></td>
<td><p>[2.5, 1.5, 0.5]</p></td>
</tr>
<tr class="row-odd"><td><p>Probabilities</p></td>
<td><p>[0.982, 0.017, 0.001]</p></td>
<td><p>[0.867, 0.119, 0.014]</p></td>
<td><p>[0.649, 0.249, 0.102]</p></td>
</tr>
</tbody>
</table>
<p><strong>2. Entropy Control</strong></p>
<p>Temperature $T$ directly controls the entropy $H(P)$ of the resulting distribution <span class="math notranslate nohighlight">\(P=\{p_0, p_1, ..., p_{|V|}\}\)</span> (<span class="math notranslate nohighlight">\(|V|\)</span> is the vocabulary size):</p>
<div class="math notranslate nohighlight">
\[H(P) = -\sum_i p_i \log p_i\]</div>
<p>As temperature approaches extreme values, the entropy behavior can be formalized:</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}\lim_{T \to 0} H(P) = 0 \quad \text{(deterministic)}\\\lim_{T \to \infty} H(P) = \log(|V|) \quad \text{(uniform distribution)}\end{aligned}\end{align} \]</div>
<p><strong>3. Scale Invariance</strong></p>
<p>Temperature scaling preserves the relative ranking of tokens while changing their probability gaps. The most likely token remains the most likely regardless of temperature, but the probability gap between tokens changes dramatically.</p>
<p><strong>4. Information Theory Connection</strong></p>
<p>From an information theory perspective, temperature controls the “surprise” factor in sampling. Lower temperatures favor high-probability, low-surprise tokens (minimizing information content per token), while higher temperatures allow more surprising, information-rich tokens to appear.</p>
<p><strong>5. Statistical Physics Origin</strong></p>
<p>The temperature parameter gets its name from statistical physics, where the Boltzmann distribution takes a similar form:</p>
<div class="math notranslate nohighlight">
\[p(E) \propto e^{-E/kT}\]</div>
<p>where E is energy, k is Boltzmann’s constant, and T is temperature. In physics, higher temperatures lead to more random states with higher entropy - exactly the behavior we see in text generation.</p>
<p>The mathematical elegance of temperature scaling lies in how a single parameter can smoothly interpolate between deterministic behavior and completely random selection, offering precise control over the exploration-exploitation trade-off in language generation.</p>
</div>
</li>
<li><p><span class="target" id="newconcept-top-k_sampling"></span><span class="newconcept">Top-K Sampling</span> and <span class="target" id="newconcept-top-p_sampling"></span><span class="newconcept">Top-P Sampling</span> (a.k.a. <span class="target" id="newconcept-nucleus_sampling"></span><span class="newconcept">Nucleus Sampling</span>) <strong class="underline-bold">prevent sampling from the long tail</strong> of low-probability tokens by truncating the distribution:</p>
<ul class="simple">
<li><p><strong>Top-K Sampling</strong> identifies the $K$ tokens with the highest next-token logits, and mask out the remaining.</p>
<ul>
<li><p><strong>K = 1</strong>: Equivalent to greedy search.</p></li>
<li><p><strong>Small K (5-20)</strong>: More focused, predictable output.</p></li>
<li><p><strong>Large K (40-100)</strong>: More diverse, creative output.</p></li>
</ul>
</li>
<li><p><strong>Top-P Sampling</strong> sorts tokens by probability in descending order, and selects the smallest set of tokens whose cumulative probability exceeds threshold $P$.</p>
<ul>
<li><p>It addresses a key limitation of Top-K: when the model’s confidence is dispersed across many tokens, Top-K might truncate too aggressively, while when the model is very confident, Top-K might include unnecessary options.</p></li>
<li><p><strong>Common P values range from 0.9 to 0.95</strong>: Include only the tokens comprising 90-95% of probability mass.</p></li>
<li><p><strong>Lower P values (0.5-0.7)</strong>: More conservative generation.</p></li>
</ul>
</li>
</ul>
</li>
</ul>
<div class="note admonition">
<p class="admonition-title">Practical Setup of Temperature, Top-K, and Top-P Sampling</p>
<blockquote>
<div><p>In practice, Temperature, Top-K, and Top-P Sampling are often combined (<strong class="underline-bold">Top-K is more optional, but at least Temperature and Top-P Sampling are combined</strong>) to control sampling-based generation behavior. Different LLM frameworks have distinct default configurations:</p>
<ul class="simple">
<li><p><strong>OpenAI GPT-4.5</strong>:</p>
<ul>
<li><p>Temperature: 1.0 (balanced creativity)</p></li>
<li><p>Top-K: Not directly exposed in the API</p></li>
<li><p>Top-P: 1.0 (considers the entire distribution)</p></li>
</ul>
</li>
<li><p><strong>Claude 3.7</strong>:</p>
<ul>
<li><p>Temperature: 1.0 via Anthropic API, 0.5-1.0 on platforms like Amazon Bedrock</p></li>
<li><p>Top-K: Not explicitly defined in core API; 250 or disabled on some platforms</p></li>
<li><p>Top-P: Not explicitly defined in core API; 0.999-1.0 on some platforms</p></li>
</ul>
</li>
<li><p><strong>DeepSeek models</strong>:</p>
<ul>
<li><p>Temperature: 1.0 (API default), but with task-specific recommendations:</p>
<ul>
<li><p>Coding/Math: 0.0-0.3 (deterministic)</p></li>
<li><p>Data Analysis: 0.5-0.7 (balanced)</p></li>
<li><p>Conversation/Translation: 0.7-1.3 (more creative)</p></li>
<li><p>Creative Writing: 1.3-1.5 (highly creative)</p></li>
</ul>
</li>
<li><p>Top-K: 0 (disabled by default in API)</p></li>
<li><p>Top-P: 1.0 (no truncation by default)</p></li>
<li><p>Note: DeepSeek V3 employs an internal mapping where model temperature = API temperature × 0.3</p></li>
</ul>
</li>
<li><p><strong>Llama 3.2</strong>:</p>
<ul>
<li><p>Temperature: Varies by implementation (0.6-1.0), with 0.8 common for llama.cpp</p></li>
<li><p>Top-K: Varies (0-50), with 40 common for llama.cpp</p></li>
<li><p>Top-P: Varies (0.95-1.0), with 0.9 common for llama.cpp</p></li>
</ul>
</li>
</ul>
<p><strong>Practical tuning recommendations by task</strong>:</p>
<ol class="arabic simple">
<li><p><strong>Factual/Technical</strong> (code, data analysis, math):</p>
<ul class="simple">
<li><p>Temperature: 0.0-0.5</p></li>
<li><p>Top-P: 0.9-0.95</p></li>
<li><p>Top-K: 20-50 if available</p></li>
</ul>
</li>
<li><p><strong>Balanced/Conversational</strong>:</p>
<ul class="simple">
<li><p>Temperature: 0.6-0.8</p></li>
<li><p>Top-P: 0.9-1.0</p></li>
<li><p>Top-K: 40-50 if available</p></li>
</ul>
</li>
<li><p><strong>Creative</strong> (stories, poetry, brainstorming):</p>
<ul class="simple">
<li><p>Temperature: 0.9-1.5 (where supported)</p></li>
<li><p>Top-P: 0.95-1.0</p></li>
<li><p>Top-K: 0 (disabled) or 50-100</p></li>
</ul>
</li>
</ol>
<p>When tuning parameters, adjust one at a time to understand its impact, and always consult the specific platform’s documentation for accurate defaults and supported ranges.</p>
</div></blockquote>
</div>
</section>
<section id="beam-search">
<h4>Beam Search<a class="headerlink" href="#beam-search" title="Link to this heading"></a></h4>
<p><span class="target" id="newconcept-beam_search"></span><span class="newconcept">Beam Search</span> represents a more sophisticated decoding approach than the other two strategies discussed above, exploring multiple possible sequence paths in parallel:</p>
<div class="folding highlight-python notranslate" id="gpt-beam-search"><div class="highlight"><pre><span></span> <span class="nd">@torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span>
 <span class="k">def</span> <span class="nf">beam_search</span><span class="p">(</span>
         <span class="bp">self</span><span class="p">,</span>
         <span class="n">input_ids</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">,</span>
         <span class="n">max_new_tokens</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
         <span class="n">num_beams</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">5</span><span class="p">,</span>
         <span class="n">pad_token_id</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
         <span class="n">eos_token_id</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
         <span class="n">length_penalty</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span>
 <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">:</span>
<span class="w">     </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">     Beam search decoding: maintains multiple candidate sequences (beams) and expands</span>
<span class="sd">     the most promising ones at each step, eventually returning the highest-scoring complete sequence.</span>

<span class="sd">     Args:</span>
<span class="sd">         input_ids: Starting token IDs of shape (batch_size, seq_length)</span>
<span class="sd">         max_new_tokens: Maximum number of tokens to generate</span>
<span class="sd">         num_beams: Number of beams (candidate sequences) to maintain at each step</span>
<span class="sd">         pad_token_id: Token ID used for padding</span>
<span class="sd">         eos_token_id: Token ID that signals sequence completion</span>
<span class="sd">         length_penalty: Controls bias for sequence length (&gt;1.0 penalizes long sequences,</span>
<span class="sd">                         &lt;1.0 rewards longer sequences)</span>

<span class="sd">     Returns:</span>
<span class="sd">         Tensor of shape (batch_size, seq_length + generated_length) containing</span>
<span class="sd">         the highest-scoring sequence for each item in the batch</span>
<span class="sd">     &quot;&quot;&quot;</span>
     <span class="c1"># ----------------</span>
     <span class="c1"># 1. Setup initial state</span>
     <span class="c1"># ----------------</span>
     <span class="n">batch_size</span> <span class="o">=</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
     <span class="n">device</span> <span class="o">=</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">device</span>
     <span class="n">vocab_size</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">vocab_size</span>

     <span class="c1"># Track which sequences have finished generating</span>
     <span class="c1"># batch_finished shape: (batch_size)</span>
     <span class="n">batch_finished</span> <span class="o">=</span> <span class="p">[</span><span class="kc">False</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)]</span>

     <span class="c1"># Total scores for each beam across all decoding steps</span>
     <span class="c1"># beam_scores shape: (batch_size * num_beams)</span>
     <span class="n">beam_scores</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_beams</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>

     <span class="c1"># Set scores for all beams except the first one to -inf</span>
     <span class="c1"># This ensures only the first beam is active at the beginning</span>
     <span class="n">beam_scores</span><span class="p">[</span><span class="mi">1</span><span class="p">::</span><span class="n">num_beams</span><span class="p">]</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="s1">&#39;-inf&#39;</span><span class="p">)</span>
     <span class="n">beam_scores</span><span class="p">[</span><span class="mi">2</span><span class="p">::</span><span class="n">num_beams</span><span class="p">]</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="s1">&#39;-inf&#39;</span><span class="p">)</span>
     <span class="c1"># ... and so on for all beams beyond the first</span>

     <span class="c1"># Expand input_ids to create initial beams (repeat each input num_beams times)</span>
     <span class="c1"># Original: [batch_0, batch_1, ...] → Expanded: [batch_0, batch_0, ..., batch_1, batch_1, ...]</span>
     <span class="c1"># From shape (batch_size, seq_length) to (batch_size * num_beams, seq_length)</span>
     <span class="n">input_ids</span> <span class="o">=</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">repeat_interleave</span><span class="p">(</span><span class="n">num_beams</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

     <span class="c1"># Keep track of the original batch index for each beam</span>
     <span class="c1"># This helps group beams that belong to the same input example</span>
     <span class="c1"># batch_idx shape: (batch_size * num_beams)</span>
     <span class="n">batch_idx</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span><span class="o">.</span><span class="n">repeat_interleave</span><span class="p">(</span><span class="n">num_beams</span><span class="p">)</span>

     <span class="c1"># ----------------</span>
     <span class="c1"># 2. Generation loop</span>
     <span class="c1"># ----------------</span>
     <span class="c1"># Continue until all sequences finish or max length is reached</span>
     <span class="n">max_length</span> <span class="o">=</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">max_new_tokens</span>
     <span class="k">while</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">&lt;</span> <span class="n">max_length</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">all</span><span class="p">(</span><span class="n">batch_finished</span><span class="p">):</span>
         <span class="c1"># Crop input to respect maximum context length if needed</span>
         <span class="n">input_ids_cond</span> <span class="o">=</span> <span class="n">input_ids</span>
         <span class="k">if</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_seq_length</span><span class="p">:</span>
             <span class="n">input_ids_cond</span> <span class="o">=</span> <span class="n">input_ids</span><span class="p">[:,</span> <span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">max_seq_length</span><span class="p">:]</span>

         <span class="c1"># ----------------</span>
         <span class="c1"># 3. Get predictions from model</span>
         <span class="c1"># ----------------</span>
         <span class="c1"># Forward pass through model to get next-token logits</span>
         <span class="c1"># logits shape: (batch_size * num_beams, vocab_size)</span>
         <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span><span class="n">input_ids_cond</span><span class="p">,</span> <span class="n">inference</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

         <span class="c1"># Convert to log probabilities for numerical stability in beam calculations</span>
         <span class="c1"># log_probs shape: (batch_size * num_beams, vocab_size)</span>
         <span class="n">log_probs</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">log_softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

         <span class="c1"># ----------------</span>
         <span class="c1"># 4. Calculate scores for all possible next tokens</span>
         <span class="c1"># ----------------</span>
         <span class="c1"># Add current beam scores to next token scores</span>
         <span class="c1"># We&#39;re maximizing (log_prob1 + log_prob2 + ...) which is equivalent to</span>
         <span class="c1"># maximizing (prob1 * prob2 * ...)</span>
         <span class="c1"># next_scores shape: (batch_size * num_beams, vocab_size)</span>
         <span class="n">next_scores</span> <span class="o">=</span> <span class="n">log_probs</span> <span class="o">+</span> <span class="n">beam_scores</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

         <span class="c1"># Reshape for beam search: combine beams and vocabulary into one dimension</span>
         <span class="c1"># next_scores shape: (batch_size, num_beams * vocab_size)</span>
         <span class="n">next_scores</span> <span class="o">=</span> <span class="n">next_scores</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_beams</span> <span class="o">*</span> <span class="n">vocab_size</span><span class="p">)</span>

         <span class="c1"># ----------------</span>
         <span class="c1"># 5. Select top 2*num_beams candidates</span>
         <span class="c1"># ----------------</span>
         <span class="c1"># We select 2*num_beams candidates to have options in case some end with EOS</span>
         <span class="c1"># next_scores shape: (batch_size, 2 * num_beams)</span>
         <span class="c1"># next_tokens shape: (batch_size, 2 * num_beams)</span>
         <span class="n">next_scores</span><span class="p">,</span> <span class="n">next_tokens</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">topk</span><span class="p">(</span>
             <span class="n">next_scores</span><span class="p">,</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">num_beams</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">largest</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="nb">sorted</span><span class="o">=</span><span class="kc">True</span>
         <span class="p">)</span>

         <span class="c1"># ----------------</span>
         <span class="c1"># 6. Decode beam indices and token indices</span>
         <span class="c1"># ----------------</span>
         <span class="c1"># next_tokens gives positions in the flattened num_beams*vocab_size space</span>
         <span class="c1"># We need to convert these to:</span>
         <span class="c1"># 1) which beam each candidate came from (parent_idx)</span>
         <span class="c1"># 2) which token to append to that beam (next_token_id)</span>

         <span class="c1"># Calculate which beam each candidate came from</span>
         <span class="c1"># parent_idx shape: (batch_size, 2 * num_beams)</span>
         <span class="n">parent_idx</span> <span class="o">=</span> <span class="n">next_tokens</span> <span class="o">//</span> <span class="n">vocab_size</span>

         <span class="c1"># Calculate which token to add to each beam</span>
         <span class="c1"># next_token_id shape: (batch_size, 2 * num_beams)</span>
         <span class="n">next_token_id</span> <span class="o">=</span> <span class="n">next_tokens</span> <span class="o">%</span> <span class="n">vocab_size</span>

         <span class="c1"># ----------------</span>
         <span class="c1"># 7. Build new beams</span>
         <span class="c1"># ----------------</span>
         <span class="c1"># Initialize containers for new beams</span>
         <span class="n">new_input_ids</span> <span class="o">=</span> <span class="p">[]</span>
         <span class="n">new_beam_scores</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_beams</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
         <span class="n">new_beam_tokens</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_beams</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)</span>
         <span class="n">new_beam_indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_beams</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)</span>

         <span class="c1"># Process each batch item separately</span>
         <span class="k">for</span> <span class="n">batch_idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">):</span>
             <span class="c1"># Skip processing if this batch item is already finished</span>
             <span class="k">if</span> <span class="n">batch_finished</span><span class="p">[</span><span class="n">batch_idx</span><span class="p">]:</span>
                 <span class="c1"># For finished sequences, just copy the highest scoring beam</span>
                 <span class="n">best_idx</span> <span class="o">=</span> <span class="mi">0</span>
                 <span class="n">best_beam_idx</span> <span class="o">=</span> <span class="n">parent_idx</span><span class="p">[</span><span class="n">batch_idx</span><span class="p">,</span> <span class="n">best_idx</span><span class="p">]</span>
                 <span class="n">best_beam_token</span> <span class="o">=</span> <span class="n">next_token_id</span><span class="p">[</span><span class="n">batch_idx</span><span class="p">,</span> <span class="n">best_idx</span><span class="p">]</span>
                 <span class="n">best_beam_score</span> <span class="o">=</span> <span class="n">next_scores</span><span class="p">[</span><span class="n">batch_idx</span><span class="p">,</span> <span class="n">best_idx</span><span class="p">]</span>

                 <span class="c1"># Fill all beam slots with the same sequence</span>
                 <span class="k">for</span> <span class="n">beam_idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_beams</span><span class="p">):</span>
                     <span class="n">new_beam_indices</span><span class="p">[</span><span class="n">batch_idx</span><span class="p">,</span> <span class="n">beam_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">best_beam_idx</span>
                     <span class="n">new_beam_tokens</span><span class="p">[</span><span class="n">batch_idx</span><span class="p">,</span> <span class="n">beam_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">best_beam_token</span>
                     <span class="n">new_beam_scores</span><span class="p">[</span><span class="n">batch_idx</span><span class="p">,</span> <span class="n">beam_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">best_beam_score</span>
                 <span class="k">continue</span>

             <span class="c1"># Track which beams have been selected for this batch item</span>
             <span class="n">beam_idx</span> <span class="o">=</span> <span class="mi">0</span>
             <span class="n">beam_candidates</span> <span class="o">=</span> <span class="p">[]</span>

             <span class="c1"># Consider all candidates in order of decreasing score</span>
             <span class="k">for</span> <span class="n">score_idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">num_beams</span><span class="p">):</span>
                 <span class="c1"># Get details of this candidate</span>
                 <span class="n">beam_token_score</span> <span class="o">=</span> <span class="n">next_scores</span><span class="p">[</span><span class="n">batch_idx</span><span class="p">,</span> <span class="n">score_idx</span><span class="p">]</span>
                 <span class="n">beam_token</span> <span class="o">=</span> <span class="n">next_token_id</span><span class="p">[</span><span class="n">batch_idx</span><span class="p">,</span> <span class="n">score_idx</span><span class="p">]</span>
                 <span class="n">beam_id</span> <span class="o">=</span> <span class="n">parent_idx</span><span class="p">[</span><span class="n">batch_idx</span><span class="p">,</span> <span class="n">score_idx</span><span class="p">]</span>

                 <span class="c1"># Calculate effective source beam index</span>
                 <span class="n">effective_beam_id</span> <span class="o">=</span> <span class="n">batch_idx</span> <span class="o">*</span> <span class="n">num_beams</span> <span class="o">+</span> <span class="n">beam_id</span>

                 <span class="c1"># Check if adding this token would complete the sequence</span>
                 <span class="n">is_eos</span> <span class="o">=</span> <span class="p">(</span><span class="n">eos_token_id</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">beam_token</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">==</span> <span class="n">eos_token_id</span><span class="p">)</span>

                 <span class="c1"># If we already have enough beams and this doesn&#39;t end with EOS, skip</span>
                 <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">beam_candidates</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="n">num_beams</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">is_eos</span><span class="p">:</span>
                     <span class="k">continue</span>

                 <span class="c1"># Add this candidate</span>
                 <span class="n">beam_candidates</span><span class="o">.</span><span class="n">append</span><span class="p">((</span>
                     <span class="n">beam_token_score</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span>
                     <span class="n">beam_token</span><span class="p">,</span>
                     <span class="n">effective_beam_id</span>
                 <span class="p">))</span>

                 <span class="c1"># If this token completes the sequence, don&#39;t add it to active beams</span>
                 <span class="k">if</span> <span class="n">is_eos</span><span class="p">:</span>
                     <span class="k">continue</span>

                 <span class="c1"># Add to active beams if we still need more</span>
                 <span class="k">if</span> <span class="n">beam_idx</span> <span class="o">&lt;</span> <span class="n">num_beams</span><span class="p">:</span>
                     <span class="n">new_beam_indices</span><span class="p">[</span><span class="n">batch_idx</span><span class="p">,</span> <span class="n">beam_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">beam_id</span>
                     <span class="n">new_beam_tokens</span><span class="p">[</span><span class="n">batch_idx</span><span class="p">,</span> <span class="n">beam_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">beam_token</span>
                     <span class="n">new_beam_scores</span><span class="p">[</span><span class="n">batch_idx</span><span class="p">,</span> <span class="n">beam_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">beam_token_score</span>
                     <span class="n">beam_idx</span> <span class="o">+=</span> <span class="mi">1</span>

             <span class="c1"># Check if all candidates for this batch item end with EOS</span>
             <span class="c1"># In that case, mark this batch item as finished</span>
             <span class="k">if</span> <span class="nb">all</span><span class="p">(</span><span class="n">beam_token</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">==</span> <span class="n">eos_token_id</span> <span class="k">for</span> <span class="n">_</span><span class="p">,</span> <span class="n">beam_token</span><span class="p">,</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">beam_candidates</span><span class="p">[:</span><span class="n">num_beams</span><span class="p">]):</span>
                 <span class="n">batch_finished</span><span class="p">[</span><span class="n">batch_idx</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>

         <span class="c1"># ----------------</span>
         <span class="c1"># 8. Update beam state</span>
         <span class="c1"># ----------------</span>
         <span class="c1"># Gather selected beams</span>
         <span class="n">beam_indices</span> <span class="o">=</span> <span class="n">new_beam_indices</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>

         <span class="c1"># Find the positions in the original beams</span>
         <span class="n">beam_indices</span> <span class="o">=</span> <span class="n">beam_indices</span> <span class="o">+</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span> <span class="o">*</span> <span class="n">num_beams</span><span class="p">)</span><span class="o">.</span><span class="n">repeat_interleave</span><span class="p">(</span><span class="n">num_beams</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

         <span class="c1"># Get the sequences that these indices point to</span>
         <span class="n">selected_input_ids</span> <span class="o">=</span> <span class="n">input_ids</span><span class="p">[</span><span class="n">beam_indices</span><span class="p">]</span>

         <span class="c1"># Append next tokens to selected beams</span>
         <span class="n">next_token_ids</span> <span class="o">=</span> <span class="n">new_beam_tokens</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
         <span class="n">input_ids</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">selected_input_ids</span><span class="p">,</span> <span class="n">next_token_ids</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

         <span class="c1"># Update beam scores</span>
         <span class="n">beam_scores</span> <span class="o">=</span> <span class="n">new_beam_scores</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>

         <span class="c1"># Apply length penalty if specified</span>
         <span class="k">if</span> <span class="n">length_penalty</span> <span class="o">!=</span> <span class="mf">1.0</span><span class="p">:</span>
             <span class="c1"># Normalize scores by length to the power of length_penalty</span>
             <span class="c1"># This helps control bias toward shorter or longer sequences:</span>
             <span class="c1"># length_penalty &gt; 1.0: penalize long sequences</span>
             <span class="c1"># length_penalty &lt; 1.0: reward long sequences</span>
             <span class="c1"># length_penalty = 1.0: no adjustment</span>
             <span class="n">curr_length</span> <span class="o">=</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
             <span class="n">beam_scores</span> <span class="o">=</span> <span class="n">beam_scores</span> <span class="o">/</span> <span class="p">(</span><span class="n">curr_length</span> <span class="o">**</span> <span class="n">length_penalty</span><span class="p">)</span>

     <span class="c1"># ----------------</span>
     <span class="c1"># 9. Prepare final output</span>
     <span class="c1"># ----------------</span>
     <span class="c1"># Select the best beam for each batch item</span>
     <span class="n">output_ids</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)</span>

     <span class="c1"># Group beams by batch</span>
     <span class="n">grouped_input_ids</span> <span class="o">=</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_beams</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
     <span class="n">grouped_beam_scores</span> <span class="o">=</span> <span class="n">beam_scores</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_beams</span><span class="p">)</span>

     <span class="c1"># Select highest scoring sequence from each batch</span>
     <span class="n">best_beam_indices</span> <span class="o">=</span> <span class="n">grouped_beam_scores</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

     <span class="c1"># Extract those sequences</span>
     <span class="k">for</span> <span class="n">batch_idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">):</span>
         <span class="n">output_ids</span><span class="p">[</span><span class="n">batch_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">grouped_input_ids</span><span class="p">[</span><span class="n">batch_idx</span><span class="p">,</span> <span class="n">best_beam_indices</span><span class="p">[</span><span class="n">batch_idx</span><span class="p">]]</span>

     <span class="k">return</span> <span class="n">output_ids</span>
</pre></div>
</div>
<p>Unlike greedy search and sampling, which maintain a single sequence, beam search <strong class="underline-bold">tracks multiple sequences (beams) simultaneously</strong> to explore different possibilities:</p>
<ol class="arabic simple">
<li><p><strong>Initialization</strong>: Start with the input sequence, duplicated for each beam</p></li>
<li><p><strong>Expansion</strong>: For each beam:</p>
<ul class="simple">
<li><p>Get next-token probabilities from the model</p></li>
<li><p>Calculate the cumulative score for each possible next token by adding the beam’s current score (in log space)</p></li>
<li><p>This effectively maximizes the product of probabilities in the sequence</p></li>
</ul>
</li>
<li><p><strong>Selection</strong>: From all potential beam × vocabulary continuations:</p>
<ul class="simple">
<li><p>Select the top N (where N = number of beams) highest-scoring candidates</p></li>
<li><p>These become the new beams for the next iteration</p></li>
</ul>
</li>
<li><p><strong>Handling Completed Sequences</strong>: Special handling for sequences that produce end-of-sequence tokens:</p>
<ul class="simple">
<li><p>Completed sequences are tracked separately but remain eligible for selection</p></li>
<li><p>When all beams for a batch item end with EOS, that item is marked as finished</p></li>
</ul>
</li>
<li><p><strong>Termination</strong>: Continue until:</p>
<ul class="simple">
<li><p>Maximum length is reached, or</p></li>
<li><p>All sequences in the batch have generated an end-of-sequence token</p></li>
</ul>
</li>
<li><p><strong>Output</strong>: Return the single highest-scoring completed sequence for each batch item</p></li>
</ol>
<p>Several key parameters control beam search behavior:</p>
<section id="beam-width">
<h5>Beam Width<a class="headerlink" href="#beam-width" title="Link to this heading"></a></h5>
<p>The number of beams (<code class="docutils literal notranslate"><span class="pre">num_beams</span></code>) to maintain, typically between 4-10:</p>
<ul class="simple">
<li><p><strong>Higher values</strong> provide a more thorough search but increase computation</p></li>
<li><p><strong>Lower values</strong> reduce computational cost but may miss better solutions</p></li>
<li><p>When beam width = 1, beam search becomes equivalent to greedy search</p></li>
</ul>
</section>
<section id="length-penalty">
<h5>Length Penalty<a class="headerlink" href="#length-penalty" title="Link to this heading"></a></h5>
<p>Beam search has an inherent bias toward shorter sequences (as each additional token can only reduce the overall probability). To counteract this, a <span class="target" id="newconcept-length_penalty"></span><span class="newconcept">length penalty</span> is applied:</p>
<div class="math notranslate nohighlight">
\[\text{score} = \frac{\text{log-probability}}{(\text{length})^{\alpha}}\]</div>
<p>Where α is the length penalty parameter:</p>
<ul class="simple">
<li><p>α &gt; 1.0: Penalizes longer sequences (favors brevity)</p></li>
<li><p>α &lt; 1.0: Rewards longer sequences (favors verbosity)</p></li>
<li><p>α = 1.0: No length normalization (raw log probabilities)</p></li>
</ul>
<p>The length penalty provides control over the model’s tendency to prefer shorter sequences, which is particularly important for tasks like translation and summarization where output length should be appropriate to the content.</p>
</section>
<section id="numerical-considerations">
<h5>Numerical Considerations<a class="headerlink" href="#numerical-considerations" title="Link to this heading"></a></h5>
<p>The implementation uses log probabilities instead of raw probabilities for numerical stability:</p>
<ul class="simple">
<li><p>Working in log space prevents numerical underflow when multiplying many small probabilities</p></li>
<li><p>Addition of log probabilities is equivalent to multiplication of raw probabilities</p></li>
<li><p>Using <code class="docutils literal notranslate"><span class="pre">log_softmax</span></code> instead of <code class="docutils literal notranslate"><span class="pre">softmax</span></code> followed by logarithm is both more efficient and numerically stable</p></li>
</ul>
<div class="note admonition">
<p class="admonition-title">Advantages and Limitations of Beam Search</p>
<p><strong>Advantages</strong>:</p>
<ul class="simple">
<li><p><strong class="underline-bold">Explores multiple promising paths simultaneously</strong></p></li>
<li><p>Less susceptible to local optima than greedy search</p></li>
<li><p>Often produces higher-quality output for tasks requiring precision</p></li>
<li><p>Deterministic (produces reproducible results)</p></li>
</ul>
<p><strong>Limitations</strong>:</p>
<ul class="simple">
<li><p><strong class="underline-bold">Computationally expensive</strong> - scales linearly with beam width</p></li>
<li><p><strong class="underline-bold">Memory intensive</strong> - must store multiple sequences and their histories</p></li>
<li><p><strong class="underline-bold">Tends to produce similar outputs</strong> - focuses on high-probability paths</p></li>
<li><p>Less effective for creative generation tasks where diversity is valued</p></li>
<li><p>Cannot recover from early mistakes shared across all beams</p></li>
</ul>
<p><strong>Optimal Use Cases</strong>:</p>
<p>Beam search is particularly effective for:</p>
<ul class="simple">
<li><p>Machine translation</p></li>
<li><p>Summarization</p></li>
<li><p>Question answering</p></li>
<li><p>Code generation</p></li>
<li><p>Tasks where output quality and coherence are prioritized over diversity</p></li>
</ul>
<p>For creative tasks where diversity and novelty are valued, sampling-based approaches typically produce better results.</p>
</div>
<p>Compared to the previously discussed strategies, beam search represents a more computationally intensive but thorough approach to sequence generation. It bridges the gap between the purely deterministic behavior of greedy search and the flexibility of sampling-based methods, providing a middle ground that works well for many practical applications.</p>
</section>
</section>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="01_data_preparation.html" class="btn btn-neutral float-left" title="Data Preparation" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="03_supervised_learning.html" class="btn btn-neutral float-right" title="Supervised Learning" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2025, Tony.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>